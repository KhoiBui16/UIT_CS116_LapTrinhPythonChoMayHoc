# -*- coding: utf-8 -*-
"""CS116_Product_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBdJYexmuPjFe2pCXEP79gTmsnjohXd9

# CS116.P22 - ĐỒ ÁN

## DANH SÁCH THÀNH VIÊN

1. **Đinh Lê Bình An** - `23520004`
2. **Vũ Gia Khang** - `23520713`
3. **Bùi Nhật Anh Khôi** - `23520761`
4. **Nguyễn Khang Hy** - `23520662`

# Bài toán: **Product Recommendation System based on Consumer Behavior (Hệ thống đề xuất sản phẩm dựa trên hành vi mua sắm của khách hàng)**

## Tên bộ dữ liệu:  [Consumer Behavior and Shopping Habits Dataset](https://www.kaggle.com/datasets/zeesolver/consumer-behavior-and-shopping-habits-dataset)

## Download dataset from Kaggle
"""

import os
import kaggle
from IPython.display import display

root_dir = os.getcwd()
print(f"Root_dir path: [ {root_dir} ]")
data_dir = os.path.join(root_dir, "Data")
print(f"Data_dir path: [ {data_dir} ]")

if not os.path.exists(data_dir):
    os.makedirs(data_dir)
    print(f"=> Created [data_dir] successfully!")

    kaggle.api.authenticate()  # Xác thực bằng API key từ tệp kaggle.json
    dataset_name = "zeesolver/consumer-behavior-and-shopping-habits-dataset" # Đổi tên biến dataset_name theo dataset mong muốn:
    kaggle.api.dataset_download_files(dataset_name, path=data_dir, unzip=True)
    print(f"Dataset đã được tải về tại: [ {data_dir} ]")
else:
    print(f"Dataset exists!")

"""## Path to Dataset"""

dataset_path = os.path.join(data_dir, "shopping_behavior_updated.csv")
print(f"Dataset_path: [ {dataset_path} ]")

"""## Path to Save Figures"""

figures_dir = os.path.join(root_dir, 'Figures')
print(f"Figures_dir path: [ {figures_dir} ]")

if not os.path.exists(figures_dir):
    os.makedirs(figures_dir)
    print(f"Created Figures dir successfully!")
else:
    print(f"Figures dir exists!")

"""# Bước 1: Đăng ký đề tài

## [1.1] - Lý do chọn dữ liệu

Bộ dữ liệu này rất phù hợp để phân tích hành vi mua sắm của khách hàng:
- Cung cấp một cái nhìn toàn diện về các yếu tố ảnh hưởng đến quyết định mua hàng như độ tuổi, giới tính, sở thích sản phẩm, phương thức thanh toán, hình thức giao hàng và tần suất mua sắm.
- Thông qua việc khai thác các thông tin như sản phẩm đã mua, mức chi tiêu, đánh giá của khách hàng, và lịch sử mua hàng trước đó, chúng ta có thể hiểu rõ hơn về xu hướng tiêu dùng, mức độ trung thành và khả năng quay lại của từng nhóm khách hàng.

**Những hiểu biết này sẽ hỗ trợ mạnh mẽ cho việc xây dựng hệ thống gợi ý sản phẩm cá nhân hóa, giúp các doanh nghiệp thương mại điện tử tối ưu hóa chiến lược tiếp thị, nâng cao trải nghiệm người dùng, và phát triển các chương trình giữ chân khách hàng một cách hiệu quả hơn**.

## [1.2] - Các thông tin về bộ dữ liệu

- Số dòng: 3900 dòng
- Số cột: 18 cột
- Loại bài toán: Đề xuất sản phẩm dựa trên thói quen mua đồ của khách hàng

- **Customer ID**: Là khoá chính định danh từng khách hàng, hỗ trợ theo dõi hành vi theo thời gian và phân tích mức độ trung thành.

- **Age**: Dữ liệu định lượng giúp phân khúc khách hàng theo độ tuổi, từ đó cá nhân hoá nội dung tiếp thị.

- **Gender**: Biến phân loại giúp xác định sự khác biệt trong hành vi tiêu dùng giữa nam và nữ.

- **Item Purchased**: Là mục tiêu chính trong bài toán gợi ý; thể hiện rõ sản phẩm mà khách hàng quan tâm.

- **Category**: Cho phép nhóm các sản phẩm cùng loại để phân tích xu hướng theo danh mục (ví dụ: thời trang, điện tử...).

- **Purchase Amount (USD)**: Dữ liệu định lượng cho biết sức chi tiêu, giúp đánh giá khách hàng có giá trị cao (high-value customers).

- **Location**: Cho phép phân tích sự khác biệt hành vi mua sắm theo vùng miền hoặc quốc gia.

- **Size**: Áp dụng cho các sản phẩm thời trang , hỗ trợ kiểm tra mức độ phù hợp kho hàng theo kích cỡ phổ biến.

- **Color**: Phân tích thị hiếu về màu sắc để cải thiện thiết kế và quản lý tồn kho hiệu quả hơn.

- **Season**: Hữu ích trong việc dự đoán nhu cầu theo mùa và lập kế hoạch nhập hàng.

- **Review Rating**: Đo lường sự hài lòng của khách hàng, có thể dùng làm chỉ số đầu vào cho mô hình gợi ý chất lượng.

- **Subscription Status**: Phân biệt giữa khách hàng thường và khách hàng trung thành (đăng ký dịch vụ định kỳ).

- **Shipping Type**: Phân tích lựa chọn giao hàng để tối ưu thời gian & chi phí vận chuyển.

- **Discount Applied**: Xác định mức độ nhạy cảm với giá và tác động của giảm giá đến hành vi mua hàng.

- **Promo Code Used**: Cho biết hiệu quả của các chiến dịch marketing qua mã khuyến mãi.

- **Previous Purchases**: Là một chỉ số quan trọng để xác định khách hàng trung thành và mô hình dự đoán mua lại.

- **Payment Method**: Giúp doanh nghiệp xác định phương thức thanh toán phổ biến và cải thiện cổng thanh toán.

- **Frequency of Purchases**: Đánh giá tần suất mua hàng, là chỉ số then chốt trong mô hình dự đoán giá trị vòng đời khách hàng (CLV).

## [1.3] - Một số nhận xét ban đầu về bộ dữ liệu

● **Thông tin dữ liệu**: Dataset bao gồm các trường quan trọng phản ánh hành vi tiêu dùng như: `thông tin khách hàng` (Customer ID, Age, Gender, Location), `thông tin sản phẩm` (Item Purchased, Category, Size, Color, Season), `chi tiêu và đánh giá` (Purchase Amount, Review Rating), cũng như `thói quen và hình thức mua sắm` (Previous Purchases, Frequency of Purchases, Subscription Status, Payment Method, Shipping Type, Promo Code Used, Discount Applied).

● **Tình trạng dữ liệu**: Bộ dữ liệu được mô tả khá đầy đủ, đa dạng và có tính ứng dụng cao. Các trường thông tin có giá trị rõ ràng, thuận lợi cho việc xử lý và phân tích dữ liệu. Chưa có dấu hiệu rõ ràng về thiếu hụt dữ liệu ở mức nghiêm trọng trong phần mô tả.

● **Định dạng dữ liệu**: Kết hợp giữa nhiều kiểu dữ liệu như số nguyên (int64), số thực (float64) và chuỗi (object), phù hợp để thực hiện các phân tích mô tả, trích xuất insight và huấn luyện các mô hình học máy.

● **Phân tích khách hàng**: Dữ liệu cho phép phân khúc người dùng theo nhân khẩu **học, hành vi chi tiêu, xu hướng sử dụng mã khuyến mãi, lựa chọn phương thức thanh toán hay hình thức vận chuyển. Đây là cơ sở vững chắc để xây dựng hệ thống cá nhân hóa và đề xuất thông minh.

# Bước 2: Phân tích dữ liệu - EDA

## Path to Save EDA Figures
"""

EDA_dir = os.path.join(figures_dir, "EDA")
print(f"EDA_dir path: [ {EDA_dir} ]")

if not os.path.exists(EDA_dir):
    os.makedirs(EDA_dir)
    print(f"Created EDA dir successfully!")
else:
    print(f"EDA dir exists!")

"""## [2.1] - Import thư viện"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

df = pd.read_csv(dataset_path)
print(f"Read [ {dataset_path} ] successfully!")

"""## [2.2] - Kiểm tra dữ liệu ban đầu

### 2.2.1) 5 dòng đầu dữ liệu
"""

print("5 dòng đầu tiên của dữ liệu:")
display(df.head())

"""### 2.2.2) Thông tin tổng quan vê DataFrame"""

print("\nThông tin tổng quan về DataFrame:")
df.info()

"""### 2.2.3) Kích thước DataFrame"""

print(f"Kích thước DataFrame: {df.shape}")

"""### 2.2.4) Thống kê các đặc trưng trong DataFrame"""

print("Thống kê mô tả cho các cột số:")
display(df.describe())

"""### 2.2.5) Thống kê các đặc trưng phân loại trong DataFrame"""

print("Thống kê mô tả cho các cột phân loại:")
display(df.describe(include='object'))

"""## [2.3] - Kiểm tra giá trị bi thiếu (NULL)"""

print("Kiểm tra số lượng giá trị thiếu (NULL) cho mỗi cột:")
missing_values = df.isnull().sum()
missing_info = pd.DataFrame({
    'Số lượng thiếu': missing_values,
})
display(missing_info)

"""## [2.4] - Kiểm tra dữ liệu trùng lặp"""

print(f"Các dữ liệu bị trùng lặp: \n{df.duplicated()}")
print(f"\nSố lượng bị trùng lặp: {df.duplicated().sum()}")

"""## [2.5] - **Nhận xét ban đầu**

- **Kích thước:** Tập dữ liệu gồm 3900 hàng và 19 cột.

- **Kiểu dữ liệu:** Các cột có vẻ có kiểu dữ liệu phù hợp (số nguyên, số thực, object/chuỗi). Cột `Customer ID` là định danh.

- **Giá trị thiếu:** Kiểm tra bằng `df.isnull().sum()` xác nhận **không có giá trị thiếu (NULL)** nào trong toàn bộ dữ liệu. Điều này rất tốt cho việc phân tích.

- **Gia trị trùng lặp**: Kiểm tra bằng `df.duplicated().sum()` xác nhận **không có giá trị trùng lặp** nào trong toàn bộ dữ liệu. Điều này rất tốt cho việc phân tích.

- **Nhóm thông tin về dữ liệu**
  + Thông tin khách hàng: `Customer ID`, `Gender`, `Age`, `Location`, `Subscripttion status`
  + Thông tin sản phẩm và giao dịch: `Item Purchased`, `Category`, `Size`, `Color`, `Season`, `Purchase Amount (USD)`, `Discount Applied`, `Promo Code Used`
  + Thông tin về hành vi mua sắm: `Frequency of Purchases`, `Previous Purchases`, `Review Rating`, `Shipping type`, `Payment Method`

- **Thống kê số:**
  - `Age`: Tuổi từ 18 đến 70, trung bình khoảng 44. Tuổi phân bố khá rộng.
  - `Purchase Amount (USD)`: Số tiền mua hàng từ 20 đến 100 USD, trung bình khoảng 60 USD. Phân bố đều.
  - `Review Rating`: Đánh giá từ 2.5 đến 5.0, trung bình 3.7. Đa số đánh giá ở mức khá cao.
  - `Previous Purchases`: Số lượt mua trước đó từ 1 đến 50, trung bình 25. Phân bố đều.

- **Thống kê phân loại:**
  - Có nhiều cột phân loại cần khám phá chi tiết hơn (ví dụ: `Gender`, `Item Purchased`, `Category`, `Location`, `Payment Method`).
  - `Gender`: Có 2 giá trị duy nhất, `Male` là phổ biến nhất cho thấy nam giới có xu hướng mua sắm nhiều hơn nữ giới.
  - `Item Purchased`: Có 25 mặt hàng khác nhau, `Blouse` phổ biến nhất
  - `Category`: Có 4 danh mục, `Clothing` phổ biến nhất.
  - `Location`: Có 50 địa điểm khác nhau, `Montana` phổ biến nhất.
  - `Size`: có 4 loại, `M` là phổ biến nhất.
  - `Color`: Có 7 màu sắc khác nhau, `Olive` là phổ biến nhất.
  - `Season`: Có 4 mùa, `Spring` phổ biến nhất vì đây là mùa xuân mát mẻ và dễ chịu cho việc mua sắm dẫn đến người dân chi tiêu nhiều hơn cho các sản phẩm về quần áo vì dụ như áo blouse, đặc biệt là thành phố `Montana` nơi có khí hậu lạnh hơn.
  - `Payment Method`: Có 6 phương thức, `Credit Card` là phương thức thường được dùng để thanh toán vì độ tiện lợi và nhanh chóng nhưng phương thích ưu thích của khách hàng lại là `PayPal` vì tính bảo mật và an toàn hơn, đặc biệt là khi mua sắm trực tuyến.
  - `Shipping type`: có 6 loại, `Free Shipping` là lựa chọn ưu tiên của khách hàng với mong muốn tiết kiệm chi tiết cho phí vận chuyển và tập trung số tiền vào sản phẩm
  - `Frequency of Purchases`: Có 7 tần suất mua hàng, `Every 3 Months` là tần suất mua hàng phố biến nhất vì đây là thời điểm mà người tiêu dùng thường có nhu cầu mua sắm nhiều hơn, đặc biệt là vào mùa xuân và mùa hè.

## [2.6] - Phân tích đơn biến (Univariate Analysis)

### Path to save Univariate Figures
"""

univariate_dir = os.path.join(EDA_dir, "Univariate")
print(f"Univariate_dir path: [ {univariate_dir} ]")

if not os.path.exists(univariate_dir):
    os.makedirs(univariate_dir)
    print(f"Created Univariate dir successfully!")
else:
    print(f"Univariate dir exists!")

"""### 2.6.1) Phân tích biến số (Numerical Variales)

#### Path to save Numerical Figures
"""

numerical_dir = os.path.join(univariate_dir, "Numerical")
print(f"Numerical_dir path: [ {numerical_dir} ]")

if not os.path.exists(numerical_dir):
    os.makedirs(numerical_dir)
    print(f"Created Numerical dir successfully!")
else:
    print(f"Numerical dir is exists!")

"""#### **Age**"""

col = 'Age'
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x=col, color='blue')
plt.title('Phân bổ của Age')
plt.xlabel('Age')
save_path = os.path.join(numerical_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

"""#### **Purchase Amount (USD)**"""

col = 'Purchase Amount (USD)'
plt.figure(figsize=(10, 6))
sns.histplot(df, x=col, bins=30, kde=True, color='green')
plt.title('Phân bổ của Purchase Amount (USD)')
plt.xlabel('Purchase Amount (USD)')
plt.ylabel('Tần suất')
save_path = os.path.join(numerical_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

"""#### **Review Rating**"""

col = 'Review Rating'
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x=col, fill=True, color='blue')
plt.title("Phân phối Review Rating")
plt.xlabel("Review Rating")
plt.ylabel("Tần suất")
save_path = os.path.join(numerical_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

"""#### **Previous Purchases**"""

col = 'Previous Purchases'
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x=col, bins=30, kde=True, color='purple')
plt.title("Phần phối của Previous Purchases")
plt.xlabel("Previous Purchases")
plt.ylabel("Tần suất")
save_path = os.path.join(numerical_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

"""#### **Nhận xét về phân tích đơn biến số**

- **Age** phân bố đều trong khoảng 18-70, không có ngoại lai đáng kể, với trung vị khoảng 45 tuổi.
- **Purchase Amount** tập trung quanh 40-80 USD và phân bố đối xứng, cho thấy phân khúc giá trung bình của các sản phẩm khá ổn định.
- **Review Rating** có phân phối lệch trái với đa số đánh giá ở mức 3.5-4.5, điều này phổ biến trong dữ liệu đánh giá và gợi ý hệ thống recommendation cần xử lý sự thiên lệch này.
- **Previous Purchases** phân bố đều ở mức 10-40 lần, cho thấy dữ liệu có sự cân bằng giữa khách hàng mới và cũ.

=> **Những đặc điểm phân phối này sẽ ảnh hưởng đến cách thiết kế hệ thống recommendation, đặc biệt là việc chuẩn hóa và xử lý outliers**.

### 2.6.2) Phân tích biến phân loai (Categorical Variables)

#### Path to save Category Figures
"""

category_dir = os.path.join(univariate_dir, "Category")
print(f"Category_dir path: [ {category_dir} ]")

if not os.path.exists(category_dir):
    os.makedirs(category_dir)
    print(f"Created Category dir successfully!")
else:
    print(f"Category dir is exists!")

"""#### **Columns categorical variables to plot**"""

categorical_columns = df.select_dtypes(include=['object']).columns
cols_to_plot = ['Gender', 'Category', 'Item Purchased', 'Size', 'Season', 'Location',
                    'Subscription Status', 'Payment Method', 'Shipping Type',
                    'Discount Applied', 'Promo Code Used', 'Frequency of Purchases']
print(f"Columns categorical variables to plot:")
for col in cols_to_plot:
    print(f"- [{col}]")

"""#### **Gender**"""

col = "Gender"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Category**"""

col = "Category"
order = df[col].value_counts().index
value_counts = df[col].value_counts()


plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()
save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print(value_counts)
print("-" * 60)

"""#### **Item Purchased**"""

col = "Item Purchased"
top_n = 15
n_unique = df[col].nunique()
order = df[col].value_counts().index
value_counts = df[col].value_counts().nlargest(top_n)

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=value_counts.index,
    legend=False,
)

plt.title(f"Top {top_n} Phân phối của {col}", fontsize=14)
print(f"* Hiển thị Top {top_n} loại phổ biến nhất (Tổng số loại: {n_unique})")

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Size**"""

col = "Size"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Season**"""

col = "Season"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Location**"""

col = "Location"
top_n = 15
n_unique = df[col].nunique()
order = df[col].value_counts().index
value_counts = df[col].value_counts().nlargest(top_n)

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=value_counts.index,
    legend=False,
)

plt.title(f"Top {top_n} Phân phối của {col}", fontsize=14)
print(f"* Hiển thị Top {top_n} loại phổ biến nhất (Tổng số loại: {n_unique})")

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Subscription Status**"""

col = "Subscription Status"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Payment Method**"""

col = "Payment Method"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Shipping Type**"""

col = "Shipping Type"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Discount Applied**"""

col = "Discount Applied"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Promo Code Used**"""

col = "Promo Code Used"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Frequency of Purchases**"""

col = "Frequency of Purchases"
order = df[col].value_counts().index
value_counts = df[col].value_counts()

plt.figure(figsize=(12, 5))
sns.barplot(
    x=value_counts.index,
    y=value_counts.values,
    hue=value_counts.index,
    palette="viridis",
    order=order,
    legend=False,
)
plt.title(f"Phân phối của {col}", fontsize=14)

plt.xlabel(col)
plt.ylabel("Số lượng")
plt.tight_layout()

save_path = os.path.join(category_dir, f'{col}_plot.png')
plt.savefig(save_path)
plt.show()

print(f"Tần suất của [{col}]:")
print("-" * 60)
print(value_counts)
print("-" * 60)

"""#### **Nhận xét về phân tích đơn biến phân loại:**

- `Gender`: **Nam (Male) chiếm đa số** (khoảng 68%), gấp đôi Nữ (Female). Đây là một điểm mất cân bằng cần lưu ý.
- `Category` (Nhãn tiềm năng): **Clothing (Quần áo)** là danh mục phổ biến nhất, chiếm gần một nửa số giao dịch. Tiếp theo là Accessories (Phụ kiện), Footwear (Giày dép) và Outerwear (Áo khoác ngoài). Phân phối không cân bằng.
- `Item Purchased` (Nhãn tiềm năng): Có 25 loại mặt hàng. **Blouse, Jeans, Pants, Shirt, Dress** là những mặt hàng được mua nhiều nhất. Phân phối khá đa dạng, không quá tập trung vào một vài mặt hàng.
- `Location`, `Color`: Có quá nhiều giá trị duy nhất (50 và 25), biểu đồ đầy đủ sẽ khó đọc. Montana là địa điểm phổ biến nhất, Olive/Yellow/Silver/Teal/Green là các màu phổ biến.
- `Size`: Size **M (Medium)** là phổ biến nhất, chiếm hơn 40%. Tiếp theo là L (Large), S (Small), và XL (Extra Large).
- `Season`: Các mùa có số lượng mua hàng **khá cân bằng**, với Spring (Xuân) và Fall (Thu) nhỉnh hơn một chút so với Winter (Đông) và Summer (Hè).
- `Subscription Status`: Đa số khách hàng **không đăng ký (No)**, chiếm khoảng 73%.
- `Payment Method`, `Preferred Payment Method`: **PayPal và Credit Card** là hai phương thức thanh toán/ưu tiên thống trị. Các phương thức khác ít phổ biến hơn. Cần xem xét mối quan hệ giữa hai cột này.
- `Shipping Type`: **Free Shipping (Miễn phí vận chuyển)** là lựa chọn phổ biến nhất, tiếp theo là Standard (Tiêu chuẩn) và Store Pickup (Nhận tại cửa hàng). Các loại hình vận chuyển nhanh ít được sử dụng hơn.
- `Discount Applied`, `Promo Code Used`: Phần lớn các giao dịch **có áp dụng giảm giá/mã khuyến mãi (Yes)**, chiếm khoảng 57%. Hai cột này có vẻ liên quan chặt chẽ.
- `Frequency of Purchases`: Tần suất mua hàng **phân bố khá đều** giữa các lựa chọn (Weekly, Fortnightly, Monthly, Quarterly, Bi-Weekly, Annually, Every 3 Months), mỗi loại chiếm khoảng 13-15%.

## [2.7] - Phân tích hai biến (Bivariate Analysis)

### Path to save Bivariate Figures
"""

bivariate_dir = os.path.join(EDA_dir, "Bivariate")
print(f"Bivariate_dir path: [ {bivariate_dir} ]")

if not os.path.exists(bivariate_dir):
    os.makedirs(bivariate_dir)
    print(f"Created Bivariate dir successfully!")
else:
    print(f"Bivariate dir exists!")

"""### 2.7.1) Mối quan hệ giữa các biến số (Feature-Feature: Numerical)

#### Path to save Numerical-Numerical Figures
"""

numerical_numerical_dir = os.path.join(bivariate_dir, "Numerical_Numerical")
print(f"Numerical_Numerical_dir path: [ {numerical_numerical_dir} ]")

if not os.path.exists(numerical_numerical_dir):
    os.makedirs(numerical_numerical_dir)
    print(f"Created Numerical Numerical dir successfully!")
else:
    print(f"Numerical Numerical dir exists!")

"""#### Tương quan giữa các biến số **(Feature-Feature: Numerical)**"""

numerical_columns = df.select_dtypes(include=np.number).columns
correlation_matrix = df[numerical_columns].corr()

print("Ma trận tương quan Pearson:")
display(correlation_matrix)

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Ma trận Tương quan Pearson giữa các Biến số', fontsize=16)

save_path = os.path.join(numerical_numerical_dir, f'numerical_numerical_pearson_correlation_matrix_plot.png')
plt.savefig(save_path)
plt.show()

"""#### **Nhận xét ma trận tương quan Numerical vs Numerical**

- Các hệ số tương quan Pearson giữa các cặp biến số (`Age`, `Purchase Amount (USD)`, `Review Rating`, `Previous Purchases`) đều **rất gần 0** (giá trị tuyệt đối < 0.04).
- Điều này cho thấy **hầu như không có mối quan hệ tuyến tính** nào đáng kể giữa các biến số này.
- Ví dụ: Tuổi của khách hàng không liên quan tuyến tính đến số tiền họ chi tiêu, đánh giá của họ, hay số lần mua hàng trước đó.
- Các biến số này cung cấp thông tin tương đối độc lập với nhau.

### 2.7.2) Mối quan hệ giữa biến số và biến phân loại (Feature-Feature/Label: Numerical vs Categorical)

#### Path to save Numerical-Categorical Figures
"""

numerical_categorical_dir = os.path.join(bivariate_dir, "Numerical_Categorical")
print(f"numerical_categorical_dir path: [ {numerical_categorical_dir} ]")

if not os.path.exists(numerical_categorical_dir):
    os.makedirs(numerical_categorical_dir)
    print(f"Created Numerical Categorical dir successfully!")
else:
    print(f"Numerical Categorical dir exists!")

"""#### **Numerical variables**"""

num_vars = ['Purchase Amount (USD)', 'Review Rating', 'Age']
print(f"Danh sách biến số (num_vars):")
for i, num in enumerate(num_vars):
    print(f"{i + 1}. [{num}]")

"""#### **Categorical variables**"""

cat_vars = ['Category', 'Gender', 'Subscription Status', 'Season', 'Frequency of Purchases']
print(f"Danh sách biến phân loại (cat_vars):")
for i, cat in enumerate(cat_vars):
    print(f"{i + 1}. [{cat}]")

"""#### **Valid numerical-categorical pairs**"""

valid_pairs = []
for num_var in num_vars:
    for cat_var in cat_vars:
        # Kiểm tra điều kiện loại bỏ
        if (
            num_var == "Age" and cat_var not in ["Gender", "Frequency of Purchases"]
        ) or (
            num_var == "Review Rating"
            and cat_var not in ["Subscription Status", "Category"]
        ):
            continue
        # Kiểm tra số giá trị duy nhất
        n_unique_cat = df[cat_var].nunique()
        if n_unique_cat > 10:
            continue
        valid_pairs.append((num_var, cat_var))

print("\nCác cặp (num_var) và (cat_var) hợp lệ:")
for i, (num_var, cat_var) in enumerate(valid_pairs, 1):
    print(f"{i}. [{num_var} - {cat_var}]")
print(f"\nTổng số cặp hợp lệ: {len(valid_pairs)}")

"""#### **Purchase Amount (USD) - Category**"""

num_var = 'Purchase Amount (USD)'
cat_var = 'Category'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Purchase Amount (USD) - Gender**"""

num_var = 'Purchase Amount (USD)'
cat_var = 'Gender'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Purchase Amount (USD) - Subscription Status**"""

num_var = 'Purchase Amount (USD)'
cat_var = 'Subscription Status'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Purchase Amount (USD) - Season**"""

num_var = 'Purchase Amount (USD)'
cat_var = 'Season'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Purchase Amount (USD) - Frequency of Purchases**"""

num_var = 'Purchase Amount (USD)'
cat_var = 'Frequency of Purchases'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)
    plt.xticks(rotation=45, ha='right')

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)
    plt.xticks(rotation=45, ha='right')

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Review Rating - Category**"""

num_var = 'Review Rating'
cat_var = 'Category'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Review Rating - Subscription Status**"""

num_var = 'Review Rating'
cat_var = 'Subscription Status'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Age - Gender**"""

num_var = 'Age'
cat_var = 'Gender'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)
    plt.xticks(rotation=45, ha='right')

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)
    plt.xticks(rotation=45, ha='right')

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Age - Frequency of Purchases**"""

num_var = 'Age'
cat_var = 'Frequency of Purchases'
n_unique_cat = df[cat_var].nunique()

if n_unique_cat <= 10:
    plt.figure(figsize=(12, 5))

    # Box Plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
    plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)
    plt.xticks(rotation=45, ha='right')

    # Violin Plot
    plt.subplot(1, 2, 2)
    sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
    plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
    plt.xlabel(cat_var)
    plt.ylabel(num_var)
    plt.xticks(rotation=45, ha='right')

    plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=0.98)
    plt.tight_layout()

    # Lưu biểu đồ
    save_path = os.path.join(numerical_categorical_dir, f'{num_var.replace(" ", "_")}_{cat_var.replace(" ", "_")}_plot.png')
    plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    print("-"*60)

"""#### **Nhận xét (Numerical vs Categorical):**

- `Purchase Amount vs Category`: Phân phối số tiền mua hàng (trung vị, tứ phân vị, mật độ) **khá tương đồng** giữa các danh mục (`Clothing`, `Footwear`, `Outerwear`, `Accessories`). Violin plot xác nhận phân phối gần như đồng đều trong mỗi danh mục. Không có danh mục nào nổi bật về mức chi tiêu cao hơn hẳn.
- `Purchase Amount vs Gender`: Phân phối số tiền mua hàng giữa **Nam và Nữ gần như giống hệt nhau**. Giới tính dường như không ảnh hưởng đến số tiền chi tiêu trung bình cho mỗi giao dịch.
- `Purchase Amount vs Season`: Phân phối số tiền mua hàng **không thay đổi đáng kể theo mùa**. Mức chi tiêu trung bình và khoảng biến thiên tương tự nhau qua các mùa.
- `Review Rating vs Category`: Đánh giá trung bình có vẻ **cao hơn một chút cho Footwear và Outerwear** so với Clothing và Accessories, nhưng sự khác biệt không lớn. Phân phối đánh giá lệch trái (nhiều điểm cao) ở tất cả các danh mục.
- `Review Rating vs Subscription Status`: Khách hàng đã đăng ký (`Yes`) có xu hướng đưa ra **đánh giá trung bình cao hơn một chút** so với khách hàng không đăng ký (`No`). Violin plot cho thấy mật độ đánh giá cao (4-5) dày đặc hơn ở nhóm 'Yes'.
- `Age vs Gender`: Phân phối tuổi giữa **Nam và Nữ rất giống nhau**, đều gần như đồng đều.
- `Age vs Frequency of Purchases`: Phân phối tuổi **khá tương đồng** giữa các nhóm tần suất mua hàng khác nhau. Không có nhóm tuổi nào đặc biệt ưa chuộng một tần suất mua hàng cụ thể.

### 2.7.3) Mối quan hệ giữa các biến phân loại (Feature-Feature: Categorical vs Categorical)

#### Path to save Categorical-Categorical Figures
"""

categorical_categorical_dir = os.path.join(bivariate_dir, "Categorical_Categorical")
print(f"Categorical_Categorical_dir path: [ {categorical_categorical_dir} ]")

if not os.path.exists(categorical_categorical_dir):
    os.makedirs(categorical_categorical_dir)
    print(f"Created Categorical Categorical dir successfully!")
else:
    print(f"Categorical Categorical dir exists!")

"""#### Categorical Variables"""

cat_vars = ['Category', 'Gender', 'Subscription Status', 'Season', 'Discount Applied', 'Promo Code Used', 'Item Purchased', 'Size']

print("Danh sách biến phân loại (cat_vars):")
for i, var in enumerate(cat_vars, 1):
    print(f"{i}. [{var}]")

valid_pairs = [
    ('Category', 'Gender'),
    ('Season', 'Subscription Status'),
    ('Discount Applied', 'Promo Code Used'),
    ('Item Purchased', 'Gender'),
    ('Gender', 'Size'),
    ('Discount Applied', 'Gender')
]

print("Các cặp hợp lệ phù hợp với Product Recommendation System:")
for i, (x_var, hue_var) in enumerate(valid_pairs, 1):
    print(f"{i}. {x_var} - {hue_var}")
print(f"\nTổng số cặp hợp lệ: {len(valid_pairs)}")

"""#### **Category - Gender**"""

x_var = "Category"
hue_var = "Gender"

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x=x_var, hue=hue_var, palette="pastel")
plt.title(f"Phân phối {x_var} theo {hue_var}", fontsize=14)
plt.xlabel(x_var)
plt.ylabel("Số lượng")
plt.tight_layout()

# Lưu biểu đồ
save_path = os.path.join(
    categorical_categorical_dir,
    f'{x_var.replace(" ", "_")}_{hue_var.replace(" ", "_")}_plot.png',
)
plt.savefig(save_path, bbox_inches="tight")
plt.show()
print("-" * 60)

"""#### **Season - Subscription Status**"""

x_var = "Season"
hue_var = "Subscription Status"
order = ["Spring", "Summer", "Fall", "Winter"]

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x=x_var, hue=hue_var, palette="coolwarm", order=order)
plt.title(f"Phân phối {x_var} theo {hue_var}", fontsize=14)
plt.xlabel(x_var)
plt.ylabel("Số lượng")
plt.tight_layout()

# Lưu biểu đồ
save_path = os.path.join(
    categorical_categorical_dir,
    f'{x_var.replace(" ", "_")}_{hue_var.replace(" ", "_")}_plot.png',
)
plt.savefig(save_path, bbox_inches="tight")
plt.show()
print("-" * 60)

"""#### **Discount Applied - Promo Code Used**"""

x_var = "Discount Applied"
hue_var = "Promo Code Used"

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x=x_var, hue=hue_var, palette="magma")
plt.title(f"Mối quan hệ giữa {x_var} và {hue_var}", fontsize=14)
plt.xlabel(x_var)
plt.ylabel("Số lượng")
plt.tight_layout()

# Lưu biểu đồ
save_path = os.path.join(
    categorical_categorical_dir,
    f'{x_var.replace(" ", "_")}_{hue_var.replace(" ", "_")}_plot.png',
)
plt.savefig(save_path, bbox_inches="tight")
plt.show()
print("Bảng chéo: Discount Applied vs Promo Code Used")
display(pd.crosstab(df["Discount Applied"], df["Promo Code Used"]))
print("-" * 60)

"""#### **Item Purchased - Gender**"""

x_var = "Item Purchased"
hue_var = "Gender"
top_items = df[x_var].value_counts().nlargest(10).index

plt.figure(figsize=(12, 8))
sns.countplot(
    data=df[df[x_var].isin(top_items)],
    y=x_var,
    hue=hue_var,
    palette="pastel",
    order=top_items,
)
plt.title(f"Top 10 {x_var} theo {hue_var}", fontsize=14)
plt.xlabel("Số lượng")
plt.ylabel(x_var)
plt.legend(title=hue_var)
plt.tight_layout()

# Lưu biểu đồ
save_path = os.path.join(
    categorical_categorical_dir,
    f'{x_var.replace(" ", "_")}_{hue_var.replace(" ", "_")}_plot.png',
)
plt.savefig(save_path, bbox_inches="tight")
plt.show()
print("-" * 60)

"""#### **Gender - Size**"""

x_var = "Gender"
hue_var = "Size"

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x=x_var, hue=hue_var, palette="Set2")
plt.title(f"Phân phối {x_var} theo {hue_var}", fontsize=14)
plt.xlabel(x_var)
plt.ylabel("Số lượng")
plt.legend(title=hue_var)
plt.tight_layout()

# Lưu biểu đồ
save_path = os.path.join(
    categorical_categorical_dir,
    f'{x_var.replace(" ", "_")}_{hue_var.replace(" ", "_")}_plot.png',
)
plt.savefig(save_path, bbox_inches="tight")
plt.show()
print("-" * 60)

"""#### **Discount Applied - Gender**"""

x_var = "Discount Applied"
hue_var = "Gender"

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x=x_var, hue=hue_var, palette="Set2")
plt.title(f"Phân phối {x_var} theo {hue_var}", fontsize=14)
plt.xlabel(x_var)
plt.ylabel("Số lượng")
plt.legend(title=hue_var)
plt.tight_layout()

# Lưu biểu đồ
save_path = os.path.join(
    categorical_categorical_dir,
    f'{x_var.replace(" ", "_")}_{hue_var.replace(" ", "_")}_plot.png',
)
plt.savefig(save_path, bbox_inches="tight")
plt.show()
print("-" * 60)

"""#### **Nhận xét (Categorical vs Categorical):**

- `Category vs Gender`: Ở **mọi danh mục**, số lượng mua hàng của **Nam đều nhiều hơn Nữ**, phản ánh tỷ lệ giới tính chung trong dữ liệu (Nam chiếm ~68%). Tỷ lệ phân bố các danh mục trong nhóm Nam và Nữ có vẻ tương tự nhau (Clothing là phổ biến nhất cho cả hai).
- `Season vs Subscription Status`: Tỷ lệ khách hàng **không đăng ký (No) luôn cao hơn** khách hàng có đăng ký (Yes) ở tất cả các mùa. Không có sự khác biệt rõ rệt về xu hướng đăng ký theo mùa; tỷ lệ Yes/No khá ổn định qua các mùa.
- `Discount Applied vs Promo Code Used`: Có một **mối quan hệ cực kỳ chặt chẽ** giữa hai biến này. Bảng chéo cho thấy 100% các trường hợp `Discount Applied = Yes` đều tương ứng với `Promo Code Used = Yes`, và 100% trường hợp `Discount Applied = No` tương ứng với `Promo Code Used = No`. Điều này xác nhận hai cột này chứa **thông tin hoàn toàn trùng lặp**.
- `Item Purchased vs Gender` (Phân tích nhãn tiềm năng): Xem xét top 10 mặt hàng phổ biến nhất, tỷ lệ Nam mua nhiều hơn Nữ được duy trì ở hầu hết các mặt hàng, phù hợp với tỷ lệ giới tính chung. Không có mặt hàng nào trong top 10 cho thấy sự ưa chuộng đặc biệt rõ rệt của một giới tính so với giới tính còn lại (ví dụ: tỷ lệ Nam/Nữ mua Blouse tương tự tỷ lệ Nam/Nữ mua Jeans).

## [2.8] - Phân tích đa biến (Multivariate Analysis)

#### Path to save Multivariate Figures
"""

multivariate_dir = os.path.join(EDA_dir, "Multivariate")
print(f"Multivariate_dir path: [ {multivariate_dir} ]")

if not os.path.exists(multivariate_dir):
    os.makedirs(multivariate_dir)
    print(f"Created Multivariate dir successfully!")
else:
    print(f"Multivariate dir exists!")

"""#### 2.8.1) **Numerical variables - Gender**"""

cols_for_pairplot = numerical_columns.tolist() + ["Gender"]
g = sns.pairplot(
    df[cols_for_pairplot], hue="Gender", diag_kind="kde", corner=True, palette="pastel"
)  # Tạo pairplot và lấy figure
g.fig.suptitle(
    "Pair Plot các Biến số theo Giới tính", y=1, fontsize=18
)  # Thêm suptitle vào figure của pairplot

plt.tight_layout()

save_path = os.path.join(multivariate_dir, "Pair_Plot_Numerical_by_Gender.png")
g.savefig(save_path, bbox_inches="tight")
plt.show()

"""##### **Nhận xét Pair Plot:**

- **Tương quan:** Các biểu đồ phân tán (scatter plots) ở các ô không nằm trên đường chéo xác nhận lại kết quả từ ma trận tương quan: **không có mối quan hệ tuyến tính rõ ràng** giữa các cặp biến số (`Age`, `Purchase Amount (USD)`, `Review Rating`, `Previous Purchases`). Các điểm dữ liệu phân bố khá ngẫu nhiên.
- **Phân phối theo Giới tính:** Các biểu đồ phân phối mật độ (KDE) trên đường chéo cho thấy hình dạng phân phối của từng biến số là **tương tự nhau giữa Nam và Nữ**. Mặc dù số lượng Nam nhiều hơn, nhưng hình dạng chung của phân phối tuổi, số tiền mua hàng, đánh giá, và số lượt mua trước đó không khác biệt đáng kể giữa hai giới.

#### 2.8.2) **Purchase Amount - Category - Gender**
"""

plt.figure(figsize=(12, 7))
sns.boxplot(
    data=df, x="Category", y="Purchase Amount (USD)", hue="Gender", palette="coolwarm"
)
plt.title("Purchase Amount theo Category và Gender", fontsize=16)
plt.xlabel('Category')
plt.ylabel('Purchase Amount (USD)')
plt.legend(title='Gender')
plt.tight_layout()

save_path = os.path.join(
    multivariate_dir, "Box_Plot_Purchase_Amount_by_Category_Gender.png"
)

plt.savefig(save_path, bbox_inches="tight")
plt.show()

"""##### **Nhận xét (Số tiền mua hàng theo Danh mục và Giới tính):**

- Biểu đồ này kết hợp thông tin từ `Purchase Amount`, `Category`, và `Gender`.
- Nó tái khẳng định những gì đã thấy trong phân tích hai biến:
  - Phân phối số tiền mua hàng **khá giống nhau giữa các danh mục**.
  - Phân phối số tiền mua hàng **gần như giống hệt nhau giữa Nam và Nữ** trong từng danh mục.
- Không có sự tương tác đáng kể nào giữa Giới tính và Danh mục đối với Số tiền mua hàng. Ví dụ, không có chuyện Nam chi tiêu nhiều hơn Nữ đáng kể chỉ ở một danh mục cụ thể nào đó.

## [2.9] - **Tổng kết và Nhận xét chung**

### 2.9.1) **Chất lượng dữ liệu:**

- Dữ liệu rất tốt, đầy đủ, **không có giá trị thiếu (NULL)** như đã kiểm tra. Các kiểu dữ liệu phù hợp.

### 2.9.2) **Phân phối biến:**

- Các biến số (`Age`, `Purchase Amount`, `Previous Purchases`) có phân phối gần như đồng đều.
- `Review Rating` lệch trái nhẹ (nhiều đánh giá cao).
- Biến `Gender` mất cân bằng (Nam nhiều hơn Nữ).
- Các biến phân loại khác như `Category`, `Size`, `Payment Method`, `Shipping Type` có sự tập trung vào một vài giá trị phổ biến (`Clothing`, `M`, `PayPal`/`Credit Card`, `Free Shipping`).
- `Season` và `Frequency of Purchases` phân bố khá đều.

### 2.9.3) **Ngoại lai:**

- Các biến số chính (`Age`, `Purchase Amount`, `Previous Purchases`) **không có ngoại lai đáng kể** theo phương pháp IQR. `Review Rating` có một số điểm thấp nhưng nằm trong phạm vi hợp lệ và không ảnh hưởng lớn.

### 2.9.4) **Mối quan hệ:**

- **Không có tương quan tuyến tính** mạnh giữa các biến số.
- Số tiền mua hàng (`Purchase Amount`) **không phụ thuộc nhiều** vào `Category`, `Gender`, hay `Season`.
- Khách hàng đăng ký (`Subscription Status=Yes`) có xu hướng **đánh giá cao hơn một chút**.
- `Discount Applied` và `Promo Code Used` chứa **thông tin hoàn toàn trùng lặp**.

### 2.9.5) **Phân tích nhãn tiềm năng (cho bài toán đề xuất):**

- `Category`: 'Clothing' chiếm đa số, các danh mục khác ít hơn đáng kể.
- `Item Purchased`: Phân phối đa dạng hơn `Category`, có 25 mặt hàng, top 5 là Blouse, Jeans, Pants, Shirt, Dress. Không có sự khác biệt lớn về sở thích mặt hàng (trong top 10) giữa Nam và Nữ.

### 2.9.6) **Hướng tiếp theo:**

- Dữ liệu sạch và có cấu trúc tốt, sẵn sàng cho các bước tiếp theo như tiền xử lý (mã hóa biến phân loại) và xây dựng mô hình.
- **Cần loại bỏ một trong hai cột `Discount Applied` hoặc `Promo Code Used`** do tính dư thừa.
- Sự mất cân bằng giới tính cần được lưu ý nếu xây dựng mô hình dự đoán liên quan đến giới tính, nhưng có thể không ảnh hưởng nhiều đến mô hình đề xuất dựa trên sản phẩm/danh mục.
- Dữ liệu phù hợp để xây dựng các mô hình đề xuất (ví dụ: lọc cộng tác dựa trên `Customer ID`, `Item Purchased`, `Review Rating`; hoặc lọc dựa trên nội dung/thuộc tính sản phẩm như `Category`, `Color`, `Season`) hoặc phân cụm khách hàng (dựa trên `Age`, `Frequency of Purchases`, `Category` ưa thích, v.v.).

## [2.10] - Tổng quan về luồng xử lý dữ liệu

Dự án này xử lý dữ liệu theo hai luồng song song, hỗ trợ cho các phương pháp recommendation khác nhau:

### 2.10.1)  **Feature Dataset** (`shopping_behavior_final_features.csv`):

- Chứa các đặc trưng của người dùng, sản phẩm và tương tác
- Được sử dụng cho Content-Based Filtering và các mô hình Machine Learning

### 2.10.2) **User-Item Matrix** (`user_item_matrix.csv`):

- Ma trận thể hiện mức độ tương tác giữa người dùng và sản phẩm
- Được sử dụng cho Collaborative Filtering (phương pháp dựa trên sự tương đồng)

**Trong các bước tiếp theo, chúng ta sẽ tiền xử lý dữ liệu, tạo các đặc trưng mới, và áp dụng các phương pháp khác nhau để xây dựng một hệ thống đề xuất hybrid (kết hợp).**

# Bước 3: Tiền xử lý dữ liệu - Preprocessing

## Path to Save Preprocessing Figures
"""

preprocessing_dir = os.path.join(figures_dir, "Preprocessing")
print(f"Preprocessing_dir path: [ {preprocessing_dir} ]")

if not os.path.exists(preprocessing_dir):
    os.makedirs(preprocessing_dir)
    print(f"Created Preprocessing dir successfully!")
else:
    print(f"Preprocessing dir exists!")

"""## [3.1] - Import thư viện"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(dataset_path)

pd.set_option('display.max_columns', None)

df.head()

"""## [3.2] - Kiểm tra lại giá trị thiêu"""

print("Kiểm tra giá trị thiếu:")
print(df.isnull().sum())

"""## [3.3] - Kiểm tra ngoại lai cho các cột số (sử dụng IQR)"""

numeric_cols_initial = [
    "Age",
    "Purchase Amount (USD)",
    "Review Rating",
    "Previous Purchases",
]

plt.figure(figsize=(15, 5))
for i, col in enumerate(numeric_cols_initial, 1):
    plt.subplot(1, len(numeric_cols_initial), i)
    sns.boxplot(y=df[col])
    plt.title(f"Box Plot của {col}")
plt.tight_layout()

save_path = os.path.join(preprocessing_dir, "Outlier_Check_Numerical_Box_Plot_IQR.png")
plt.savefig(save_path, bbox_inches="tight")
plt.show()

print("Số lượng giá trị duy nhất trong mỗi cột:")
unique_counts = df.nunique()
for col in numeric_cols_initial:
    print(f"- [{col}]: {unique_counts[col]}")

# Dựa trên EDA trước đó, không có ngoại lai đáng kể cần loại bỏ ở bước này.
# Nếu có, có thể xử lý bằng cách loại bỏ hoặc capping (ví dụ: thay thế bằng percentile 1 và 99).
# Ví dụ capping (chưa áp dụng):
# lower_bound = df['Review Rating'].quantile(0.01)
# upper_bound = df['Review Rating'].quantile(0.99)
# df['Review Rating'] = np.where(df['Review Rating'] < lower_bound, lower_bound, df['Review Rating'])
# df['Review Rating'] = np.where(df['Review Rating'] > upper_bound, upper_bound, df['Review Rating'])

print("Kiểm tra ngoại lai bằng phương pháp IQR:")
for col in numeric_cols_initial:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
    print(f"- [{col}]: Số lượng ngoại lai = {len(outliers)}, Giá trị ngoại lai = {outliers.tolist()}")

"""## [3.4] - Mã hóa đặc trưng phân loại

- Sử dụng `One-Hot Encoding` cho các biến có ít giá trị duy nhất và không có thứ tự tự nhiên **(Gender, Category)**.

- Sử dụng `Label Encoding` cho các biến có nhiều giá trị hơn hoặc có thể có thứ tự tiềm ẩn (mặc dù LabelEncoder không bảo toàn thứ tự, nó tiết kiệm chiều dữ liệu).

### 3.4.1) One-hot encoding cho biến **Gender** và **Category**

#### Dữ liệu trước khi one-hot encoding
"""

df.head()

"""#### Dữ liệu sau khi one-hot encoding"""

one_hot_cols = ['Gender', 'Category', ]
df = pd.get_dummies(df, columns=one_hot_cols, prefix=one_hot_cols, dtype=int)

df.head()

"""### 3.4.2) Chuẩn hóa các biến phân loại

#### Danh sách các biến phân loại đem đi chuẩn hóa (encoder
"""

categorical_cols = [
    "Location",
    "Size",
    "Color",
    "Subscription Status",
    "Shipping Type",
    "Discount Applied",
    "Promo Code Used",
    "Payment Method",
]

print(f"Danh sách các biến phân loại đem đi chuẩn hóa (encoder):")
for i, col in enumerate(categorical_cols):
    print(f"{i + 1}. [{col}]")

"""#### Kiểm tra chuẩn hóa"""

le = LabelEncoder()
label_mappings = {}
encoding_success = True

try:
    for col in categorical_cols:
        # Chuẩn hóa cột và thêm vào DataFrame
        df[col + "_encoded"] = le.fit_transform(df[col])
        # Lưu ánh xạ
        label_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))

    # Kiểm tra xem các cột mới có được tạo không
    for col in categorical_cols:
        if col + "_encoded" not in df.columns:
            encoding_success = False
            print(f"Lỗi: Cột {col}_encoded không được tạo.")
            break

except Exception as e:
    encoding_success = False
    print(f"Lỗi trong quá trình chuẩn hóa: {str(e)}")


if encoding_success:
    print("Chuẩn hóa thành công")
    # In ánh xạ để kiểm tra (tùy chọn)
    print("\nÁnh xạ giá trị sau chuẩn hóa:")
    for col, mapping in label_mappings.items():
        print(f"- {col}: {mapping}")
else:
    print("Chuẩn hóa thất bại. Vui lòng kiểm tra lại dữ liệu và quá trình xử lý.")

df.head()

"""### 3.4.3) Kiểm tra tương quan giữa các đặc trưng đã mã hóa (**Redundancy Check**)

#### Danh sách các cột đã mã hóa (encoded)
"""

encoded_cols = [col for col in df.columns if '_encoded' in col or 'Gender_' in col or 'Category_' in col]

print(f"Danh sách các cột đã mã hóa (encoded):")
for i, encoded_col in enumerate(encoded_cols):
    print(f"{i + 1}. [{encoded_col}]")

"""#### Ma trận tương quan giữa các đặc trưng đã mã hóa:"""

corr_matrix_encoded = df[encoded_cols].corr()
corr_matrix_encoded

corr_matrix_encoded_abs = corr_matrix_encoded.abs()
corr_matrix_encoded_abs

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix_encoded, annot=True, cmap='coolwarm', fmt='.2f')
plt.title(f'Ma trận tương quan giữa các đặc trưng đã mã hóa')

save_path = os.path.join(preprocessing_dir, "Correlation_Heatmap_of_Encoded_Features.png")
plt.savefig(save_path, bbox_inches="tight")
plt.show()

"""#### Heatmap cho các đặc trưng có độ tương quan cao"""

high_corr_threshold = 0.8

# Tạo mask để che đường chéo chính và các giá trị lặp lại
mask = np.eye(len(corr_matrix_encoded), dtype=bool)
high_coor = (corr_matrix_encoded_abs > high_corr_threshold) & ~mask

high_corr_matrix = corr_matrix_encoded.where(high_coor)

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(high_corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title(f"High Correlation (>{high_corr_threshold}) Between Encoded Features")

# Save the plot
threshold_str = str(high_corr_threshold)#.replace(".", "")
filename = f"High_Correlation_Heatmap_Encoded_Features_thr{threshold_str}.png"
save_path = os.path.join(preprocessing_dir, filename)
plt.savefig(save_path, bbox_inches="tight")

# Display the plot
plt.show()

"""- `Gender_Female` và `Gender_Male` có hệ số tương quan hoàn hảo là `-1`. Đây là dấu hiệu của tính **dư thừa thông tin**, vì hai biến này hoàn toàn đối lập. Do đó, một trong hai cột nên được loại bỏ để tránh **đa cộng tuyến (multicollinearity)**.

- `Discount Applied_encoded` và `Promo Code Used_encoded` có **hệ số tương quan hoàn hảo** là `+1`. Điều này cho thấy hai cột này **mang thông tin giống hệt nhau**, vì vậy một trong hai cũng nên bị loại bỏ.

#### Loại bỏ cột dư thừa (Gender_Male và Promo Code Used_encoded)
"""

columns_to_drop = ['Gender_Male', 'Promo Code Used_encoded']

for col in columns_to_drop:
    if col in df.columns:
        df = df.drop(columns=[col])
        print(f"Đã loại bỏ cột: {col}")
    else:
        print(f"Cột '{col}' không còn tồn tại trong DataFrame.")

"""## [3.5] - Chuẩn hóa đặc trưng số

- Sử dụng MinMaxScaler để đưa các cột số về khoảng [0, 1], vì dữ liệu không có phân phối chuẩn
- Điều này phù hợp với bài toán đề xuất vì dữ liệu không có ngoại lai (xác nhận ở 3.3).
- Điều này quan trọng cho các thuật toán nhạy cảm với thang đo như KNN, SVM, hoặc các phương pháp dựa trên khoảng cách.

### 3.5.1) Danh sách các cột số cần chuẩn hóa
"""

numeric_cols = ['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases']

print(f"Danh sách các cột số cần chuẩn hóa")
for i, numeric_col in enumerate(numeric_cols):
    print(f"{i+ 1}. [{numeric_col}]")

"""### 3.5.2) Chuẩn hóa"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
df[numeric_cols].head(5)

"""### 3.5.3) Phân bố các đặc trưng đã chuẩn hóa"""

plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df[col], kde=True, bins=30, color='skyblue')
    plt.title(f'Phân phối của {col} (sau MinMaxScaler)', fontsize=12)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Tần suất', fontsize=10)

# Điều chỉnh bố cục
plt.tight_layout()

# Lưu biểu đồ
filename = "MinMaxScaler_Numerical_Histogram_Plots.png"
save_path = os.path.join(preprocessing_dir, filename)
plt.savefig(save_path, bbox_inches="tight")

# Hiển thị biểu đồ
plt.show()

display(df.head())

"""## [3.6] - Tổng kết và Lưu dữ liệu đã xử lý

### **Các bước đã thực hiện:**

1.  Kiểm tra và nhận xét về dữ liệu thiếu/ngoại lai.
3.  Mã hóa biến phân loại (One-Hot và Label Encoding).
4.  Kiểm tra và loại bỏ đặc trưng dư thừa (multicollinearity).
5.  Chuẩn hóa biến số (`MinMaxScaler`).

Dữ liệu đã được tiền xử lý và sẵn sàng cho các bước xây dựng mô hình tiếp theo.

## [3.7] - Path to Save Processed DataFrame
"""

processed_file_path = os.path.join(data_dir, "shopping_behavior_processed.csv")
print(f"Processed_file path: [ {processed_file_path} ]")

try:
    df.to_csv(processed_file_path, index=False)
    print(f"Đã lưu Processed file: [ {processed_file_path} ] đã xử lý vào Data dir: [ {data_dir} ]")

    # Kiểm tra
    if os.path.exists(processed_file_path):
        df_check = pd.read_csv(processed_file_path)
        print("Dữ liệu vừa lưu (5 hàng đầu tiên):")
        display(df_check.head())
    else:
        print(f"File {processed_file_path} không tồn tại!")

except Exception as e:
    print(f"Lỗi khi lưu file: {e}")

"""## [3.8] - Kết nối với Bước 4

Sau khi hoàn thành tiền xử lý dữ liệu cơ bản ở Bước 3, chúng ta đã có một tập dữ liệu sạch với các biến đã được mã hóa và chuẩn hóa. Trong Bước 4 tiếp theo, chúng ta sẽ sử dụng tập dữ liệu này để tạo ra các đặc trưng mới có ý nghĩa hơn cho bài toán đề xuất sản phẩm.

# Bước 4: Xử lý và lựa chọn đặc trưng - Feature Engineering - Feature Selection

## **Mục tiêu**

- Tạo các đặc trưng mới từ dữ liệu đã tiền xử lý ở Bước 3
- Xây dựng các đặc trưng tổng hợp phản ánh hành vi và sở thích người dùng
- Lựa chọn các đặc trưng quan trọng nhất cho mô hình đề xuất
- Chuẩn bị tập dữ liệu cuối cùng cho việc xây dựng mô hình

## **Cấu trúc**

1. Tải dữ liệu đã tiền xử lý
2. Tạo đặc trưng mới (sản phẩm, người dùng, tương tác)
3. Loại bỏ đặc trưng không cần thiết
4. Lựa chọn đặc trưng quan trọng
5. Đánh giá tầm quan trọng của đặc trưng
6. Tổng kết và xuất dữ liệu

## Path to Save Feature Engineering Figures
"""

feature_engineering_dir = os.path.join(figures_dir, "Feature_Engineering")
print(f"Feature_engineering_dir path: [ {feature_engineering_dir} ]")

if not os.path.exists(feature_engineering_dir):
    os.makedirs(feature_engineering_dir)
    print(f"Created Feature Engineering dir successfully!")
else:
    print(f"Feature Engineering dir exists!")

"""## [4.1] - Import thư viện"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import f_regression
from sklearn.preprocessing import StandardScaler
from scipy import stats

pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')

"""## [4.2] - Tải dữ liệu đã tiền xử lý"""

df_processed = pd.read_csv(processed_file_path)
display(df_processed)

df_processed.info()

df_processed.describe()

"""## [4.3] - Tạo đặc trưng mới

### 4.3.1) Tạo đặc trưng liên quan đến sản phẩm

#### Đặc trưng mới - `Product_ID`

**Mục tiêu**: Tạo mã định danh duy nhất cho sản phẩm, vì `Item Purchased` không đủ chi tiết để phân biệt các sản phẩm khác nhau.

**Vấn đề:** Trong dữ liệu gốc, cột `Item Purchased` chỉ chứa tên sản phẩm (như "Blouse", "Sweater") mà không bao gồm thông tin chi tiết như `Category`, `Size`, hoặc `Color`. Điều này gây khó khăn trong việc phân biệt các sản phẩm cụ thể (ví dụ: Blouse màu Gray size L khác với Blouse màu Black size M).

**Giải pháp:** Tạo một định danh sản phẩm duy nhất bằng cách kết hợp các thuộc tính sản phẩm thành một chuỗi:
"""

new_features_dir = os.path.join(feature_engineering_dir, "New_features")
print(f"New_features path: [ {new_features_dir} ]")

os.makedirs(new_features_dir, exist_ok=True)
print(f"Created New_features dir [ {new_features_dir} ] successfully!")

"""##### Tìm tất cả các cột one-hot của Category"""

display(df_processed)

category_columns = [col for col in df_processed.columns if col.startswith('Category_')]
print("Danh sách các cột one-hot của Category:")
for i, cat_col in enumerate(category_columns):
    print(f"{i + 1}. [{cat_col}]")

"""##### Khôi phục các cột đã one-hot của Category"""

# Định nghĩa các giá trị category có thể có
categories = ["Accessories", "Clothing", "Footwear", "Outerwear"]


# Hàm để khôi phục lại giá trị Category từ các cột one-hot
def get_category(row):
    for cat in categories:
        if row.get(f"Category_{cat}", 0) == 1:
            return cat
    return "Unknown"  # Trả về Unknown nếu không tìm thấy


# Tạo cột Category_reconstructed
df_processed["Category_reconstructed"] = df_processed.apply(get_category, axis=1)
display(df_processed["Category_reconstructed"])
display(df_processed["Category_reconstructed"].value_counts())

"""##### Sử dụng cột **Item Purchased** - **Category_reconstructed** - **Size** - **Color** để tạo `Product_ID`"""

df_processed["Product_ID"] = (
    df_processed["Item Purchased"]
    + "_"
    + df_processed["Category_reconstructed"]
    + "_"
    + df_processed["Size"]
    + "_"
    + df_processed["Color"]
)

display(df_processed.head())

"""#### Đặc trưng mới - `Product_Category`

**Mục tiêu:** Tạo một đặc trưng đơn giản hơn để phân loại sản phẩm, kết hợp thông tin từ `Item Purchased` và `Category`, nhưng không bao gồm `Size` và `Color`.

**Sự khác biệt với `Product_ID`:**
- `Product_ID`: Định danh đầy đủ và duy nhất cho mỗi sản phẩm (bao gồm size, color) → Dùng cho Collaborative Filtering
- `Product_Category`: Phân loại sản phẩm ở cấp độ cao hơn → Dùng cho Content-Based và phân tích xu hướng

**Lợi ích:**
- Giảm số chiều dữ liệu so với `Product_ID` mà vẫn giữ đủ thông tin
- Tạo nhóm sản phẩm tương tự giúp phân tích xu hướng mua sắm hiệu quả hơn
- Hỗ trợ cold-start problem bằng cách gợi ý dựa trên loại sản phẩm thay vì sản phẩm cụ thể
- **Collaborative Filtering:** Tạo ma trận User-Item chính xác hơn với các sản phẩm được định danh rõ r
- **Content-Based Filtering:** Giữ nguyên thông tin chi tiết về sản phẩm, giúp tìm sản phẩm tương tự chính xác hơn
- **Gợi ý sản phẩm:** Tránh đề xuất cùng một mặt hàng nhưng không phù hợp về kích thước hoặc màu sắc

Đặc trưng này rất quan trọng cho việc xây dựng hồ sơ sở thích người dùng (user preference profiles) trong Content-Based Filtering. => Dùng cho Xây dựng `User-Item matrix`

##### Sử dung cột **Item Purchased** - **Category_reconstructed** tạo `Product_Category`
"""

df_processed["Product_Category"] = (
    df_processed["Item Purchased"] + "_" + df_processed["Category_reconstructed"]
)

display(df_processed['Product_Category'])

print(f"=> Đã tạo đặc trưng Product_Category.\n - Số lượng danh mục sản phẩm: {df_processed['Product_Category'].nunique()}")

display(df_processed['Product_Category'].value_counts())

"""#### Đặc trưng mới `Dominant_Season`

##### Tính số lần xuất hiện của mỗi Item Purchased trong từng Season lưu vào **product_season**
"""

product_season = df_processed.groupby(['Item Purchased', 'Season']).size().unstack(fill_value=0)
display(product_season)

"""##### Xác định mùa có số lần xuất hiện cao nhất cho mỗi Item và tạo `Dominant_Season` tư **product_season**"""

product_season['Dominant_Season'] = product_season.idxmax(axis=1)
display(product_season['Dominant_Season'])

"""##### Số lượng unique item Purchased"""

print(f"Number of unique items in df_processed: {df_processed['Item Purchased'].nunique()}")

"""##### Số lượng item trong **product_season**"""

print(f"Number of items in product_season: {len(product_season)}")

"""##### Gộp df_processed theo `Item Purchased`"""

if "Dominant_Season" not in df_processed.columns:
    df_processed = df_processed.merge(
        product_season["Dominant_Season"],
        left_on="Item Purchased", # khóa gộp sản phẩm
        right_index=True,
        how="left", # giữ tất cả từ df_processed
        validate="many_to_one", # # Kiểm tra rằng mỗi giá trị trong "Item Purchased" ánh xạ đến tối đa 1 giá trị Dominant_Season
    )

# Kiểm tra dữ liệu sau merge
print(
    f"Số giao dịch thiếu Dominant_Season: {df_processed['Dominant_Season'].isna().sum()}"
)

"""##### Số lượng giao dịch theo Dominant_Season"""

print("Số lượng giao dịch theo Mùa phổ biến của sản phẩm:")
print(df_processed['Dominant_Season'].value_counts())

plt.figure(figsize=(8, 6))
sns.countplot(
    data=df_processed,
    x="Dominant_Season",
    hue="Dominant_Season",
    palette="Set2",
    order=["Spring", "Summer", "Fall", "Winter"],
    legend=False,
)
plt.title("Phân bố Mùa phổ biến (Dominant Season) của các giao dịch", fontsize=12)
plt.xlabel("Mùa phổ biến của sản phẩm được mua", fontsize=10)
plt.ylabel("Số lượng giao dịch", fontsize=10)
plt.tight_layout()

save_path = os.path.join(new_features_dir, "dominant_season_distribution.png")
plt.savefig(save_path)
plt.show()

"""#####  Season và Dominant_Season truớc khi mã hóa bằng One-hot Encoding"""

display(df_processed)

"""##### Mã hóa Season và Dominant_Season bằng One-hot Encoding"""

df_processed = pd.get_dummies(df_processed, columns=['Dominant_Season'], prefix='Dominant_Season', dtype=int)
print("Dữ liệu sau khi mã hóa Dominant_Season:")
display(df_processed.head())

"""**Mục tiêu**: Xác định mùa phổ biến nhất cho mỗi loại mặt hàng (`Item Purchased`), giúp hiểu rõ hơn về tính thời vụ của sản phẩm.
- **Vấn đề:**
  - Mùa ảnh hưởng mạnh đến lựa chọn sản phẩm (ví dụ: áo khoác vào Winter), giúp gợi ý chính xác hơn.
- **Ý nghĩa:**
  - Đặc trưng này có thể hữu ích cho việc gợi ý sản phẩm theo ngữ cảnh mùa hiện tại.
  - Ví dụ: Nếu đang là mùa đông, hệ thống có thể ưu tiên gợi ý các sản phẩm có `Dominant_Season` là 'Winter'.

### 4.3.2) Tạo đặc trưng liên quan đến tương tác

#### Đặc trưng mới - `Customer_Loyalty_Score`

##### **Mục tiêu**:

Tạo một điểm số tổng hợp thể hiện mức độ trung thành của khách hàng, kết hợp số lần mua trước, tần suất mua và trạng thái đăng ký.

##### **Lưu ý về trọng số:**

- Các trọng số (0.4, 0.4, 0.2) được chọn dựa trên phán đoán heuristic. Trong thực tế, các trọng số này có thể được tối ưu hóa bằng các kỹ thuật như phân tích thành phần chính (PCA) hoặc dựa trên kết quả đánh giá mô hình (ví dụ: grid search nếu điểm số này được dùng trực tiếp trong mô hình).

##### Danh sách tần suất mua hàng hiện tại
"""

for freq in df_processed['Frequency of Purchases'].unique():
    print(f"[{freq}]")

"""##### Gán điểm số cho **Frequency of Purchases** dựa trên tân suất (cao hơn => thường xuyên hơn)"""

frequency_ranking = {
    "Annually": 1,
    "Quarterly": 2,
    "Every 3 Months": 2,
    "Monthly": 3,
    "Fortnightly": 4,
    "Weekly": 5,
    "Bi-Weekly": 6,
}

for item in frequency_ranking:
    print(f"[{item}] -> {frequency_ranking[item]}")

df_processed['Frequency_score'] = df_processed['Frequency of Purchases'].map(frequency_ranking)
display(df_processed['Frequency_score'])
print(f"Số lượng tần suất sau khi gán điểm:\n {df_processed['Frequency_score'].value_counts()}")

"""##### Chuẩn hóa điểm tần suất để có thể kết hợp với các biến đã chuẩn hóa khác"""

from sklearn.preprocessing import MinMaxScaler
scaler_freq = MinMaxScaler()
df_processed['Frequency_score'] = scaler_freq.fit_transform(df_processed[['Frequency_score']])
display(df_processed['Frequency_score'])

"""##### Tính `Customer_Loyalty_Score` từ **Previous Purchases** - **Frequency_score** - **Subscription Status_encoded**

- Tính điểm trung thành - sử dụng các biến đã được chuẩn hóa/mã hóa
- Previous Purchases đã được chuẩn hóa (mean 0, std 1)
- Frequency_score_scaled đã được chuẩn hóa (mean 0, std 1)
- Subscription Status_encoded là 0 (No) hoặc 1 (Yes)
- Lưu ý: Trọng số 0.4, 0.4, 0.2 là heuristic, có thể cần tối ưu hóa.
"""

df_processed["Customer_Loyalty_Score"] = (
    df_processed["Previous Purchases"] * 0.4
    + df_processed["Frequency_score"] * 0.4
    + df_processed["Subscription Status_encoded"] * 0.2
)

display(df_processed["Customer_Loyalty_Score"])

plt.figure(figsize=(8, 6))
sns.histplot(df_processed["Customer_Loyalty_Score"], kde=True, bins=30, color="skyblue")
plt.title("Phân phối của Customer Loyalty Score", fontsize=12)
plt.xlabel("Loyalty Score", fontsize=10)
plt.ylabel("Tần suất", fontsize=10)

save_path = os.path.join(
    new_features_dir, "Customer_Loyal_Score_distribution_MinMax.png"
)
plt.tight_layout()
plt.savefig(save_path)

plt.show()

display(df_processed.head())

"""#### Đặc trưng mới - `Interaction_Score`

##### **Mục tiêu**:

- Tạo ma trận biểu diễn sự tương tác giữa người dùng và sản phẩm, cần thiết cho các thuật toán lọc cộng tác.

- Tạo `Interaction_Score`: Kết hợp `Review Rating` và `Purchase Amount (USD)` (đã chuẩn hóa) để thể hiện mức độ tương tác/quan tâm của người dùng đối với sản phẩm.

##### **Lưu ý về trọng số:**

- Trọng số 0.7 cho `Review Rating` và 0.3 cho `Purchase Amount` là heuristic, ưu tiên đánh giá của người dùng. Có thể điều chỉnh hoặc sử dụng các phương pháp khác để xác định điểm tương tác (ví dụ: chỉ dùng rating, hoặc dùng mô hình phức tạp hơn).

##### Tính `Interaction_Score` sử dụng các cột đã chuẩn hóa **Review Rating** - **Purchase Amount (USD)**
"""

# Lưu ý: Trọng số 0.7 và 0.3 là heuristic, có thể cần tối ưu hóa.
df_processed['Interaction_Score'] = (df_processed['Review Rating'] * 0.7 + df_processed['Purchase Amount (USD)'] * 0.3)
display(df_processed['Interaction_Score'])

plt.figure(figsize=(8, 6))
sns.histplot(df_processed["Interaction_Score"], kde=True, bins=30, color="lightgreen")
plt.title("Phân phối của Interaction Score (MinMaxScaler)", fontsize=12)
plt.xlabel("Interaction Score", fontsize=10)
plt.ylabel("Tần suất", fontsize=10)

save_path = os.path.join(
    new_features_dir, "Interaction_Score_distribution_MinMax.png"
)
plt.tight_layout()
plt.savefig(save_path)

plt.show()

"""#### Đặc trưng mới - `User-Item` matrix

##### Tạo `User-Item` matrix từ **Interaction_Score** - **Customer ID** - **Product_Category**
"""

user_item_matrix = df_processed.pivot_table(
    index="Customer ID",            # Mỗi dòng là ID
    columns="Product_Category",     # Mỗi cột là Item + category
    values="Interaction_Score",     # Mỗi ô là giá trị tương tác giữ người dùng
    aggfunc="mean",                 # Nếu có nhiều tương tác giữa 1 user & item => Lấy trung bình
).fillna(0)

display(user_item_matrix.head(10))

"""##### Kích thước ma trận `User-Item`"""

print(f"Kích thước ma trận User-Item: {user_item_matrix.shape}")

"""##### Hiển thị một phần nhỏ của heatmap do ma trận User-Item lớn"""

top_N = 40
plt.figure(figsize=(12, 8))
sns.heatmap(user_item_matrix.iloc[:top_N], cmap='Blues', cbar=True)
plt.title(f'Heatmap của ma trận User-Item ({top_N} users)')
plt.xlabel('Product_ID')
plt.ylabel('Customer ID')

plt.tight_layout()
save_path = os.path.join(new_features_dir, "Heatmap_User_Item_Matrix.png")
plt.savefig(save_path)
plt.show()

"""##### Tạo tập tin chứa ma trận `User-Item`  và lưu trong **Data_dir**"""

matrix_file_path = os.path.join(data_dir, "user_item_matrix.csv")
try:
    user_item_matrix.to_csv(matrix_file_path)
    print(f"Đã lưu ma trận User-Item: [ {matrix_file_path} ] vào Data_dir: [ {data_dir} ]")
except Exception as e:
    print(f"Lỗi khi lưu file: {e}")

"""## [4.4] - Loại bỏ đặc trưng **không cần thiết** hoặc **dư thừa**

### **Lưu ý**
"""

unnecessary_features_dropping_dir = os.path.join(feature_engineering_dir, "unnecessary_features_dropping")
print(f"Unnecessary_features_dropping path: [ {unnecessary_features_dropping_dir} ]")

os.makedirs(unnecessary_features_dropping_dir, exist_ok=True)
print(f"Created Unnecessary Features Dropping dir [ {unnecessary_features_dropping_dir} ] successfully!")

"""### 4.4.1) Loại bỏ các cột không cần thiết"""

cols_to_drop = [
    'Promo Code Used',
    'Gender', 'Category', 'Location', 'Size', 'Color',
    'Subscription Status', 'Payment Method', 'Shipping Type', 'Discount Applied',
    'Frequency of Purchases', 'Frequency_score', 'Product_ID', 'Purchase Amount (USD)',
    'Season'
]

cols_to_drop_existing = [col for col in cols_to_drop if col in df_processed.columns]
print("Các cột không cần thiết cần loại bỏ:")
for i, col in enumerate(cols_to_drop_existing):
    print(f"{i + 1}. [{col}]")

df_final_features = df_processed.drop(columns=cols_to_drop_existing)
print(f"Đã loại bỏ {len(cols_to_drop_existing)} cột.")

"""### 4.4.2) Các còn lại sau khi loại"""

print("Các cột còn lại:")
for final_col in df_final_features.columns.tolist():
    print(f"[{final_col}]")
print(f"Số cột còn lại sau khi loại bỏ: {len(df_final_features.columns)}")
display(df_final_features.head())

"""### 4.4.3) Kiểm tra các đặc trưng có phương sai cao (High Variance)

- Các đặc trưng có phương sai thấp (gần như hằng số) không cung cấp nhiều thông tin hữu ích cho mô hình và có thể bị loại bỏ.
"""

numeric_features = df_final_features.select_dtypes(include=['int32', 'int64', 'float64'])
display(numeric_features)

variances = numeric_features.var()
display(variances)

variances_df = pd.DataFrame({'Feature': variances.index, 'Variance': variances.values})
display(variances_df)

"""#### 10 đặc trưng có phương sai cao nhất"""

variances_df = variances_df.sort_values('Variance', ascending=False)
print("10 đặc trưng có phương sai cao nhất trong df_final_features:")
display(variances_df)

"""#### Ngưỡng phương sai"""

# Loại bỏ các đặc trưng có phương sai cao nhưng giữ lại Customer ID
high_variance_threshold = 1
high_variance_features = variances_df[
    (variances_df["Variance"] > high_variance_threshold)
]["Feature"].tolist()

print(f"\nCác đặc trưng có phương sai > {high_variance_threshold}:")
print(high_variance_features)

"""**Nhận xét:**
- Phân tích phương sai giúp xác định các đặc trưng có ít sự biến thiên.
- Các đặc trưng có phương sai thấp như `Category_*` có thể do chúng là biến one-hot.
- Cẩn thận với các biến one-hot vì độ phổ biến thấp không đồng nghĩa với không quan trọng.

#### Phân phối của các đặc trưng có phương sai cao
"""

if high_variance_features:
    plt.figure(figsize=(15, 5 * min(3, len(high_variance_features))))
    for i, feature in enumerate(high_variance_features[:3]):
        plt.subplot(min(3, len(high_variance_features)), 1, i+1)
        sns.histplot(df_processed[feature], kde=True)
        plt.title(f'Phân phối của {feature} (Variance: {variances[feature]:.6f})')
    plt.tight_layout()
    save_path = os.path.join(unnecessary_features_dropping_dir, "features_with_high_variaces")
    plt.savefig(save_path)
    plt.show()
else:
    print("Không có đặc trưng nào có phương sai cao hơn ngưỡng.")

"""**Nhận xét:**
- Phân tích phương sai giúp chúng ta xác định các đặc trưng có rất ít sự biến thiên (gần như hằng số).
- Các đặc trưng có phương sai thấp như `Category_*` có thể do chúng là biến one-hot, chỉ nhận giá trị 0 hoặc 1, và sự phân bố không đồng đều giữa các danh mục.
- Nếu có bất kỳ đặc trưng nào có phương sai gần như bằng 0, chúng ta nên cân nhắc loại bỏ vì chúng không cung cấp thông tin phân biệt.
- Tuy nhiên, cần cẩn thận với các biến one-hot vì độ phổ biến thấp của một danh mục không đồng nghĩa với việc nó không quan trọng.

#### Kiểm tra đặc trưng nhị phân trong df_final_features
"""

binary_features = [col for col in df_final_features.columns if set(df_final_features[col].unique()).issubset({0, 1})]
print("\nCác đặc trưng nhị phân trong df_final_features:", binary_features)

# Tỷ lệ giá trị phổ biến nhất trong các đặc trưng nhị phân
print("\nTỷ lệ giá trị phổ biến nhất trong các đặc trưng nhị phân:")
for col in binary_features:
    value_counts = df_final_features[col].value_counts(normalize=True)
    most_common_value = value_counts.index[0]
    most_common_ratio = value_counts.iloc[0]
    print(f"{col}: {most_common_value} ({most_common_ratio:.2%})")

"""#### Loại bỏ các đặc trưng có phương sai cao (bảo vệ đặc trưng Category_*)"""

additional_cols_to_drop = [col for col in high_variance_features if not col.startswith('Category_') and col in df_final_features.columns]

print(f"\nĐặc trưng đề xuất loại bỏ (phương sai cao) (ngoại trừ Category_*): {additional_cols_to_drop}")

if additional_cols_to_drop:
    df_final_features = df_final_features.drop(columns=additional_cols_to_drop)
    print(f"Đã loại bỏ thêm {len(additional_cols_to_drop)} đặc trưng. Số đặc trưng còn lại: {len(df_final_features.columns)}")
else:
    print("Không có đặc trưng nào được đề xuất loại bỏ thêm.")

# Hiển thị các cột còn lại
print("\nCác cột còn lại trong df_final_features:")
for final_col in df_final_features.columns.tolist():
    print(f"[{final_col}]")
print(f"Số cột còn lại sau khi loại bỏ: {len(df_final_features.columns)}")

"""## [4.5] - Lựa chọn đặc trưng (Feature Selection)"""

features_selection_dir = os.path.join(feature_engineering_dir, "features_selection")
print(f"Features_selection path: [ {features_selection_dir} ]")

os.makedirs(features_selection_dir, exist_ok=True)
print(f"Created Features Selection dir [ {features_selection_dir} ] successfully!")

"""### 4.5.1) Dựa vào Ma trận tương quan (Correlation Matrix) giữa các biến cuối cùng"""

plt.figure(figsize=(18, 12))
correlation_matrix_final = df_final_features.corr(numeric_only=True)
display(correlation_matrix_final)
sns.heatmap(
    correlation_matrix_final,
    annot=True,
    cmap="coolwarm",
    fmt=".2f",
    linewidths=0.5,
    annot_kws={"size": 8},
)

plt.title("Ma trận Tương quan giữa các đặc trưng cuối cùng", fontsize=16)
plt.yticks(rotation=0)

correlation_matrix_dir = os.path.join(features_selection_dir, "Correlation_Matrix")

os.makedirs(correlation_matrix_dir, exist_ok=True)

save_path = os.path.join(
    correlation_matrix_dir, "Heapmap_correlation_final_features_plot.png"
)
plt.savefig(save_path)

plt.tight_layout()
plt.show()

"""#### Tìm các cặp tương quan cao"""

# Đặt ngưỡng tương quan
threshold = 0.8
corr_pairs = correlation_matrix_final.unstack().reset_index()
corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']
corr_pairs = corr_pairs[corr_pairs['Feature 1'] != corr_pairs['Feature 2']]

# Lấy giá trị tuyệt đối của ma trận tương quan
corr_pairs['Abs Correlation'] = corr_pairs['Correlation'].abs()
strong_corr_pairs = corr_pairs[corr_pairs['Abs Correlation'] > threshold].sort_values(by='Abs Correlation', ascending=False)
strong_corr_pairs = strong_corr_pairs.drop_duplicates(subset=['Abs Correlation'])

# Hiển thị các cặp có tương quan cao
print(f"Các cặp tương quan mạnh (theo giá trị tuyệt đối, threshold = {threshold}):")
print(strong_corr_pairs[['Feature 1', 'Feature 2', 'Correlation']])

"""#### Tìm các cặp độ tương quan thấp (tham khảo)"""

low_threshold = 0.3  # Chỉ để kiểm tra, không dùng để loại bỏ
low_corr_pairs = corr_pairs[(corr_pairs['Abs Correlation'] < low_threshold) & (corr_pairs['Abs Correlation'] > 0)].sort_values(by='Abs Correlation')
low_corr_pairs = low_corr_pairs.drop_duplicates(subset=['Abs Correlation'])
print(f"\nCác cặp tương quan thấp (theo giá trị tuyệt đối, threshold < {low_threshold}):")
print(low_corr_pairs[['Feature 1', 'Feature 2', 'Correlation']])

"""#### Quyết định loại bỏ đặc trưng (chỉ loại bỏ tương quan cao)"""

cols_to_drop_corr = []
for _, row in strong_corr_pairs.iterrows():
    feature1, feature2 = row["Feature 1"], row["Feature 2"]
    # Giữ Interaction_Score vì là đặc trưng tổng hợp
    if feature1 == "Interaction_Score":
        cols_to_drop_corr.append(feature2)
    elif feature2 == "Interaction_Score":
        cols_to_drop_corr.append(feature1)

cols_to_drop_corr = list(set(cols_to_drop_corr))
print(f"\nĐặc trưng đề xuất loại bỏ (tương quan cao): {cols_to_drop_corr}")

if cols_to_drop_corr:
    df_final_features = df_final_features.drop(columns=cols_to_drop_corr)
    print(f"Đã loại bỏ {len(cols_to_drop_corr)} đặc trưng do tương quan cao. Số đặc trưng còn lại: {len(df_final_features.columns)}")
else:
    print("Không có đặc trưng nào được đề xuất loại bỏ do tương quan cao.")

print("\nCác cột còn lại trong df_final_features:")
for final_col in df_final_features.columns.tolist():
    print(f"[{final_col}]")
print(f"Số cột còn lại sau khi loại bỏ: {len(df_final_features.columns)}")

""""**Nhận xét:**
- Ma trận tương quan cho thấy hầu hết các đặc trưng có độ tương quan thấp với nhau, với ngưỡng tương quan cao (> 0.8) chỉ xuất hiện giữa `Interaction_Score` và `Review Rating`, dẫn đến việc loại bỏ `Review Rating` để tránh đa cộng tuyến vì `Interaction_Score` được xây dựng một phần dựa trên `Review Rating`.


- Không có cặp tương quan nghịch đảo hoàn hảo (như `Gender_Female` và `Gender_Male`) cần xử lý thêm, vì `Gender_Male` đã được loại bỏ ở các bước trước.

- Các đặc trưng như `Purchase Amount (USD)` và `Customer_Loyalty_Score` có tương quan rất thấp (gần 0), cho thấy chúng mang thông tin độc lập và hữu ích.

- `Interaction_Score` không cho thấy tương quan mạnh với các đặc trưng khác ngoài `Review Rating`, điều này phù hợp vì nó được tổng hợp từ nhiều yếu tố, trong đó `Review Rating` là thành phần chính.

- Nhìn chung, không có dấu hiệu rõ ràng về đa cộng tuyến nghiêm trọng sau khi loại bỏ `Review Rating`, và các đặc trưng còn lại có thể được giữ lại cho các bước phân tích tiếp theo."

### 4.5.2) Dựa vào Model-Based Methods (Feature Importance)

- Sử dụng RandomForestRegressor để đánh giá tầm quan trọng của các đặc trưng trong việc dự đoán `Interaction_Score` (một proxy cho mức độ yêu thích sản phẩm).
"""

feature_importances_dir = os.path.join(features_selection_dir, "Feature_importances")

os.makedirs(feature_importances_dir, exist_ok=True)

"""#### Chuẩn hóa tên cột trong df_final_features để khớp với X_model"""

df_final_features.columns = df_final_features.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)

"""#### Kiểm tra các cột sau khi đồng bộ"""

print("Các cột trong df_final_features sau khi đồng bộ:")
for col in df_final_features.columns.tolist():
    print(f"[{col}]")
print(f"Số cột trong df_final_features: {len(df_final_features.columns)}")
display(df_final_features)

"""#### Loại bỏ các cột string và các cột không dùng cho model

##### Xác định các cột có kiểu dữ liệu object (chuỗi) trong df_final_features
"""

string_columns = [col for col in df_final_features.columns if df_final_features[col].dtype == 'object']
print("Các cột string tự động phát hiện:", string_columns)

"""##### Loại bỏ hoặc thêm các cột string cố định nếu cần (nếu bạn muốn giữ một số cột cụ thể)"""

string_columns = list(set(string_columns))  # Loại bỏ trùng lặp
string_columns = [col for col in string_columns if col in df_final_features.columns]
print("Các cột string sẽ loại bỏ:", string_columns)

"""#### Chọn các đặc trưng cho mô hình (loại bỏ Interaction_Score để tránh rò rỉ dữ liệu)"""

model_features = [col for col in df_final_features.columns if col not in string_columns and col != 'Interaction_Score']
print("Các đặc trưng được chọn cho mô hình:", model_features)

"""#### Tạo X_model từ df_final_features đã đồng bộ"""

X_model = df_final_features[model_features]
print("Các cột trong X_model:", list(X_model.columns))

"""#### Kiểm tra kiểu dữ liệu của X_model để đảm bảo không còn chuỗi"""

print("Kiểu dữ liệu của các cột trong X_model:")
print(X_model.dtypes)

"""#### Chọn biến mục tiêu"""

y = df_final_features['Interaction_Score']
print("\nBiến mục tiêu (Interaction_Score) - 10 dòng đầu tiên:")
print(y.head(10))

"""#### Random Forest đánh giá tầm quan trọng của các đặc trưng trong việc dự đoán `Interaction_Score`"""

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
print("Mô hình Random Forest đã được huấn luyện.")
rf.fit(X_model, y)

"""#### Lấy tầm quan trọng của các đặc trưng"""

importances = rf.feature_importances_
feature_names = X_model.columns

"""#### Tạo DataFrame cho Feature Importance"""

feature_importances_df = pd.DataFrame({'Features': feature_names, 'Importances': importances})
print("Trước khi sắp xếp:")
print(feature_importances_df)

feature_importances_df = feature_importances_df.sort_values(by='Importances', ascending=False)
print("Sau khi sắp xếp giảm dần:")
print(feature_importances_df)

"""#### Hiển thị Top 10 đặc trưng quan trọng trong việc dự đoán `Interaction_Score`"""

print("\nTop 10 đặc trưng quan trọng nhất trong dự đoán Interaction Score:")
print(feature_importances_df.head(10))

"""#### Biểu đồ Top 10 Feature Importance"""

plt.figure(figsize=(12, 8))
sns.barplot(
    x="Importances",
    y="Features",
    data=feature_importances_df.head(10),
    palette="viridis",
    hue="Features",
    legend=False,
)
plt.title(
    "Top 10 Feature Importances từ Random Forest (dự đoán Interaction_Score)",
    fontsize=16,
)
plt.xlabel("Importance Score")
plt.ylabel("Features")

plt.tight_layout()
# Lưu biểu đồ
save_path = os.path.join(feature_importances_dir, "Top10_Feature_Importances_plot.png")
plt.savefig(save_path, dpi=300, bbox_inches="tight")
print(f"Đã lưu biểu đồ vào: {save_path}")

plt.show()

"""#### Cải thiện biểu diễn Feature Importance (theo phần trăm)"""

# Tính tầm quan trọng theo phần trăm
importances_pct = importances / np.sum(importances) * 100
feature_importances_pct = pd.DataFrame(
    {"Features": feature_names, "Importances (%)": importances_pct}
)

feature_importances_pct = feature_importances_pct.sort_values(
    by="Importances (%)", ascending=False
)
display(feature_importances_pct)

"""#### Vẽ biểu đồ Top 10 đặc trưng quan trọng nhất (theo %)"""

plt.figure(figsize=(14, 10))
ax = sns.barplot(
    x="Importances (%)",
    y="Features",
    data=feature_importances_pct.head(10),
    palette="viridis",
    hue="Features",
    legend=False,
)
for i, v in enumerate(feature_importances_pct.head(10)["Importances (%)"]):
    if v >= 0.1:
        ax.text(v + 0.3, i, f"{v:.2f}%", va="center")
plt.title(
    "Top 10 Feature Importances (%) for Predicting Interaction_Score", fontsize=16
)
plt.xlabel("Importances (%)")
plt.ylabel("Features")
plt.xlim(0, max(importances_pct) * 1.1)

plt.tight_layout()
# Lưu biểu đồ
save_path = os.path.join(
    feature_importances_dir, "Top10_Feature_Importances_Percentage_plot.png"
)
plt.savefig(save_path, dpi=300, bbox_inches="tight")
print(f"Đã lưu biểu đồ vào: {save_path}")

plt.show()

"""#### Tính tầm quan trọng tích lũy"""

feature_importances_pct["Cumulative Importances (%)"] = feature_importances_pct[
    "Importances (%)"
].cumsum()

features_for_95pct = len(
    feature_importances_pct[feature_importances_pct["Cumulative Importances (%)"] <= 95]
)

print(
    f"\nChỉ cần {features_for_95pct + 1} đặc trưng hàng đầu là đủ để giải thích 95% độ quan trọng tổng cộng."
)

print(f"\nTop {features_for_95pct + 1} đặc trưng quan trọng nhất:")
print(feature_importances_pct.head(features_for_95pct + 1))

"""#### Lưu kết quả cuối cùng"""

rf_features = feature_importances_pct.head(12)['Features'].tolist()
print(rf_features)

"""**Nhận xét:**
- Random Forest xác định `Customer_Loyalty_Score`, `Age`, và `PreviousPurchases` là ba đặc trưng quan trọng nhất để dự đoán `Interaction_Score`, cho thấy các yếu tố về lòng trung thành khách hàng và lịch sử mua sắm đóng vai trò quan trọng trong mức độ tương tác của khách hàng.

- Các đặc trưng khác như `Size_encoded`, `Dominant_Season_Fall`, và `DiscountApplied_encoded` có đóng góp đáng kể (trên 2%), nhưng mức độ quan trọng giảm dần, trong khi các đặc trưng như `Category_Outerwear` và `SubscriptionStatus_encoded` có mức độ ảnh hưởng thấp hơn (dưới 1.5%).

- Đặc biệt, các đặc trưng one-hot của `Category` và `Gender_Female` có tầm quan trọng tương đối thấp, có thể do chúng không trực tiếp ảnh hưởng tuyến tính đến `Interaction_Score`, nhưng vẫn có thể hữu ích trong các mô hình khuyến nghị dựa trên ngữ cảnh.

- Với 12 đặc trưng hàng đầu giải thích 95% độ quan trọng tổng cộng, mô hình có thể được đơn giản hóa bằng cách loại bỏ các đặc trưng có tầm quan trọng rất thấp (như `Category_Outerwear`), nhưng trong bài toán Recommendation, việc giữ lại các đặc trưng ngữ cảnh (như mùa, danh mục) là cần thiết để tăng độ chính xác.

### 4.5.3) Dựa vào Statistical Tests (ANOVA F-test)

Sử dụng ANOVA F-test để kiểm tra mối quan hệ thống kê giữa từng đặc trưng đầu vào (số hoặc đã mã hóa, bao gồm cả `Season` nếu đã mã hóa) và biến mục tiêu `Interaction_Score`.
"""

anova_f_test_dir = os.path.join(features_selection_dir, "ANOVA_F_test")
os.makedirs(anova_f_test_dir, exist_ok=True)

"""#### Chọn các đặc trưng số từ X_model cho ANOVA F-test"""

X_numeric = X_model.select_dtypes(include=['float64']) #  # Chỉ chọn các cột số học
display(X_numeric)

print("Các cột số học trong X_numeric:", X_numeric.columns.tolist())

"""#### Tạo dữ liệu thực hiện"""

X = X_numeric
y = df_final_features['Interaction_Score']

"""#### Thực hiện ANOVA F-Test"""

from sklearn.feature_selection import f_regression
f_values, p_values = f_regression(X, y)

"""#### Tạo DataFrame cho kết quả"""

anova_results = pd.DataFrame({
    "Features": X.columns,
    "F-Values": f_values,
    "p-values": p_values
})

anova_results = anova_results.sort_values(by="F-Values", ascending=False)
print("\nKết quả ANOVA F-Test:")
print(anova_results)

"""#### Vẽ biểu đồ F-Score"""

plt.figure(figsize=(12, 8))
bar_plot = sns.barplot(
    x="F-Values",
    y="Features",
    data=anova_results,
    palette="viridis",
    hue="Features",
    legend=False,
)
plt.title("Top Features by ANOVA F-Value", fontsize=16)
plt.xlabel("F-Value", fontsize=12)
plt.ylabel("Features", fontsize=12)

# Thêm nhãn giá trị F-values
for p in bar_plot.patches:
    width = p.get_width()
    plt.text(
        width + 0.1,
        p.get_y() + p.get_height() / 2,
        f"{width:.2f}",
        ha="left",
        va="center",
        fontsize=10,
        color="black",
    )

plt.tight_layout()
save_path = os.path.join(anova_f_test_dir, "Top_ANOVA_F_Value_plot.png")
plt.savefig(save_path, dpi=300, bbox_inches="tight")
plt.show()

"""#### Lọc các đặc trưng có ý nghĩa thống kê `(p-value < 0.1)`"""

significant_features_anova = anova_results[anova_results['p-values'] < 0.1]['Features'].tolist()

if (significant_features_anova):
    print("\nCác đặc trưng có ý nghĩa thống kê (p-value < 0.1) với Interaction_Score:")
    print(significant_features_anova)
else:
    print("Không có đặc trưng nào thỏa (p < 0.1) với đặc trưng Interaction_Score")

"""#### Biểu đồ các đặc trưng sau khi lọc `(p < 0.1)`"""

if len(anova_results[anova_results['p-values'] < 0.1]) > 0:
    plt.figure(figsize=(12, 8))
    sns.barplot(x='p-values', y='Features', data=anova_results[anova_results['p-values'] < 0.1],
                palette='viridis', hue='Features', legend=False)
    plt.title('Top Features by ANOVA (p-values < 0.1)', fontsize=16)
    plt.xlabel('p-values', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.tight_layout()
    save_path = os.path.join(anova_f_test_dir, "Top_ANOVA_p_values_plot.png")
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()
else:
    print("Không vẽ biểu đồ p-values vì không có đặc trưng nào thỏa mãn p < 0.1.")

"""**Nhận xét:**
- Kết quả ANOVA F-test cho thấy không có đặc trưng số học nào (`Age`, `PreviousPurchases`, `Customer_Loyalty_Score`) có mối quan hệ tuyến tính có ý nghĩa thống kê với `Interaction_Score` ở ngưỡng p-value < 0.1, với giá trị p cao nhất là 0.133705 (`Age`). Điều này có thể phản ánh rằng mối quan hệ giữa các đặc trưng này và `Interaction_Score` không phải là tuyến tính đơn giản.

- So với Feature Importance từ Random Forest, `Customer_Loyalty_Score` và `Age` có tầm quan trọng cao, nhưng ANOVA không xác nhận ý nghĩa thống kê, gợi ý rằng chúng có thể ảnh hưởng thông qua các tương tác phức tạp hơn.

- Các đặc trưng phân loại (như `Gender_Female`, `Category_*`) không được đánh giá trong ANOVA vì chúng đã được mã hóa one-hot và không thuộc nhóm số học, nhưng có thể được kiểm tra bằng Chi-square Test ở bước tiếp theo.

- Kết quả này bổ sung cho Feature Importance, nhấn mạnh rằng các phương pháp thống kê khác (như Chi-square) cần được kết hợp để đánh giá đầy đủ các đặc trưng phân loại.

### 4.5.4) - Dựa vào Chi-square Test

Sử dụng **Chi-square test** để đánh giá mối quan hệ giữa các đặc trưng phân loại và biến mục tiêu, ở đây chúng ta sẽ dùng `Dominant_Season_` làm biến mục tiêu để dữ đoán
"""

import os
chi_square_test_dir = os.path.join(features_selection_dir, "Chi_Square_Test")
os.makedirs(chi_square_test_dir, exist_ok=True)

"""#### Sử dụng `Dominant_Season_` đã phân loại từ 4.6.4 (nếu có) hoặc tạo mới"""

from scipy.stats import chi2_contingency

"""#### Định nghĩa các đặc trưng phân loại"""

X_categories = X_model.select_dtypes(include=['int64', 'int32'])
print(X_categories.columns.tolist())

categorical_features = [
    col
    for col in X_categories.columns
    if not col.startswith('Dominant_Season_')
]

print("\nCác đặc trưng phân loại (không bao gồm Dominant_season_*):", categorical_features)

"""#### Sử dụng Dominant_season làm biến mục tiêu (lấy giá trị max từ one-hot)"""

dominant_season_features = [
    col for col in X_categories.columns
    if col.startswith("Dominant_Season_")
]

dominant_season_target = df_final_features[dominant_season_features].idxmax(axis=1).str.replace('Dominant_Season_', '')
print("Phân phối Dominant_season_target:")
print(dominant_season_target.value_counts())

"""#### Thực hiện Chi-square Test và phân tích kết quả"""

chi2_results = {}
for feature in categorical_features:
    contingency_table = pd.crosstab(df_final_features[feature], dominant_season_target) # Tạo bảng chéo
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    chi2_results[feature] = {"chi2": chi2_stat, "p-value": p_val}

"""#### Tạo DataFrame cho kết quả Chi-square"""

chi2_results_df = pd.DataFrame.from_dict(chi2_results, orient="index")
chi2_results_df = chi2_results_df.sort_values("chi2", ascending=False)

print("\nKết quả Chi-square Test:")
display(chi2_results_df)

"""#### Hiển thị kết quả"""

print("\nKết quả Chi-square Test (top 10):")
display(chi2_results_df.head(10))

p_value_threshold = 0.1
significant_features_chi2 = chi2_results_df[chi2_results_df['p-value'] < p_value_threshold].index.tolist()
print(f"\nĐặc trưng có ý nghĩa thống kê (p < 0.1): {len(significant_features_chi2)} features")
for feature in significant_features_chi2:
    print(f"- {feature}")

"""#### Vẽ biểu đồ Chi-square Values (giới hạn top 10)"""

if not significant_features_chi2:
        print("Không có đặc trưng nào thỏa mãn ngưỡng p-value. Biểu đồ sẽ không được vẽ.")
else:
    filtered_data = chi2_results_df.loc[significant_features_chi2].sort_values("chi2", ascending=False)

    plt.figure(figsize=(10, 6))
    bar_plot = sns.barplot(
        x="chi2",
        y=filtered_data.index,
        hue="chi2",
        data=filtered_data.reset_index(),
        palette="viridis",
    )
    plt.title("Chi-square Values của các đặc trưng có ý nghĩa thống kê", fontsize=16)
    plt.xlabel("Chi-square Value", fontsize=12)
    plt.ylabel("Features", fontsize=12)

    for p in bar_plot.patches:
        width = p.get_width()
        plt.text(
            width + 0.1,
            p.get_y() + p.get_height() / 2,
            f"{width:.2f}",
            ha="left",
            va="center",
            fontsize=10,
            color="black",
        )

    plt.tight_layout()
    save_path = os.path.join(chi_square_test_dir, "Chi2_vs_SignificantFeatures_plot.png")
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    print(f"\nĐã vẽ biểu đồ cho {len(significant_features_chi2)} đặc trưng có ý nghĩa thống kê (p < {p_value_threshold}):")
    for feature in significant_features_chi2:
        print(f"- {feature}")

if len(significant_features_chi2) == 0:
    print("Gợi ý: Không có đặc trưng nào thỏa mãn ngưỡng. Cân nhắc tăng ngưỡng p-value (ví dụ: 0.15).")
elif len(significant_features_chi2) > 20:
    print("Gợi ý: Số lượng đặc trưng quá lớn. Cân nhắc giảm ngưỡng p-value (ví dụ: 0.05).")

"""#### **Nhận xét:**

**Nhận xét:**

- **Chi-square Test** đã xác định 4 đặc trưng phân loại có mối quan hệ ý nghĩa thống kê với `Dominant_Season` ở ngưỡng **(p-value < 0.1)**: `Category_Clothing`, `Category_Accessories`, `Category_Footwear`, và `Category_Outerwear`, với giá trị Chi-square rất cao (từ 244.24 đến 616.96), cho thấy tác động mạnh mẽ của các danh mục sản phẩm đến mùa mua sắm.

- Các đặc trưng như `Gender_Female`, `DiscountApplied_encoded`, `Size_encoded`, và `SubscriptionStatus_encoded` có **(p-value > 0.3)**, cho thấy không có mối quan hệ thống kê đáng kể với `Dominant_Season`, và có thể được xem xét loại bỏ để giảm kích thước dữ liệu.

- Kết quả này bổ sung cho các phương pháp trước, đặc biệt là ANOVA (chỉ tập trung vào đặc trưng số học), và nhấn mạnh vai trò của các đặc trưng phân loại trong việc dự đoán ngữ cảnh mua sắm (mùa).

- Kết hợp với Correlation và Feature Importance, danh sách đặc trưng cuối cùng nên bao gồm cả các đặc trưng số quan trọng (từ Random Forest) và phân loại có ý nghĩa (từ Chi-square) để tối ưu hóa mô hình Recommendation.

## [4.6] - Xác định tập đặc trưng cuối cùng

### 4.6.1) Danh sách các đặc trưng cuối cùng
"""

final_features = list(set(rf_features + significant_features_anova + significant_features_chi2))
if 'Interaction_Score' not in final_features:

    final_features.append('Interaction_Score')

"""### 4.6.2) In kết quả"""

print("\nDanh sách đặc trưng cuối cùng sau khi tổng hợp:")
print(final_features)
print(f"Số đặc trưng cuối cùng: {len(final_features)}")

"""### 4.6.3) Tạo DataFrame"""

df_final_features = df_final_features[final_features]
print("\nDữ liệu sau khi giữ lại các đặc trưng cuối cùng:")
display(df_final_features.head())

"""### 4.6.4) Lưu các đặc trưng cuối cùng dưới dạng để sử dụng cho Bước 5"""

filename = 'shopping_behavior_final_features.csv'
save_path = os.path.join(data_dir, filename)
df_final_features.to_csv(save_path, index=False)
print(f"Đã lưu DataFrame với các đặc trưng cuối cùng vào [ {save_path} ]")

"""**Nhận xét:**

- Kết hợp từ các phương pháp (Correlation, Random Forest, ANOVA F-test, Chi-square Test) cho ra danh sách **{len(final_features)} đặc trưng cuối cùng**, bao gồm các đặc trưng số quan trọng (`Customer_Loyalty_Score`, `Age`, `PreviousPurchases`) và các đặc trưng phân loại có ý nghĩa thống kê (`Category_Clothing`, `Category_Accessories`, v.v.).

- Phương pháp loại bỏ `Review Rating` từ Correlation giúp tránh đa cộng tuyến, trong khi Random Forest xác định các đặc trưng đóng góp lớn vào `Interaction_Score`. ANOVA không tìm thấy mối quan hệ tuyến tính mạnh, nhưng Chi-square xác nhận vai trò của các đặc trưng phân loại liên quan đến mùa.

- Số lượng đặc trưng ({len(final_features)}) là hợp lý để xây dựng mô hình Recommendation, cân bằng giữa độ chính xác và độ phức tạp. Các đặc trưng như `Gender_Female`, `Size_encoded` có thể được loại bỏ nếu cần tinh giản thêm, dựa trên kết quả Chi-square.

- Danh sách này sẽ được sử dụng làm đầu vào cho các bước tiếp theo (như xây dựng mô hình Item-Item CF, Content-Based, KNN CF, Hybrid) để tối ưu hóa khuyến nghị sản phẩm.

## [4.7] - Tổng kết quá trình tạo đặc trưng (Feature Engineering)

- Đã tiến hành phân tích, lựa chọn và tổng hợp các đặc trưng từ dữ liệu hành vi mua sắm của khách hàng.
- Sử dụng nhiều phương pháp khác nhau như: thống kê mô tả, phân tích tương quan, lựa chọn đặc trưng dựa trên mô hình (Random Forest), kiểm định ANOVA và kiểm định Chi-squared.
- Đã xây dựng các đặc trưng mới như: Customer_Loyalty_Score, Dominant_Season, Category, Size_encoded, DiscountApplied_encoded, v.v.
- Đặc biệt, đã bổ sung đặc trưng Interaction_Score để phản ánh tổng mức độ tương tác của khách hàng với sản phẩm.
- Kết quả, đã chọn ra danh sách các đặc trưng cuối cùng phục vụ cho bước huấn luyện mô hình ở Bước 5.

## [4.8] - Tổng kết và đánh giá các bước Feature Engineering

## [4.8] - Tóm tắt quyết định Feature Engineering (CHƯA CHỈNH)

Dựa trên tất cả các phân tích trên, chúng ta đã thực hiện các quyết định sau trong quá trình Feature Engineering:

### 4.11.1) **Loại bỏ đặc trưng**

- Đã loại bỏ các đặc trưng định danh không có giá trị dự đoán (`Customer ID`).
- Đã loại bỏ các đặc trưng dư thừa (`Promo Code Used` ~ `Discount Applied`).
- Đã loại bỏ các cột gốc đã được mã hóa (ngoại trừ `Item Purchased` được giữ lại cho mục đích đề xuất sản phẩm).
- Đã loại bỏ các đặc trưng trung gian dùng để tính toán.
- Đã xác định và cân nhắc loại bỏ các đặc trưng có phương sai cực thấp.

=> **Đã loại bỏ `Season_sin`, `Season_cos` để giữ nguyên đặc trưng `Season`.**

### 4.11.2) **Tạo đặc trưng mới**

- Đã tạo `Product_ID` bằng cách kết hợp thông tin sản phẩm.
- Đã tạo `Customer_Loyalty_Score` từ các chỉ số hành vi mua hàng.
- Đã tạo `Interaction_Score` là mục tiêu tiềm năng cho mô hình.

### 4.11.3) **Lựa chọn phương pháp mã hóa**

- Sử dụng One-hot encoding cho `Gender` và `Category`.
- Sử dụng Label encoding cho các biến phân loại còn lại.
- Chuẩn hóa các biến số với `StandardScaler`.
- Giữ nguyên biến `Item Purchased` để phục vụ cho việc đề xuất sản phẩm cụ thể.

=> **Giữ nguyên đặc trưng `Season` (không mã hóa tuần hoàn, không dùng Season_sin/cos).**

### 4.11.4) **Phương pháp chọn lọc đặc trưng**

- Sử dụng ma trận tương quan để phát hiện đa cộng tuyến.
- Sử dụng **Random Forest Feature Importance** để xác định đặc trưng quan trọng.
- Sử dụng **ANOVA F-test** và **Chi-square test** để đánh giá ý nghĩa thống kê.
- Phân tích phương sai để xác định đặc trưng không có giá trị phân biệt.
- `Loại trừ` các đặc trưng chuỗi như `Item Purchased` khỏi các phân tích số học nhưng giữ lại trong bộ dữ liệu cuối cùng.

# Bước 5: Huấn luyện, đánh giá và tinh chỉnh mô hình

**Mục tiêu:** Kết hợp Collaborative Filtering và Content-Based Filtering để đề xuất sản phẩm cá nhân hóa, tận dụng cả lịch sử tương tác và đặc trưng sản phẩm/người dùng.

## File sử dụng:
- `shopping_behavior_final_features.csv` (dữ liệu đặc trưng đã xử lý)
- `user_item_matrix.csv` (ma trận tiện ích User-Item)

## [5.1] - Import Thư viện
"""

import os
import time
import psutil
import joblib
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor, VotingRegressor, BaggingRegressor, StackingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, ndcg_score
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from scipy import sparse
from sklearn.model_selection import KFold
import optuna

pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')


training_dir = os.path.join(figures_dir, "Training")
os.makedirs(training_dir, exist_ok=True)
print(f"Training_dir path: [ {training_dir} ]")

def load_data():

    final_features_csv = os.path.join(data_dir, "shopping_behavior_final_features.csv")
    user_item_matrix_csv = os.path.join(data_dir, "user_item_matrix.csv")
    processed_features_csv = os.path.join(data_dir, "shopping_behavior_processed.csv")

    features_df = pd.read_csv(final_features_csv)
    user_item_matrix = pd.read_csv(user_item_matrix_csv, index_col=0)
    processed_features_df = pd.read_csv(processed_features_csv)
    return features_df, user_item_matrix, processed_features_df

# Gọi hàm để tải dữ liệu
features_df, user_item_matrix, processed_features_df = load_data()

# In thông tin cơ bản về user_item_matrix để kiểm tra
print("user_item_matrix shape:", user_item_matrix.shape)
print("Số user:", user_item_matrix.shape[0])
print("Số item (features):", user_item_matrix.shape[1])

# Kiểm tra tính duy nhất của Customer ID trong user_item_matrix
unique_customer_ids = user_item_matrix.index.nunique()
print("Số lượng Customer ID unique:", unique_customer_ids)
num_rows = user_item_matrix.shape[0]
print("Số lượng dòng (rows):", num_rows)

# So sánh để phát hiện trùng lặp Customer ID
if unique_customer_ids == num_rows:
    print("OK: Mỗi dòng là một Customer ID unique.")
else:
    print(f"Cảnh báo: Có {num_rows - unique_customer_ids} dòng bị trùng Customer ID!")

print(user_item_matrix.columns.tolist())

# Danh sách các cột bắt buộc phải có trong features_df
required_cols = [
    'PreviousPurchases', 'Gender_Female', 'Customer_Loyalty_Score', 'Size_encoded',
    'Dominant_Season_Spring', 'Category_Footwear', 'Age', 'Dominant_Season_Winter',
    'Dominant_Season_Summer', 'DiscountApplied_encoded', 'Dominant_Season_Fall',
    'Category_Clothing', 'Category_Outerwear', 'Category_Accessories', 'Interaction_Score'
]

# Kiểm tra xem các cột bắt buộc có tồn tại không
if not all(col in features_df.columns for col in required_cols):
    raise ValueError(f"Thiếu cột: {set(required_cols) - set(features_df.columns)}")

# Danh sách các cột số cần kiểm tra
numerical_cols = ['Age', 'PreviousPurchases', 'Customer_Loyalty_Score']
if not all(col in features_df.columns for col in numerical_cols):
    raise ValueError(f"Thiếu cột số: {set(numerical_cols) - set(features_df.columns)}")

# Kiểm tra sự tồn tại của cột Interaction_Score
if 'Interaction_Score' not in features_df.columns:
    raise ValueError("Thiếu cột 'Interaction_Score' trong features_df")

# Xử lý và đồng bộ cột Customer ID giữa các DataFrame
customer_id_col = 'Customer ID'
if 'Customer ID' not in processed_features_df.columns:
    if 'Customer_ID' in processed_features_df.columns:
        customer_id_col = 'Customer_ID'
    else:
        raise ValueError("processed_features_df không có cột 'Customer ID' hoặc 'Customer_ID'")

# Thêm cột Customer ID vào features_df nếu chưa có
if 'Customer ID' not in features_df.columns:
    features_df.insert(0, 'Customer ID', processed_features_df[customer_id_col].iloc[:len(features_df)])

# Đảm bảo kiểu dữ liệu đồng nhất cho cột Customer ID
features_df['Customer ID'] = features_df['Customer ID'].astype(int)
user_item_matrix.index = user_item_matrix.index.astype(int)

# Lưu file features_df đã cập nhật
updated_features_path = os.path.join(data_dir, 'shopping_behavior_final_features_with_customer_id.csv')
features_df.to_csv(updated_features_path, index=False)

"""## [5.1] - train_test_split"""

train_df, test_df = train_test_split(features_df, test_size=0.2, random_state=42, shuffle=True)
print(f"train_df.shape: {train_df.shape}")
display(train_df.head())
print(f"test_df.shape: {test_df.shape}")
display(test_df.head())

"""## [5.2] - User_profiles_featuers"""

# Định nghĩa các nhóm cột đặc trưng để sử dụng trong mô hình
category_cols = [col for col in features_df.columns if col.startswith("Category_")]
season_cols = [col for col in features_df.columns if col.startswith("Dominant_Season_")]
size_encoded_col = "Size_encoded"
discount_encoded_col = "DiscountApplied_encoded"
gender_female_col = "Gender_Female"
age_col = "Age"
previous_purchases_col = "PreviousPurchases"
loyalty_score_col = "Customer_Loyalty_Score"
interaction_score_col = "Interaction_Score"

# Tạo danh sách đặc trưng cho profile user
user_profile_features = (
    category_cols
    + season_cols
    + [size_encoded_col, discount_encoded_col, gender_female_col, age_col, previous_purchases_col, loyalty_score_col]
)

print(f"user_profile_features: {user_profile_features}")

"""## [5.3] - Item Categories"""

# Kiểm tra tính khớp nối giữa danh mục trong user_item_matrix và features_df
items = user_item_matrix.columns.tolist()
categories = set(col.replace('Category_', '') for col in category_cols)
item_categories = set(item.split('_')[-1] for item in items)

if not categories.issuperset(item_categories):
    raise ValueError(f"Một số danh mục trong user_item_matrix không khớp Category_*: {item_categories - categories}")

print(f"Item categories: {item_categories}")

"""## [5.4] - Assign Item columns"""

def assign_item_column(df, user_item_matrix, features_df):
    """
    Gán cột Item cho DataFrame dựa trên Customer_Loyalty_Score để chọn nhiều item.

    Mục đích: Gán các item mà user đã tương tác dựa trên điểm loyalty để làm giàu dữ liệu.
    Lý do : Giúp mô hình hiểu được sở thích của user qua các item đã tương tác.
    Input: df (DataFrame cần gán), user_item_matrix (ma trận user-item), features_df (dữ liệu đặc trưng).
    Output: Danh sách các item được gán cho từng user.
    """
    items = []
    for cid in df['Customer ID']:
        if cid in user_item_matrix.index:
            user_row = user_item_matrix.loc[cid]
            interacted_items = user_row[user_row > 0].index.tolist()

            if not interacted_items:
                items.append([np.nan])
                # print(f"User {cid}: Không có tương tác trong user_item_matrix.")
            else:
                user_data = features_df[features_df['Customer ID'] == cid]

                if not user_data.empty:
                    loyalty_score = user_data['Customer_Loyalty_Score'].iloc[0]
                    item_scores = {}

                    for item in interacted_items:
                        category = item.split('_')[-1]
                        category_col = f'Category_{category}'

                        if category_col in user_data.columns:
                            base_score = user_row[item] if not np.isnan(user_row[item]) else 0
                            item_scores[item] = base_score * loyalty_score

                    if item_scores:
                        top_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:3]
                        items.append([item for item, score in top_items])
                        # print(f"User {cid}: Gán {len(top_items)} item: {top_items}")
                    else:
                        items.append([np.nan])
                        # print(f"User {cid}: Không có score hợp lệ.")
                else:
                    items.append([np.nan])
                    # print(f"User {cid}: Không tìm thấy trong features_df.")
        else:
            items.append([np.nan])
            # print(f"User {cid}: Không trong user_item_matrix.")
    return items

# Gán cột Item cho train_df và test_df
train_df['Item'] = assign_item_column(train_df, user_item_matrix, features_df)
test_df['Item'] = assign_item_column(test_df, user_item_matrix, features_df)

"""### train_df"""

print("Train_df:")
display(train_df.head())

"""### test_df"""

print("Test_df:")
display(test_df.head())

"""## [5.5] - Xây dựng User Profiles với trọng số `Customer_Loyalty_Score`"""

# Hàm tính profile user có trọng số
def weighted_user_profile(df, features, weight_col='Customer_Loyalty_Score'):
    """
        Tính profile của user dựa trên trung bình có trọng số của các đặc trưng.

        Mục đích: Xây dựng profile user dựa trên các đặc trưng có trọng số để phản ánh sở thích chính xác hơn.
        Lý do: Trọng số từ Customer_Loyalty_Score giúp ưu tiên các đặc trưng quan trọng của user.
        Input: df (DataFrame đầu vào), features (danh sách đặc trưng), weight_col (cột dùng làm trọng số).
        Output: DataFrame chứa profile user với các đặc trưng đã tính toán.
    """
    def _weighted_avg(x):
        w = x[weight_col].fillna(0)  # Xử lý NaN bằng 0
        w_sum = w.sum()  # Tính tổng trọng số
        if not np.isscalar(w_sum):
            raise ValueError(f"w_sum không phải là scalar: {w_sum}")
        if np.isclose(w_sum, 0):  # Kiểm tra tổng bằng 0 với độ chính xác số
            return x[features].mean().values
        return np.average(x[features], axis=0, weights=w)

    # Kiểm tra cột weight_col có tồn tại không
    if weight_col not in df.columns:
        raise ValueError(f"Cột '{weight_col}' không tồn tại trong DataFrame.")

    # Loại bỏ trùng lặp cột bằng set trước khi groupby
    columns_to_select = list(set(features + [weight_col]))
    result = df.groupby('Customer ID')[columns_to_select].apply(_weighted_avg)
    return pd.DataFrame(result.tolist(), index=result.index, columns=features)

# Tính user_profiles với trọng số Customer_Loyalty_Score
user_profiles = weighted_user_profile(features_df, user_profile_features, 'Customer_Loyalty_Score')
print("user_profiles.columns:", user_profiles.columns)
print("\nUser Profiles:")
display(user_profiles.head())

"""## [5.6] - Item features"""

def create_item_features(df, user_item_matrix, features):
    item_features = pd.DataFrame(index=user_item_matrix.columns)
    for item in user_item_matrix.columns:
        item_mask = df['Item'].apply(lambda x: item in x if isinstance(x, list) else False)
        if item_mask.any():
            item_df = df[item_mask]
            item_features.loc[item, features] = item_df[features].mean()
        else:
            category = item.split('_')[-1]
            category_col = f'Category_{category}'
            mask = df[category_col] == 1
            if mask.any():
                item_features.loc[item, features] = df.loc[mask, features].mean()
            else:
                item_features.loc[item, features] = 0.1
    return item_features.fillna(0.1)

item_features = create_item_features(train_df, user_item_matrix, user_profile_features)
print("Item_features:")
display(item_features.head())

"""### Lọc bỏ các dòng có Item là NaN trong train_df và test_df"""

train_df = train_df[train_df['Item'].apply(lambda x: not isinstance(x, float) or not np.isnan(x))].copy()
print("Train_df:")
display(train_df.head())
test_df = test_df[test_df['Item'].apply(lambda x: not isinstance(x, float) or not np.isnan(x))].copy()
print("Test_df:")
display(test_df.head())

"""## [5.7] - User_item_matrix"""

# Mục đích: Sử dụng trực tiếp ma trận user-item gốc cho Item-Item CF.
# Lý do: Tránh sử dụng SVD do dữ liệu 1 user - 1 item không phù hợp với làm đầy.
# Input: user_item_matrix.
# Output: user_item_matrix_cf (ma trận gốc).
user_item_matrix_cf = user_item_matrix.copy()
print("Sử dụng ma trận user-item gốc do mỗi user chỉ tương tác 1 item.")
print("user_item_matrix_cf.shape:", user_item_matrix_cf.shape)

print("Sử dụng user_item_matrix_train cho huấn luyện và user_item_matrix_test cho đánh giá.")
user_item_matrix_train = user_item_matrix.loc[train_df['Customer ID']].copy()
print(f"\nuser_item_matrix_train shape: {user_item_matrix_train.shape}")
display(user_item_matrix_train.head())

user_item_matrix_test = user_item_matrix.loc[test_df['Customer ID']].copy()
print(f"user_item_matrix_test shape: {user_item_matrix_test.shape}")
display(user_item_matrix_test.head())

"""## [5.8] - CB Score"""

def predict_cb_score(user_id, item_id, user_profiles, item_features, min_similarity=0.15):
    if user_id not in user_profiles.index:
        category = item_id.split('_')[-1]
        category_col = f'Category_{category}'
        if category_col in train_df.columns:
            mean_score = train_df[train_df[category_col] == 1]['Customer_Loyalty_Score'].mean()
            return mean_score if not np.isnan(mean_score) else 0.1
        return 0.1
    if item_id not in item_features.index:
        return 0.1

    cat_features = category_cols + season_cols
    user_cat_vec = user_profiles.loc[user_id, cat_features].values.reshape(1, -1)
    item_cat_vec = item_features.loc[item_id, cat_features].values.reshape(1, -1)
    cat_similarity = cosine_similarity(user_cat_vec, item_cat_vec)[0][0]

    num_features = [age_col, previous_purchases_col, loyalty_score_col]
    user_num_vec = user_profiles.loc[user_id, num_features].values
    item_num_vec = item_features.loc[item_id, num_features].values
    weights = np.array([1.0, 1.5, 2.0])
    user_weighted = user_num_vec * weights
    item_weighted = item_num_vec * weights
    num_similarity = cosine_similarity(user_weighted.reshape(1, -1), item_weighted.reshape(1, -1))[0][0]

    final_similarity = cat_similarity * 0.5 + num_similarity * 0.5

    score = (final_similarity + 1) / 2
    return min(max(score, 0), 1.0) if score >= min_similarity else 0.1

"""### CB model"""

class CBModel:
    def __init__(self, user_profiles, item_features):
        self.user_profiles = user_profiles
        self.item_features = item_features
    def predict(self, user_id, target_items):
        preds = []
        for item_id in target_items:
            score = predict_cb_score(user_id, item_id, self.user_profiles, self.item_features)
            preds.append(score)
        return np.array(preds)

"""## [5.9] - Collaborative Filtering (Item-Item-CF)"""

def predict_item_cf_score(user_id, item_id, user_item_matrix_train, global_mean=None):
    if user_id not in user_item_matrix_train.index or item_id not in user_item_matrix_train.columns:
        return 0.1 if global_mean is None else float(global_mean)

    sparse_matrix = sparse.csr_matrix(user_item_matrix_train.values)
    item_item_matrix = sparse_matrix.T
    item_indices = user_item_matrix_train.columns
    item_idx = item_indices.get_loc(item_id)

    knn = NearestNeighbors(n_neighbors=len(item_indices), metric='cosine', algorithm='brute')
    knn.fit(item_item_matrix)
    item_vector = item_item_matrix[item_idx:item_idx+1]
    distances, indices = knn.kneighbors(item_vector, n_neighbors=len(item_indices))
    similarities = 1 - distances[0]
    similar_item_ids = item_indices[indices[0]]

    user_vector = user_item_matrix_train.loc[user_id].values
    interacted_items = [(item, score) for item, score in zip(user_item_matrix_train.columns, user_vector) if score > 0]

    if not interacted_items:
        return 0.1 if global_mean is None else float(global_mean)

    weighted_sum = 0
    similarity_sum = 0
    for sim_item_id, similarity in zip(similar_item_ids, similarities):
        for interacted_item_id, user_interaction in interacted_items:
            if sim_item_id == interacted_item_id:
                weighted_sum += similarity * user_interaction
                similarity_sum += abs(similarity)
                break

    if similarity_sum == 0:
        return 0.1 if global_mean is None else float(global_mean)

    score = weighted_sum / similarity_sum
    return float(np.clip(score, 0, 1))

"""### Item-CF Model"""

class ItemCFModel:
    def __init__(self, user_item_matrix_train):
        self.user_item_matrix_train = user_item_matrix_train
        self.global_mean = user_item_matrix_train.values.mean()
    def predict(self, user_id, target_items):
        preds = []
        for item_id in target_items:
            score = predict_item_cf_score(user_id, item_id, self.user_item_matrix_train, self.global_mean)
            preds.append(score)
        return np.array(preds)

"""## [5.10] - Hybrid Score"""

def hybrid_score(user_id, item_id, user_profiles, item_features, user_item_matrix_train, alpha=0.5, beta=0.5):
    cb_score = predict_cb_score(user_id, item_id, user_profiles, item_features)
    item_cf_score = predict_item_cf_score(user_id, item_id, user_item_matrix_train)
    return alpha * cb_score + beta * item_cf_score

"""### Hybrid Model"""

class HybridModel:
    def __init__(self, user_profiles, item_features, user_item_matrix_train, alpha=0.5, beta=0.5):
        self.user_profiles = user_profiles
        self.item_features = item_features
        self.user_item_matrix_train = user_item_matrix_train
        self.alpha = alpha
        self.beta = beta

    def predict(self, user_id, target_items):
        preds = []
        for item_id in target_items:
            score = hybrid_score(user_id, item_id, self.user_profiles, self.item_features, self.user_item_matrix_train, self.alpha, self.beta)
            preds.append(score)
        return np.array(preds)

"""## [5.11] - evaluate model function"""

def evaluate_models(user_id, test_df, user_profiles, all_items, models):
    gt_items = test_df[test_df['Customer ID'] == user_id]['Item'].iloc[0] if user_id in test_df['Customer ID'].values else []
    gt_items = gt_items if isinstance(gt_items, list) else [gt_items] if pd.notna(gt_items) else []
    summary = {'Customer ID': user_id}
    for name, model in models:
        preds = model.predict(user_id, all_items)
        gt_scores = {item_id: round(preds[all_items.index(item_id)], 4) for item_id in gt_items if item_id in all_items}
        summary[name] = (gt_items, gt_scores)
    return summary

"""## [5.12] - top-n-recommendation"""

def print_top_n_recommendations(user_id, test_df, user_profiles, all_items, models, top_n=5):
    print(f"\nUser {user_id}:")
    gt_items = test_df[test_df['Customer ID'] == user_id]['Item'].iloc[0] if user_id in test_df['Customer ID'].values else []
    gt_items = gt_items if isinstance(gt_items, list) else [gt_items] if pd.notna(gt_items) else []

    print(f"  Đã mua: {gt_items}")
    for name, model in models:
        preds = model.predict(user_id, all_items)
        top_n_idx = np.argsort(preds)[::-1][:top_n]
        top_n_items = [all_items[i] for i in top_n_idx]
        top_n_scores = [preds[i] for i in top_n_idx]
        print(f"  {name} gợi ý Top-{top_n}:")
        for rank, (item, score) in enumerate(zip(top_n_items, top_n_scores), 1):
            print(f"    {rank}. {item:25} | Score: {score:.4f}")

"""## [5.13] - Demo"""

np.random.seed(42)
demo_users = test_df['Customer ID'].sample(5).tolist()
all_items = list(user_item_matrix.columns)
top_n = 5

cb_model = CBModel(user_profiles, item_features)
item_cf_model = ItemCFModel(user_item_matrix_train)
hybrid_model = HybridModel(user_profiles, item_features, user_item_matrix_train, alpha=0.7, beta=0.3)

models = [
    ('CB_Model', cb_model),
    ('Item-CF_Model', item_cf_model),
    ('Hybrid_Model', hybrid_model)
]

"""### Bảng 1: Đã mua và Score các item đã mua"""

summary_results = []
for user_id in demo_users:
    result = evaluate_models(user_id, test_df, user_profiles, all_items, models)
    summary_results.append(result)
summary_df = pd.DataFrame([
    {'Customer ID': r['Customer ID'], 'Model': model, 'Đã mua': r[model][0], 'Score các item đã mua': r[model][1]}
    for r in summary_results for model in ['CB_Model', 'Item-CF_Model', 'Hybrid_Model']
])

print('\nBẢNG 1: Đã mua và điểm dự đoán của từng mô hình')
display(summary_df)

"""### Demo users"""

for user_id in demo_users:
    print_top_n_recommendations(user_id, test_df, user_profiles, all_items, models)

"""## [5.14] - ML features and training"""

# Sao chép danh sách đặc trưng để sử dụng trong mô hình ML
ml_features = user_profile_features.copy()
print(f"\nml_features: {ml_features}")

"""### ML dataset"""

def create_ml_dataset(interaction_df, user_profiles, item_features, interaction_df_source, is_train=True):
    """
    Tạo tập dữ liệu đầu vào và nhãn cho mô hình ML.

    Mục đích: Chuẩn bị dữ liệu để huấn luyện hoặc kiểm tra mô hình ML.
    Lý do: Tích hợp profile user và item để tạo ra tập dữ liệu đầy đủ cho dự đoán.
    Input: interaction_df (dữ liệu tương tác), user_profiles, item_features, interaction_df_source (train_df/test_df), is_train (flag train/test).
    Output: X (đặc trưng), y (nhãn), và DataFrame chứa cặp user-item.
    """
    data = []
    for _, row in interaction_df.iterrows():
        user_id = row['Customer ID']
        item = row['Item'][0] if isinstance(row['Item'], list) else row['Item']

        # Kiểm tra user_id có trong user_profiles
        if user_id in user_profiles.index:
            user_feat = user_profiles.loc[user_id]
        else:
            user_feat = user_profiles.mean()  # Giá trị mặc định cho user mới

        # Kiểm tra item có trong item_features
        if item in item_features.index:
            item_feat = item_features.loc[item]
        else:
            item_feat = pd.Series(0, index=item_features.columns)  # Giá trị mặc định cho item mới

        # Lấy Interaction_Score từ interaction_df_source để tránh rò rỉ
        interaction_score = interaction_df_source[interaction_df_source['Customer ID'] == user_id]['Interaction_Score'].iloc[0] if user_id in interaction_df_source['Customer ID'].values else 0

        # Kết hợp đặc trưng user và item
        combined = pd.concat([user_feat.rename(lambda x: f"{x}_user"), item_feat.rename(lambda x: f"{x}_item")])
        combined['Customer ID'] = user_id
        combined['Item'] = item
        combined['Interaction_Score'] = interaction_score
        data.append(combined)

    merged = pd.DataFrame(data)

    # Điền giá trị mặc định cho NaN
    merged = merged.fillna(0)

    # Loại bỏ các hàng có NaN ở các cột quan trọng
    merged = merged.dropna(subset=['Customer ID', 'Item', 'Interaction_Score'])

    # Tạo feature_cols, loại bỏ cột không phải đặc trưng
    feature_cols = [col for col in merged.columns if col not in ['Customer ID', 'Item', 'Interaction_Score']]
    X = merged[feature_cols]
    y = merged['Interaction_Score']

    # Kiểm tra đồng bộ số hàng
    if len(X) != len(y) or len(X) != len(merged[['Customer ID', 'Item']]):
        raise ValueError(f"Số hàng không đồng bộ: X={len(X)}, y={len(y)}, pairs={len(merged[['Customer ID', 'Item']])}")

    # Kiểm tra và in thông tin để debug
    print(f"Created dataset shape: {X.shape}, NaN check: {X.isna().sum().sum()}")
    return X, y, merged[['Customer ID', 'Item']]

"""### train_test_split for ML"""

X_train, y_train, train_pairs = create_ml_dataset(train_df, user_profiles, item_features, train_df, is_train=True)
X_test, y_test, test_pairs = create_ml_dataset(test_df, user_profiles, item_features, test_df, is_train=False)
print(f"X_train.shape: {X_train.shape}")
print(f"y_train.shape: {y_train.shape}")
print(f"X_test.shape: {X_test.shape}")
print(f"y_test.shape: {y_test.shape}")

"""### X_train"""

print("X_train:")
display(X_train.head())
print("\ny_train:")
display(y_train.head())
print("\ntrain_pairs head:\n", train_pairs.head())

"""### X_test"""

print("X_test:")
display(X_test.head())
print("\ny_test:")
display(y_test.head())
print("\ntest_pairs head:\n", test_pairs.head())

"""## [5.15] - models"""

cb_model = CBModel(user_profiles, item_features)
item_cf_model = ItemCFModel(user_item_matrix_train)
hybrid_model = HybridModel(user_profiles, item_features, user_item_matrix_train, alpha=0.7, beta=0.3)

ml_models = {
    'Ridge Regression': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(random_state=42),
    'LightGBM': LGBMRegressor(random_state=42, verbose=-1),
    'XGBoost': XGBRegressor(random_state=42, verbosity=0),
    'CatBoost': CatBoostRegressor(verbose=0, random_state=42),
}

all_models = {**ml_models}

"""## [5.16] - Utils functions

### measure_performance
"""

def measure_performance(func, *args):
    """
        Đo hiệu năng (thời gian, CPU, RAM) khi chạy một hàm bất kỳ.

        Args:
            func (callable): Hàm cần đo.
            *args: Tham số truyền vào hàm.

        Returns:
            tuple: (result, duration, cpu_avg, ram_used, ram_avg)
                - result: Kết quả trả về của hàm.
                - duration: Thời gian thực thi (giây).
                - cpu_avg: CPU trung bình (%).
                - ram_used: RAM sử dụng (MB).
                - ram_avg: RAM trung bình (MB).
    """

    start_time = time.time()
    process = psutil.Process()
    cpu_percentages = [psutil.cpu_percent(interval=None)]
    ram_usages = [process.memory_info().rss / 1024 / 1024]
    result = func(*args)
    cpu_percentages.append(psutil.cpu_percent(interval=None))
    ram_usages.append(process.memory_info().rss / 1024 / 1024)
    duration = time.time() - start_time
    cpu_avg = np.mean(cpu_percentages)
    ram_used = max(ram_usages) - min(ram_usages)
    ram_avg = np.mean(ram_usages)
    return result, duration, cpu_avg, ram_used, ram_avg

"""### get_model_size"""

def get_model_size(model, filename='temp_model.joblib'):
    """
    Tính kích thước của model sau khi dump ra file joblib.

    Args:
        model: Model đã train.
        filename (str): Tên file tạm lưu model.

    Returns:
        float: Kích thước file (MB). Nếu lỗi trả về 0.
    """
    try:
        joblib.dump(model, filename)
        size_mb = os.path.getsize(filename) / 1024 / 1024
        os.remove(filename)
        return size_mb
    except Exception:
        return 0

"""
### recommend_ml_top_n"""

def recommend_ml_top_n(model, user_id, items, user_profiles, item_features, N=5):
    if user_id not in user_profiles.index:
        user_feat = user_profiles.mean()  # Dùng giá trị trung bình cho user mới
    else:
        user_feat = user_profiles.loc[user_id]

    input_rows = []
    for item in items:
        item_feat = item_features.loc[item]
        row = pd.concat([user_feat.rename(lambda x: f"{x}_user"), item_feat.rename(lambda x: f"{x}_item")])
        input_rows.append(row)
    input_df = pd.DataFrame(input_rows)
    preds = model.predict(input_df)
    item_scores = list(zip(items, preds))
    top_n = sorted(item_scores, key=lambda x: x[1], reverse=True)[:N]
    return [item for item, score in top_n], [score for item, score in top_n]

"""### evaluate_ranking_vectorized"""

def evaluate_ranking_vectorized(model, test_df, items, category_cols, X_test, y_test=None, N=5):
    is_ml = hasattr(model, 'fit') and hasattr(model, 'predict') and model.__class__.__name__ not in ['CBModel', 'ItemCFModel', 'HybridModel']
    results = []
    for user_id in test_df['Customer ID'].unique():
        user_data = test_df[test_df['Customer ID'] == user_id]
        if 'Item' in user_data.columns:
            gt_items = user_data['Item'].dropna().apply(lambda x: x if not isinstance(x, list) else x[0] if x else None).dropna().unique()
            gt_items = [item for item in gt_items if item in items]
        else:
            gt_items = []
        if not gt_items:
            continue
        if is_ml:
            rec_items, rec_scores = recommend_ml_top_n(model, user_id, items, user_profiles, item_features, N)
        else:
            scores = []
            for item in items:
                score = model.predict(user_id, [item])[0]
                scores.append((item, score))
            scores = sorted(scores, key=lambda x: x[1], reverse=True)[:N]
            rec_items = [s[0] for s in scores]
        hits = len(set(rec_items) & set(gt_items))
        precision = hits / N
        recall = hits / len(gt_items) if gt_items else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        relevance = [1 if r in gt_items else 0 for r in rec_items]
        ideal = sorted(relevance, reverse=True)
        dcg = np.sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)])
        idcg = np.sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal)])
        ndcg = dcg / idcg if idcg > 0 else 0
        mrr = 0
        for i, rec in enumerate(rec_items, 1):
            if rec in gt_items:
                mrr = 1 / i
                break
        hit = 1 if hits > 0 else 0
        results.append({
            'Precision@N': precision,
            'Recall@N': recall,
            'F1@N': f1,
            'NDCG@N': ndcg,
            'MRR': mrr,
            'Hit Rate': hit
        })
    metrics = pd.DataFrame(results).mean().to_dict() if results else {}
    if is_ml and y_test is not None:
        preds = model.predict(X_test)
        metrics['RMSE'] = np.sqrt(mean_squared_error(y_test, preds))
        metrics['MAE'] = mean_absolute_error(y_test, preds)
        metrics['R² Score'] = r2_score(y_test, preds)
        n = X_test.shape[0]
        p = X_test.shape[1]
        metrics['Adjusted R²'] = 1 - (1 - metrics['R² Score']) * (n - 1) / (n - p - 1)
        metrics['MAPE (%)'] = np.mean(np.abs((y_test - preds) / y_test)) * 100 if (y_test != 0).all() else np.nan
        metrics['Explained Variance'] = explained_variance_score(y_test, preds)
    return metrics

"""### evaluate_coverage"""

def evaluate_coverage(model, test_df, items, N=5, user_profiles=None, item_features=None, model_type="cb", ml_model=None):
    recommended_items = set()
    for user_id in test_df['Customer ID'].unique():
        if model_type == "ml":
            rec_items, _ = recommend_ml_top_n(ml_model, user_id, items, user_profiles, item_features, N)
            recommended_items.update(rec_items)
            continue
        scores = []
        for item in items:
            if model_type == "cb":
                score = model.predict(user_id, [item])[0]
            elif model_type == "itemcf":
                score = model.predict(user_id, [item])[0]
            else:
                score = 0
            if not np.isnan(score) and score > 0:
                scores.append((item, score))
        if scores:
            scores = sorted(scores, key=lambda x: x[1], reverse=True)[:N]
            recommended_items.update([s[0] for s in scores])
    return len(recommended_items) / len(items) if items else 0

"""### train_model"""

def train_model(model, X_train, y_train, measure_performance):
    def _train():
        if hasattr(model, 'fit'):
            return model.fit(X_train, y_train)
        return model
    return measure_performance(_train)

"""### predict_model"""

def predict_model(trained_model, X_test, test_df, items, measure_performance):
    def _predict():
        is_ml = (
            hasattr(trained_model, 'fit')
            and hasattr(trained_model, 'predict')
            and trained_model.__class__.__name__ not in ['CBModel', 'ItemCFModel', 'HybridModel']
        )
        if is_ml:
            return trained_model.predict(X_test)
        else:
            preds = []
            for idx, row in test_df.iterrows():
                user_id = row['Customer ID']
                item_id = row['Item'] if 'Item' in row else items[0]
                preds.append(trained_model.predict(user_id, [item_id])[0])
            return np.array(preds)
    return measure_performance(_predict)

"""### get_top_5_with_scores"""

def get_top_5_with_scores(model, user_id, items, user_profiles, item_features, N=5):
    if model.__class__.__name__ in ['CBModel', 'ItemCFModel', 'HybridModel']:
        scores = model.predict(user_id, items)
        item_scores = list(zip(items, scores))
        top_5 = sorted(item_scores, key=lambda x: x[1], reverse=True)[:N]
        return [(item, score) for item, score in top_5]
    else:
        rec_items, rec_scores = recommend_ml_top_n(model, user_id, items, user_profiles, item_features, N)
        return list(zip(rec_items, rec_scores))

"""## [5.17] - CB_CF_Hybrid test"""

cb_cf_hybrid_results = []
for name, model in [('CB Model', cb_model), ('Item-CF Model', item_cf_model), ('Hybrid Model', hybrid_model)]:
    metrics = evaluate_ranking_vectorized(model, test_df, items, category_cols, X_test, y_test=None)
    coverage = evaluate_coverage(model, test_df, items, N=5, user_profiles=user_profiles, item_features=item_features, model_type="cb" if name == "CB Model" else "itemcf" if name == "Item-CF Model" else "hybrid")
    filtered_metrics = {k: v for k, v in metrics.items() if k in ['Precision@N', 'Recall@N', 'F1@N', 'NDCG@N', 'MRR', 'Hit Rate']}
    row = {'Model': name, **filtered_metrics, 'Coverage': coverage}
    cb_cf_hybrid_results.append(row)

cb_cf_hybrid_results = pd.DataFrame(cb_cf_hybrid_results)
print('\n### Kết quả CB, CF, Hybrid:')
display(cb_cf_hybrid_results)

"""## [5.18] - ML & ensemble test"""

results = []
cpu_cores = psutil.cpu_count()

missing_users = set(test_df['Customer ID'].unique()) - set(user_profiles.index)
if missing_users:
    print(f"Cảnh báo: Các user_id không có trong user_profiles: {missing_users}")

for name, model in all_models.items():
    trained_model, train_time, train_cpu_avg, train_ram_used, ram_avg = train_model(model, X_train, y_train, measure_performance)
    model_size = get_model_size(trained_model, f'temp_{name.replace(" ", "_")}.joblib')

    def _predict():
        return trained_model.predict(X_test)
    preds, inference_time, inference_cpu_avg, _, inference_ram_avg = measure_performance(_predict)

    ranking_metrics = evaluate_ranking_vectorized(trained_model, test_df, items, category_cols, X_test, y_test)
    coverage = evaluate_coverage(trained_model, test_df, items, N=5, user_profiles=user_profiles, item_features=item_features, model_type="ml", ml_model=trained_model)
    results.append({
        'Model': name,
        **ranking_metrics,
        'Coverage': coverage,
        'Train Time (s)': train_time,
        'Model Size (MB)': model_size,
        'CPU Cores': cpu_cores,
        'Train CPU Avg (%)': train_cpu_avg,
        'Train RAM Used (MB)': train_ram_used,
        'Train RAM Avg (%)': ram_avg / (psutil.virtual_memory().total / 1024 / 1024) * 100,
        'Inference Time (s)': inference_time,
        'Inference CPU Avg (%)': inference_cpu_avg,
        'Inference RAM Avg (%)': inference_ram_avg / (psutil.virtual_memory().total / 1024 / 1024) * 100,
    })

results_df = pd.DataFrame(results)
print('\nCác mô hình ML & Ensemble:')
display(results_df.head())

"""## [5.19] - CB_CF_Hybrid & ML_Ensemble Comparasion"""

comparison_df = pd.concat([results_df, cb_cf_hybrid_results], ignore_index=True)


desired_order = [
    'Model', 'Precision@N', 'Recall@N', 'F1@N', 'NDCG@N', 'MRR', 'Hit Rate', 'Coverage',
    'RMSE', 'MAE', 'R² Score', 'Adjusted R²', 'MAPE (%)', 'Explained Variance',
    'Train Time (s)', 'Model Size (MB)', 'CPU Cores', 'Train CPU Avg (%)',
    'Train RAM Used (MB)', 'Train RAM Avg (%)', 'Inference Time (s)',
    'Inference CPU Avg (%)', 'Inference RAM Avg (%)'
]
comparison_df = comparison_df.reindex(columns=desired_order)

print('\n=== So sánh metrics giữa CB/CF/Hybrid và các model ML training:')
display(comparison_df)

"""## [5.20] - Visualize Recommndation Metrics"""

# Gộp dữ liệu từ cb_cf_hybrid_results và results_df
metrics = ['Precision@N', 'Recall@N', 'F1@N', 'NDCG@N', 'MRR', 'Hit Rate']
plot_data = pd.concat([cb_cf_hybrid_results[['Model'] + metrics], results_df[['Model'] + metrics]], ignore_index=True)

# Định nghĩa màu sắc cho từng độ đo
colors = {
    'Precision@N': '#1f77b4',  # Xanh dương
    'Recall@N': '#ff7f0e',     # Cam
    'F1@N': '#2ca02c',         # Xanh lá
    'NDCG@N': '#d62728',       # Đỏ
    'MRR': '#9467bd',          # Tím
    'Hit Rate': '#8c564b'      # Nâu
}

# Định nghĩa chú thích cho legend với công thức/mô tả ngắn gọn trong dấu ngoặc
labels_vn = {
    # Đo tỷ lệ item đúng trong top-N gợi ý.
    'Precision@N': 'Precision@N',
    # Đo tỷ lệ item đúng được gợi ý trong top-N trên tổng số item đúng.
    'Recall@N': 'Recall@N',
    # Trung bình điều hòa giữa Precision@N và Recall@N.
    'F1@N': 'F1@N',
    # Đánh giá chất lượng thứ tự sắp xếp các item đúng trong top-N. VD: Item đúng ở đầu danh sách sẽ được điểm cao hơn.
    'NDCG@N': 'NDCG@N',
    # Đo vị trí xuất hiện của item đúng đầu tiên. VD: Nếu item đúng đầu tiên ở vị trí 3 thì MRR = 1/3.
    'MRR': 'MRR',
    # Đo tỷ lệ có ít nhất một item đúng trong top-N gợi ý.
    'Hit Rate': 'Hit Rate'
}

# Thiết lập kích thước biểu đồ
plt.figure(figsize=(14, 8))

# Số mô hình và số độ đo
n_models = len(plot_data)
n_metrics = len(metrics)
bar_width = 0.12  # Độ rộng của mỗi bar
index = np.arange(n_models)  # Vị trí các mô hình trên trục x

# Vẽ bar cho từng độ đo
for i, metric in enumerate(metrics):
    bars = plt.bar(index + i * bar_width, plot_data[metric], bar_width,
                   label=labels_vn[metric], color=colors[metric])

    # Hiển thị số liệu trên đầu mỗi bar
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01,
                 f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# Thiết lập tiêu đề và nhãn
plt.xlabel('Models', fontsize=12)
plt.ylabel('Values', fontsize=12)
plt.title('Recommendation Metrics', fontsize=14)
plt.xticks(index + bar_width * (n_metrics - 1) / 2, plot_data['Model'], rotation=45, ha='right')
plt.ylim(0, 1.1)  # Giới hạn trục y để đủ chỗ cho số liệu

# Đặt legend bên ngoài biểu đồ (bên phải)
plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize=10)

plt.tight_layout()
save_path = os.path.join(training_dir, 'recommendation_metrics_comparison.png')
plt.savefig(save_path, dpi=300, bbox_inches='tight')

plt.show()

"""## [5.21] - Demo recommend product for user on test"""

# Demo recommendation thực tế với một số user trên test set
available_users = [uid for uid in demo_users if uid in test_df['Customer ID'].values]
if len(available_users) < len(demo_users):
    print(f"Cảnh báo: Một số user không có trong test_df: {set(demo_users) - set(available_users)}")

print("\n--- DEMO kết quả recommend (CB/CF/Hybrid + ML) trên test set ---")
for user_id in available_users:
    print(f"\nUser {user_id}:")
    gt_items = test_df[test_df['Customer ID'] == user_id]['Item'].iloc[0] if user_id in test_df['Customer ID'].values else []
    gt_items = gt_items if isinstance(gt_items, list) else [gt_items] if pd.notna(gt_items) else []
    print(f"  Đã mua: {gt_items}")
    for name, model in [('CB_Model', cb_model), ('Item-CF_Model', item_cf_model), ('Hybrid_Model', hybrid_model)] + [(name, model) for name, model in all_models.items()]:
        top_5 = get_top_5_with_scores(model, user_id, items, user_profiles, item_features, N=5)
        print(f"  {name} gợi ý Top-5:")
        for rank, (item, score) in enumerate(top_5, 1):
            print(f"    {rank}. {item:<25} | Score: {score:.4f}")

"""## [5.22] - K-Fold Cross Validation"""

# K-Fold Cross-Validation chỉ cho các model ML/Ensemble
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_results = []

# all_models đã chỉ chứa ML/Ensemble (Ridge Regression, Random Forest, LightGBM, XGBoost, CatBoost)
ml_ensemble_models = all_models

for name, model in ml_ensemble_models.items():
    precision_scores, recall_scores, f1_scores, ndcg_scores = [], [], [], []
    mrr_scores, hit_rate_scores = [], []
    rmse_scores, mae_scores, r2_scores, adjusted_r2_scores, mape_scores, explained_variance_scores = [], [], [], [], [], []

    for train_idx, val_idx in kf.split(X_train):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        val_df = test_df.iloc[val_idx] if len(test_df) == len(X_train) else train_df.iloc[val_idx]  # Đồng bộ với test_df hoặc train_df

        # Fit model ML/ensemble
        model.fit(X_tr, y_tr)
        # Đánh giá trên validation fold
        metrics = evaluate_ranking_vectorized(model, val_df, items, category_cols, X_val, y_val)

        precision_scores.append(metrics.get('Precision@N', 0))
        recall_scores.append(metrics.get('Recall@N', 0))
        f1_scores.append(metrics.get('F1@N', 0))
        ndcg_scores.append(metrics.get('NDCG@N', 0))
        mrr_scores.append(metrics.get('MRR', 0))
        hit_rate_scores.append(metrics.get('Hit Rate', 0))
        rmse_scores.append(metrics.get('RMSE', np.nan))
        mae_scores.append(metrics.get('MAE', np.nan))
        r2_scores.append(metrics.get('R² Score', np.nan))
        adjusted_r2_scores.append(metrics.get('Adjusted R²', np.nan))
        mape_scores.append(metrics.get('MAPE (%)', np.nan))
        explained_variance_scores.append(metrics.get('Explained Variance', np.nan))

    # Tổng hợp metric cho model này
    cv_results.append({
        'Model': name,
        'Precision@N Mean': np.mean(precision_scores),
        'Precision@N Std': np.std(precision_scores),
        'Recall@N Mean': np.mean(recall_scores),
        'Recall@N Std': np.std(recall_scores),
        'F1@N Mean': np.mean(f1_scores),
        'F1@N Std': np.std(f1_scores),
        'NDCG@N Mean': np.mean(ndcg_scores),
        'NDCG@N Std': np.std(ndcg_scores),
        'MRR Mean': np.mean(mrr_scores),
        'MRR Std': np.std(mrr_scores),
        'Hit Rate Mean': np.mean(hit_rate_scores),
        'Hit Rate Std': np.std(hit_rate_scores),
        'RMSE Mean': np.nanmean(rmse_scores),
        'RMSE Std': np.nanstd(rmse_scores),
        'MAE Mean': np.nanmean(mae_scores),
        'MAE Std': np.nanstd(mae_scores),
        'R² Score Mean': np.nanmean(r2_scores),
        'R² Score Std': np.nanstd(r2_scores),
        'Adjusted R² Mean': np.nanmean(adjusted_r2_scores),
        'Adjusted R² Std': np.nanstd(adjusted_r2_scores),
        'MAPE (%) Mean': np.nanmean(mape_scores),
        'MAPE (%) Std': np.nanstd(mape_scores),
        'Explained Variance Mean': np.nanmean(explained_variance_scores),
        'Explained Variance Std': np.nanstd(explained_variance_scores)
    })

cv_results_df = pd.DataFrame(cv_results)
print('\nK-Fold Cross-Validation Results (ML/Ensemble Only):')
display(cv_results_df.head())

"""## [5.23] - Visualize K-Fold Result"""

def visualize_kfold_results(cv_results_df):
    # Định nghĩa mapping cho các metric mean với công thức ngắn gọn
    metric_labels_kfold = {
        # Đo tỷ lệ item đúng trong top-N gợi ý.
        'Precision@N Mean': 'Precision@N',
        # Đo tỷ lệ item đúng được gợi ý trong top-N trên tổng số item đúng.
        'Recall@N Mean': 'Recall@N',
        # Trung bình điều hòa giữa Precision@N và Recall@N.
        'F1@N Mean': 'F1@N',
        # Đánh giá chất lượng thứ tự sắp xếp các item đúng trong top-N. VD: Item đúng ở đầu danh sách sẽ được điểm cao hơn.
        'NDCG@N Mean': 'NDCG@N',
        # Đo vị trí xuất hiện của item đúng đầu tiên. VD: Nếu item đúng đầu tiên ở vị trí 3 thì MRR = 1/3.
        'MRR Mean': 'MRR',
        # Đo tỷ lệ có ít nhất một item đúng trong top-N gợi ý.
        'Hit Rate Mean': 'Hit Rate'
    }

    # Chỉ lấy những metric có trong df
    metrics = [m for m in metric_labels_kfold.keys() if m in cv_results_df.columns]
    plot_models = cv_results_df.set_index('Model')

    # Lấy tên mới cho legend
    new_labels = [metric_labels_kfold.get(m, m) for m in metrics]

    fig, ax = plt.subplots(figsize=(16, 8))
    # Plot với errorbar
    bars = plot_models[metrics].plot(
        kind='barh',
        yerr=plot_models[[m.replace('Mean', 'Std') for m in metrics]],
        ax=ax,
        width=0.7,
        legend=False
    )
    ax.set_title('Recommendation Metrics (K-Fold CV)', fontsize=18)
    ax.set_xlabel('Values', fontsize=14)
    ax.set_ylabel('Models', fontsize=14)
    ax.grid(axis='x', linestyle='--', alpha=0.7)

    handles, labels_ = bars.get_legend_handles_labels()
    ax.legend(
        handles, new_labels,
        title='Metrics',
        bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=12, title_fontsize=13, frameon=True
    )
    plt.tight_layout(rect=[0, 0, 0.82, 1])
    save_path = os.path.join(training_dir, 'K_Fold_result.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

visualize_kfold_results(cv_results_df)

"""## [5.25] - Fine-tuning models"""

param_grids = {
    'Ridge Regression': {
        'alpha': [0.01, 0.1, 1.0, 10.0]
    },
    'Random Forest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    },
    'LightGBM': {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 7, -1],
        'learning_rate': [0.01, 0.05, 0.1],
        'num_leaves': [31, 50, 100]
    },
    'XGBoost': {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 7, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 1.0]
    },
    'CatBoost': {
        'iterations': [50, 100, 200],
        'depth': [3, 6, 10],
        'learning_rate': [0.01, 0.05, 0.1]
    }
}

"""### Demo recommendation cho các model sau tuning"""

def get_top_5_with_scores(model, user_id, items, user_profiles, item_features, N=5):
    # Hàm này giữ nguyên như phrase 2
    if model.__class__.__name__ in ['CBModel', 'ItemCFModel', 'HybridModel']:
        scores = model.predict(user_id, items)
        item_scores = list(zip(items, scores))
        top_5 = sorted(item_scores, key=lambda x: x[1], reverse=True)[:N]
        return [(item, score) for item, score in top_5]
    else:
        rec_items, rec_scores = recommend_ml_top_n(model, user_id, items, user_profiles, item_features, N)
        return list(zip(rec_items, rec_scores))

"""### Optuna tuning cho từng model"""

optuna.logging.set_verbosity(optuna.logging.WARNING)

def optuna_objective_factory(name, model, X, y, param_grid):
    def objective(trial):
        params = {}
        for param, values in param_grid.items():
            params[param] = trial.suggest_categorical(param, values)
        # Always create a new model instance (fix CatBoostError)
        if name == "CatBoost":
            _model = CatBoostRegressor(verbose=0, random_state=42, **params)
        elif name == "LightGBM":
            _model = LGBMRegressor(random_state=42, verbose=-1, **params)
        elif name == "XGBoost":
            _model = XGBRegressor(random_state=42, verbosity=0, **params)
        elif name == "Random Forest":
            _model = RandomForestRegressor(random_state=45, **params)
        elif name == "Ridge Regression":
            _model = Ridge(**params)
        else:
            _model = model.__class__(**params)
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(
            _model, X, y, cv=3, scoring='neg_mean_squared_error', n_jobs=-1
        )
        return scores.mean()
    return objective

"""### Tuning Result"""

tuned_results = []
tuned_models = {}

for name, model in all_models.items():
    print(f"\n--- Tuning model: {name} (Optuna) ---")
    param_grid = param_grids.get(name, {})
    if param_grid:
        study = optuna.create_study(direction='maximize')
        study.optimize(
            optuna_objective_factory(name, model, X_train, y_train, param_grid),
            n_trials=30,
            show_progress_bar=True
        )
        best_params = study.best_params
        # Always fit a new instance for the best params (no set_params)
        if name == "CatBoost":
            tuned_model = CatBoostRegressor(verbose=0, random_state=42, **best_params)
        elif name == "LightGBM":
            tuned_model = LGBMRegressor(random_state=42, verbose=-1, **best_params)
        elif name == "XGBoost":
            tuned_model = XGBRegressor(random_state=42, verbosity=0, **best_params)
        elif name == "Random Forest":
            tuned_model = RandomForestRegressor(random_state=45, **best_params)
        elif name == "Ridge Regression":
            tuned_model = Ridge(**best_params)
        else:
            tuned_model = model.__class__(**best_params)
        tuned_model.fit(X_train, y_train)
    else:
        tuned_model = model
        tuned_model.fit(X_train, y_train)
        best_params = {}

    tuned_models[f'Tuned {name}'] = tuned_model

    ranking_metrics = evaluate_ranking_vectorized(
        tuned_model, test_df, items, category_cols, X_test, y_test
    )
    coverage = evaluate_coverage(
        tuned_model, test_df, items, N=5, user_profiles=user_profiles, item_features=item_features, model_type="ml", ml_model=tuned_model
    )
    tuned_results.append({
        'Model': f'Tuned {name}',
        **ranking_metrics,
        'Coverage': coverage,
        'Best Params': best_params
    })

tuned_results_df = pd.DataFrame(tuned_results)
print("\n### Kết quả sau tuning (Optuna):")
display(tuned_results_df)

"""### Comparasion models after tuning"""

cb_cf_hybrid_results['Best Params'] = [{} for _ in range(cb_cf_hybrid_results.shape[0])]
tuned_comparison_df = pd.concat([tuned_results_df, cb_cf_hybrid_results], ignore_index=True)

desired_order = [
    'Model', 'Precision@N', 'Recall@N', 'F1@N', 'NDCG@N', 'MRR', 'Hit Rate', 'Coverage'
]
tuned_comparison_df = tuned_comparison_df.reindex(columns=desired_order)

print('\n=== So sánh metrics giữa CB/CF/Hybrid và các model ML SAU tuning:')
display(tuned_comparison_df)

"""### Visualize result after tuning

"""

def visualize_tuned_results(tuned_comparison_df):
    # Các metric cần vẽ
    metrics = ['Precision@N', 'Recall@N', 'F1@N', 'NDCG@N', 'MRR', 'Hit Rate']
    # Lấy chỉ các metrics có trong DataFrame
    metrics = [m for m in metrics if m in tuned_comparison_df.columns]

    # Chuyển DataFrame về dạng dễ vẽ
    plot_data = tuned_comparison_df[['Model'] + metrics].copy()
    n_models = len(plot_data)
    n_metrics = len(metrics)

    # Thiết lập màu cho từng metric
    colors = {
        'Precision@N': '#1f77b4',
        'Recall@N':    '#ff7f0e',
        'F1@N':        '#2ca02c',
        'NDCG@N':      '#d62728',
        'MRR':         '#9467bd',
        'Hit Rate':    '#8c564b',
    }

    bar_width = 0.12
    index = np.arange(n_models)

    plt.figure(figsize=(14, 8))
    for i, metric in enumerate(metrics):
        values = plot_data[metric]
        bars = plt.bar(
            index + i * bar_width,
            values,
            bar_width,
            label=metric,
            color=colors.get(metric, '#333333')
        )
        # Ghi giá trị lên bar
        for bar in bars:
            h = bar.get_height()
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                h + 0.01,
                f'{h:.3f}',
                ha='center',
                va='bottom',
                fontsize=8
            )

    # Nhãn và tiêu đề
    plt.xlabel('Model', fontsize=12)
    plt.ylabel('Value', fontsize=12)
    plt.title('Comparison of Recommendation Metrics', fontsize=14)
    plt.xticks(
        index + bar_width * (n_metrics - 1) / 2,
        plot_data['Model'],
        rotation=45,
        ha='right'
    )
    plt.ylim(0, 1.1)
    plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    save_path = os.path.join(training_dir, 'Metric_models_after_fine-tuning.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')


    plt.show()

visualize_tuned_results(tuned_comparison_df)

"""## Demo Recommned product for user after tuning"""

# Demo với các user mẫu
print("\n--- DEMO kết quả recommend (sau fine-tune Optuna) trên test set ---")
for user_id in demo_users:
    print(f"\nUser: {user_id}")
    for name, model in [('CB Model', cb_model), ('Item-CF Model', item_cf_model), ('Hybrid Model', hybrid_model)]:
        top5 = get_top_5_with_scores(model, user_id, items, user_profiles, item_features, N=5)
        print(f"{name} Top-5:")
        for rank, (item, score) in enumerate(top5, 1):
            print(f"    {rank}. {item:<25} | Score: {score:.4f}")

    for model_name, tuned_model in tuned_models.items():
        top5 = get_top_5_with_scores(tuned_model, user_id, items, user_profiles, item_features, N=5)
        print(f"{model_name} Top-5:")
        for rank, (item, score) in enumerate(top5, 1):
            print(f"    {rank}. {item:<25} | Score: {score:.4f}")