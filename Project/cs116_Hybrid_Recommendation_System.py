# -*- coding: utf-8 -*-
"""CS116 - Đồ án

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pQFUGIe-vrwqfnYG9dTfnpsBlx4VXSYa

# CS116.P22 - ĐỒ ÁN

## DANH SÁCH THÀNH VIÊN

1. **Đinh Lê Bình An** - `23520004`
2. **Vũ Gia Khang** - `23520713`
3. **Bùi Nhật Anh Khôi** - `23520761`
4. **Nguyễn Khang Hy** - `23520662`

## Tên bộ dữ liệu:  [Consumer Behavior and Shopping Habits Dataset](https://www.kaggle.com/datasets/zeesolver/consumer-behavior-and-shopping-habits-dataset)

Tên bài toán: **Customer behavior analysis for product recommendations (Đề xuất sản phẩm dựa trên hành vi mua sắm của khách hàng)**
"""

from google.colab import files
files.upload()  # Chọn tệp kaggle.json từ máy tính của bạn

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

import os
root_dir = "/content"
dataset_dir = os.path.join(root_dir, "datasets")

if not os.path.exists(dataset_dir):
    os.makedirs(dataset_dir)

import kaggle
kaggle.api.authenticate()  # Xác thực bằng API key từ tệp kaggle.json
# Đổi tên biến dataset_name theo dataset mong muốn:
dataset_name = "zeesolver/consumer-behavior-and-shopping-habits-dataset"
kaggle.api.dataset_download_files(dataset_name, path=dataset_dir, unzip=True)
print(f"Dataset đã được tải về tại: {dataset_dir}")

dataset_path = '/content/datasets/shopping_behavior_updated.csv'

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

df = pd.read_csv(dataset_path)
print("Đọc file CSV thành công.")

"""# Bước 1: Đăng ký đề tài

## 1.1 Lý do chọn dữ liệu

Bộ dữ liệu này rất phù hợp để phân tích hành vi mua sắm của khách hàng:
- Cung cấp một cái nhìn toàn diện về các yếu tố ảnh hưởng đến quyết định mua hàng như độ tuổi, giới tính, sở thích sản phẩm, phương thức thanh toán, hình thức giao hàng và tần suất mua sắm.
- Thông qua việc khai thác các thông tin như sản phẩm đã mua, mức chi tiêu, đánh giá của khách hàng, và lịch sử mua hàng trước đó, chúng ta có thể hiểu rõ hơn về xu hướng tiêu dùng, mức độ trung thành và khả năng quay lại của từng nhóm khách hàng.

**Những hiểu biết này sẽ hỗ trợ mạnh mẽ cho việc xây dựng hệ thống gợi ý sản phẩm cá nhân hóa, giúp các doanh nghiệp thương mại điện tử tối ưu hóa chiến lược tiếp thị, nâng cao trải nghiệm người dùng, và phát triển các chương trình giữ chân khách hàng một cách hiệu quả hơn**.

## 1.2 Các thông tin về bộ dữ liệu

- Số dòng: 3900 dòng
- Số cột: 18 cột
- Loại bài toán: Đề xuất sản phẩm dựa trên thói quen mua đồ của khách hàng

- **Customer ID**: Là khoá chính định danh từng khách hàng, hỗ trợ theo dõi hành vi theo thời gian và phân tích mức độ trung thành.

- **Age**: Dữ liệu định lượng giúp phân khúc khách hàng theo độ tuổi, từ đó cá nhân hoá nội dung tiếp thị.

- **Gender**: Biến phân loại giúp xác định sự khác biệt trong hành vi tiêu dùng giữa nam và nữ.

- **Item Purchased**: Là mục tiêu chính trong bài toán gợi ý; thể hiện rõ sản phẩm mà khách hàng quan tâm.

- **Category**: Cho phép nhóm các sản phẩm cùng loại để phân tích xu hướng theo danh mục (ví dụ: thời trang, điện tử...).

- **Purchase Amount (USD)**: Dữ liệu định lượng cho biết sức chi tiêu, giúp đánh giá khách hàng có giá trị cao (high-value customers).

- **Location**: Cho phép phân tích sự khác biệt hành vi mua sắm theo vùng miền hoặc quốc gia.

- **Size**: Áp dụng cho các sản phẩm thời trang , hỗ trợ kiểm tra mức độ phù hợp kho hàng theo kích cỡ phổ biến.

- **Color**: Phân tích thị hiếu về màu sắc để cải thiện thiết kế và quản lý tồn kho hiệu quả hơn.

- **Season**: Hữu ích trong việc dự đoán nhu cầu theo mùa và lập kế hoạch nhập hàng.

- **Review Rating**: Đo lường sự hài lòng của khách hàng, có thể dùng làm chỉ số đầu vào cho mô hình gợi ý chất lượng.

- **Subscription Status**: Phân biệt giữa khách hàng thường và khách hàng trung thành (đăng ký dịch vụ định kỳ).

- **Shipping Type**: Phân tích lựa chọn giao hàng để tối ưu thời gian & chi phí vận chuyển.

- **Discount Applied**: Xác định mức độ nhạy cảm với giá và tác động của giảm giá đến hành vi mua hàng.

- **Promo Code Used**: Cho biết hiệu quả của các chiến dịch marketing qua mã khuyến mãi.

- **Previous Purchases**: Là một chỉ số quan trọng để xác định khách hàng trung thành và mô hình dự đoán mua lại.

- **Payment Method**: Giúp doanh nghiệp xác định phương thức thanh toán phổ biến và cải thiện cổng thanh toán.

- **Frequency of Purchases**: Đánh giá tần suất mua hàng, là chỉ số then chốt trong mô hình dự đoán giá trị vòng đời khách hàng (CLV).

## 1.3 Một số nhận xét ban đầu về bộ dữ liệu

● **Thông tin dữ liệu**: Dataset bao gồm các trường quan trọng phản ánh hành vi tiêu dùng như: `thông tin khách hàng` (Customer ID, Age, Gender, Location), `thông tin sản phẩm` (Item Purchased, Category, Size, Color, Season), `chi tiêu và đánh giá` (Purchase Amount, Review Rating), cũng như `thói quen và hình thức mua sắm` (Previous Purchases, Frequency of Purchases, Subscription Status, Payment Method, Shipping Type, Promo Code Used, Discount Applied).

● **Tình trạng dữ liệu**: Bộ dữ liệu được mô tả khá đầy đủ, đa dạng và có tính ứng dụng cao. Các trường thông tin có giá trị rõ ràng, thuận lợi cho việc xử lý và phân tích dữ liệu. Chưa có dấu hiệu rõ ràng về thiếu hụt dữ liệu ở mức nghiêm trọng trong phần mô tả.

● **Định dạng dữ liệu**: Kết hợp giữa nhiều kiểu dữ liệu như số nguyên (int64), số thực (float64) và chuỗi (object), phù hợp để thực hiện các phân tích mô tả, trích xuất insight và huấn luyện các mô hình học máy.

● **Phân tích khách hàng**: Dữ liệu cho phép phân khúc người dùng theo nhân khẩu **học, hành vi chi tiêu, xu hướng sử dụng mã khuyến mãi, lựa chọn phương thức thanh toán hay hình thức vận chuyển. Đây là cơ sở vững chắc để xây dựng hệ thống cá nhân hóa và đề xuất thông minh.

# Bước 2: Phân tích dữ liệu - EDA

### Kiểm tra dữ liệu ban đầu
"""

print("5 dòng đầu tiên của dữ liệu:")
display(df.head())

print("\nThông tin tổng quan về DataFrame:")
df.info()

print(f"\nKích thước DataFrame: {df.shape}")
print("\nThống kê mô tả cho các cột số:")
display(df.describe())

print("\nThống kê mô tả các cột:")
display(df.describe())

print("\nThống kê mô tả cho các cột phân loại:")
display(df.describe(include='object'))

"""#### Kiểm tra giá trị thiếu (NULL)"""

print("Kiểm tra số lượng giá trị thiếu (NULL) cho mỗi cột:")
missing_values = df.isnull().sum()
missing_info = pd.DataFrame({
    'Số lượng thiếu': missing_values,
})
display(missing_info)

"""#### Kiểm tra dữ liệu trùng lặp"""

# check duplicate value

print(df.duplicated().sum())

"""## Nhận xét ban đầu:
- **Kích thước:** Tập dữ liệu gồm 3900 hàng và 19 cột.

- **Kiểu dữ liệu:** Các cột có vẻ có kiểu dữ liệu phù hợp (số nguyên, số thực, object/chuỗi). Cột `Customer ID` là định danh.

- **Giá trị thiếu:** Kiểm tra bằng `df.isnull().sum()` xác nhận **không có giá trị thiếu (NULL)** nào trong toàn bộ dữ liệu. Điều này rất tốt cho việc phân tích.

- **Gia trị trùng lặp**: Kiểm tra bằng `df.duplicated().sum()` xác nhận **không có giá trị trùng lặp** nào trong toàn bộ dữ liệu. Điều này rất tốt cho việc phân tích.


- **Nhóm thông tin về dữ liệu**
  + Thông tin khách hàng: `Customer ID`, `Gender`, `Age`, `Location`, `Subscripttion status`
  + Thông tin sản phẩm và giao dịch: `Item Purchased`, `Category`, `Size`, `Color`, `Season`, `Purchase Amount (USD)`, `Discount Applied`, `Promo Code Used`
  + Thông tin về hành vi mua sắm: `Frequency of Purchases`, `Previous Purchases`, `Review Rating`, `Shipping type`, `Payment Method`


- **Thống kê số:**
  - `Age`: Tuổi từ 18 đến 70, trung bình khoảng 44. Tuổi phân bố khá rộng.
  - `Purchase Amount (USD)`: Số tiền mua hàng từ 20 đến 100 USD, trung bình khoảng 60 USD. Phân bố đều.
  - `Review Rating`: Đánh giá từ 2.5 đến 5.0, trung bình 3.7. Đa số đánh giá ở mức khá cao.
  - `Previous Purchases`: Số lượt mua trước đó từ 1 đến 50, trung bình 25. Phân bố đều.

- **Thống kê phân loại:**
  - Có nhiều cột phân loại cần khám phá chi tiết hơn (ví dụ: `Gender`, `Item Purchased`, `Category`, `Location`, `Payment Method`).
  - `Gender`: Có 2 giá trị duy nhất, `Male` là phổ biến nhất cho thấy nam giới có xu hướng mua sắm nhiều hơn nữ giới.
  - `Item Purchased`: Có 25 mặt hàng khác nhau, `Blouse` phổ biến nhất
  - `Category`: Có 4 danh mục, `Clothing` phổ biến nhất.
  - `Location`: Có 50 địa điểm khác nhau, `Montana` phổ biến nhất.
  - `Size`: có 4 loại, `M` là phổ biến nhất.
  - `Color`: Có 7 màu sắc khác nhau, `Olive` là phổ biến nhất.
  - `Season`: Có 4 mùa, `Spring` phổ biến nhất vì đây là mùa xuân mát mẻ và dễ chịu cho việc mua sắm dẫn đến người dân chi tiêu nhiều hơn cho các sản phẩm về quần áo vì dụ như áo blouse, đặc biệt là thành phố `Montana` nơi có khí hậu lạnh hơn.
  - `Payment Method`: Có 6 phương thức, `Credit Card` là phương thức thường được dùng để thanh toán vì độ tiện lợi và nhanh chóng nhưng phương thích ưu thích của khách hàng lại là `PayPal` vì tính bảo mật và an toàn hơn, đặc biệt là khi mua sắm trực tuyến.
  - `Shipping type`: có 6 loại, `Free Shipping` là lựa chọn ưu tiên của khách hàng với mong muốn tiết kiệm chi tiết cho phí vận chuyển và tập trung số tiền vào sản phẩm
  - `Frequency of Purchases`: Có 7 tần suất mua hàng, `Every 3 Months` là tần suất mua hàng phố biến nhất vì đây là thời điểm mà người tiêu dùng thường có nhu cầu mua sắm nhiều hơn, đặc biệt là vào mùa xuân và mùa hè.

## Phân tích đơn biến (Univariate Analysis)

#### 5.1. Phân tích biến số (Numerical Variables)
"""

# Phân bố tuổi
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Age', color='blue')
plt.title('Boxplot of Age')
plt.xlabel('Age')
plt.show()

# Phân bố Purchase Amount (USD)
plt.figure(figsize=(10, 6))
sns.histplot(df, x='Purchase Amount (USD)', bins=30, kde=True, color='green')
plt.title('Distribution of Purchase Amount (USD)')
plt.xlabel('Purchase Amount (USD)')
plt.ylabel('Frequency')
plt.show()

# Phân bố review rating
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x='Review Rating', fill=True, color='blue')
plt.title("Distribution of Review Rating")
plt.xlabel("Review Rating")
plt.ylabel("Density")
plt.show()

# Phân bố Previous Purchase
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Previous Purchases', bins=30, kde=True, color='purple')
plt.title("Distribution of Previous Purchases")
plt.xlabel("Previous Purchases")
plt.ylabel("Frequency")
plt.show()

"""#### 5.2. Phân tích biến phân loại (Categorical Variables)"""

categorical_columns = df.select_dtypes(include=['object']).columns
cols_to_plot = ['Gender', 'Category', 'Item Purchased', 'Size', 'Season', 'Location',
                    'Subscription Status', 'Payment Method', 'Shipping Type',
                    'Discount Applied', 'Promo Code Used', 'Frequency of Purchases']

for col in categorical_columns:
    n_unique = df[col].nunique()
    if col in cols_to_plot or n_unique <= 20:
        plt.figure(figsize=(12, 5))
        order = df[col].value_counts().index

        if n_unique > 15:
            top_n = 15
            value_counts = df[col].value_counts().nlargest(top_n)
            sns.barplot(x=value_counts.index, y=value_counts.values, hue=value_counts.index, palette='viridis', order=value_counts.index, legend=False)
            plt.title(f'Top {top_n} Phân phối của {col}', fontsize=14)
            print(f"* Hiển thị Top {top_n} loại phổ biến nhất (Tổng số loại: {n_unique})")
        else:
            value_counts = df[col].value_counts()
            sns.barplot(x=value_counts.index, y=value_counts.values, hue=value_counts.index, palette='viridis', order=order, legend=False)
            plt.title(f'Phân phối của {col}', fontsize=14)

        plt.xlabel(col)
        plt.ylabel('Số lượng')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

        print(f"Tần suất của '{col}':")
        print(value_counts)
    else:
        print(f"* Cột '{col}' có {n_unique} giá trị duy nhất. Chỉ hiển thị 5 giá trị phổ biến nhất:")
        print(df[col].value_counts().head())
    print("-"*60)

"""**Nhận xét biến phân loại:**
- `Gender`: **Nam (Male) chiếm đa số** (khoảng 68%), gấp đôi Nữ (Female). Đây là một điểm mất cân bằng cần lưu ý.
- `Category` (Nhãn tiềm năng): **Clothing (Quần áo)** là danh mục phổ biến nhất, chiếm gần một nửa số giao dịch. Tiếp theo là Accessories (Phụ kiện), Footwear (Giày dép) và Outerwear (Áo khoác ngoài). Phân phối không cân bằng.
- `Item Purchased` (Nhãn tiềm năng): Có 25 loại mặt hàng. **Blouse, Jeans, Pants, Shirt, Dress** là những mặt hàng được mua nhiều nhất. Phân phối khá đa dạng, không quá tập trung vào một vài mặt hàng.
- `Location`, `Color`: Có quá nhiều giá trị duy nhất (50 và 25), biểu đồ đầy đủ sẽ khó đọc. Montana là địa điểm phổ biến nhất, Olive/Yellow/Silver/Teal/Green là các màu phổ biến.
- `Size`: Size **M (Medium)** là phổ biến nhất, chiếm hơn 40%. Tiếp theo là L (Large), S (Small), và XL (Extra Large).
- `Season`: Các mùa có số lượng mua hàng **khá cân bằng**, với Spring (Xuân) và Fall (Thu) nhỉnh hơn một chút so với Winter (Đông) và Summer (Hè).
- `Subscription Status`: Đa số khách hàng **không đăng ký (No)**, chiếm khoảng 73%.
- `Payment Method`, `Preferred Payment Method`: **PayPal và Credit Card** là hai phương thức thanh toán/ưu tiên thống trị. Các phương thức khác ít phổ biến hơn. Cần xem xét mối quan hệ giữa hai cột này.
- `Shipping Type`: **Free Shipping (Miễn phí vận chuyển)** là lựa chọn phổ biến nhất, tiếp theo là Standard (Tiêu chuẩn) và Store Pickup (Nhận tại cửa hàng). Các loại hình vận chuyển nhanh ít được sử dụng hơn.
- `Discount Applied`, `Promo Code Used`: Phần lớn các giao dịch **có áp dụng giảm giá/mã khuyến mãi (Yes)**, chiếm khoảng 57%. Hai cột này có vẻ liên quan chặt chẽ.
- `Frequency of Purchases`: Tần suất mua hàng **phân bố khá đều** giữa các lựa chọn (Weekly, Fortnightly, Monthly, Quarterly, Bi-Weekly, Annually, Every 3 Months), mỗi loại chiếm khoảng 13-15%.

## Phân tích hai biến (Bivariate Analysis)

#### Tương quan giữa các biến số (Feature-Feature: Numerical)
"""

# Tính ma trận tương quan
numerical_columns = df.select_dtypes(include=np.number).columns
correlation_matrix = df[numerical_columns].corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Ma trận Tương quan Pearson giữa các Biến số', fontsize=16)
plt.show()
print("Ma trận tương quan Pearson:")
display(correlation_matrix)

"""**Nhận xét ma trận tương quan:**
- Các hệ số tương quan Pearson giữa các cặp biến số (`Age`, `Purchase Amount (USD)`, `Review Rating`, `Previous Purchases`) đều **rất gần 0** (giá trị tuyệt đối < 0.04).
- Điều này cho thấy **hầu như không có mối quan hệ tuyến tính** nào đáng kể giữa các biến số này.
- Ví dụ: Tuổi của khách hàng không liên quan tuyến tính đến số tiền họ chi tiêu, đánh giá của họ, hay số lần mua hàng trước đó.
- Các biến số này cung cấp thông tin tương đối độc lập với nhau.

#### Mối quan hệ giữa biến số và biến phân loại (Feature-Feature/Label: Numerical vs Categorical)
"""

num_vars = ['Purchase Amount (USD)', 'Review Rating', 'Age']
cat_vars = ['Category', 'Gender', 'Subscription Status', 'Season', 'Frequency of Purchases']

for num_var in num_vars:
    for cat_var in cat_vars:
        if (num_var == 'Age' and cat_var not in ['Gender', 'Frequency of Purchases']) or \
            (num_var == 'Review Rating' and cat_var not in ['Subscription Status', 'Category']):
            continue
        n_unique_cat = df[cat_var].nunique()
        if n_unique_cat > 10:
                continue
        plt.figure(figsize=(12, 5))
        # Box Plot
        plt.subplot(1, 2, 1)
        sns.boxplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='viridis', legend=False)
        plt.title(f'Box Plot: {num_var} theo {cat_var}', fontsize=12)
        plt.xlabel(cat_var)
        plt.ylabel(num_var)
        plt.xticks(rotation=45, ha='right')

        # Violin Plot (cung cấp thêm thông tin về mật độ)
        plt.subplot(1, 2, 2)
        sns.violinplot(data=df, x=cat_var, y=num_var, hue=cat_var, palette='plasma', legend=False)
        plt.title(f'Violin Plot: {num_var} theo {cat_var}', fontsize=12)
        plt.xlabel(cat_var)
        plt.ylabel(num_var)
        plt.xticks(rotation=45, ha='right')

        plt.suptitle(f'Phân tích {num_var} và {cat_var}', fontsize=14, y=1.02)
        plt.tight_layout()
        plt.show()
        print("-"*60)

"""**Nhận xét (Numerical vs Categorical):**
- `Purchase Amount vs Category`: Phân phối số tiền mua hàng (trung vị, tứ phân vị, mật độ) **khá tương đồng** giữa các danh mục (`Clothing`, `Footwear`, `Outerwear`, `Accessories`). Violin plot xác nhận phân phối gần như đồng đều trong mỗi danh mục. Không có danh mục nào nổi bật về mức chi tiêu cao hơn hẳn.
- `Purchase Amount vs Gender`: Phân phối số tiền mua hàng giữa **Nam và Nữ gần như giống hệt nhau**. Giới tính dường như không ảnh hưởng đến số tiền chi tiêu trung bình cho mỗi giao dịch.
- `Purchase Amount vs Season`: Phân phối số tiền mua hàng **không thay đổi đáng kể theo mùa**. Mức chi tiêu trung bình và khoảng biến thiên tương tự nhau qua các mùa.
- `Review Rating vs Category`: Đánh giá trung bình có vẻ **cao hơn một chút cho Footwear và Outerwear** so với Clothing và Accessories, nhưng sự khác biệt không lớn. Phân phối đánh giá lệch trái (nhiều điểm cao) ở tất cả các danh mục.
- `Review Rating vs Subscription Status`: Khách hàng đã đăng ký (`Yes`) có xu hướng đưa ra **đánh giá trung bình cao hơn một chút** so với khách hàng không đăng ký (`No`). Violin plot cho thấy mật độ đánh giá cao (4-5) dày đặc hơn ở nhóm 'Yes'.
- `Age vs Gender`: Phân phối tuổi giữa **Nam và Nữ rất giống nhau**, đều gần như đồng đều.
- `Age vs Frequency of Purchases`: Phân phối tuổi **khá tương đồng** giữa các nhóm tần suất mua hàng khác nhau. Không có nhóm tuổi nào đặc biệt ưa chuộng một tần suất mua hàng cụ thể.

### Mối quan hệ giữa các biến phân loại (Feature-Feature: Categorical vs Categorical)

- Category vs Gender
"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Category', hue='Gender', palette='pastel')
plt.title('Phân phối Danh mục theo Giới tính', fontsize=14)
plt.xlabel('Danh mục')
plt.ylabel('Số lượng')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""- Season vs Subscription Status"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Season', hue='Subscription Status', palette='coolwarm', order=['Spring', 'Summer', 'Fall', 'Winter'])
plt.title('Phân phối Trạng thái đăng ký theo Mùa', fontsize=14)
plt.xlabel('Mùa')
plt.ylabel('Số lượng')
plt.tight_layout()
plt.show()

"""- Discount Applied vs Promo Code Used"""

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Discount Applied', hue='Promo Code Used', palette='magma')
plt.title('Mối quan hệ giữa Áp dụng giảm giá và Sử dụng mã KM', fontsize=14)
plt.xlabel('Đã áp dụng giảm giá')
plt.ylabel('Số lượng')
plt.tight_layout()
plt.show()
# Bảng chéo để xem chi tiết
print("Bảng chéo: Discount Applied vs Promo Code Used")
display(pd.crosstab(df['Discount Applied'], df['Promo Code Used']))

"""- Phân tích nhãn tiềm năng 'Item Purchased' với 'Gender'"""

plt.figure(figsize=(12, 8))
top_items = df['Item Purchased'].value_counts().nlargest(10).index
sns.countplot(data=df[df['Item Purchased'].isin(top_items)], y='Item Purchased', hue='Gender', palette='pastel', order=top_items)
plt.title('Top 10 Mặt hàng được mua nhiều nhất theo Giới tính', fontsize=14)
plt.xlabel('Số lượng')
plt.ylabel('Mặt hàng')
plt.legend(title='Giới tính')
plt.tight_layout()
plt.show()

"""- Gender and Size"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Gender', hue='Size', palette='Set2')
plt.title("Count Plot of Size by Gender")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.legend(title="Size")
plt.show()

"""- Discount applied vs Gender"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Discount Applied', hue='Gender', palette='Set2')
plt.title("Count Plot of Discount Applied by Gender")
plt.xlabel("Discount Applied")
plt.ylabel("Count")
plt.legend(title="Gender")
plt.show()

"""**Nhận xét (Categorical vs Categorical):**
- `Category vs Gender`: Ở **mọi danh mục**, số lượng mua hàng của **Nam đều nhiều hơn Nữ**, phản ánh tỷ lệ giới tính chung trong dữ liệu (Nam chiếm ~68%). Tỷ lệ phân bố các danh mục trong nhóm Nam và Nữ có vẻ tương tự nhau (Clothing là phổ biến nhất cho cả hai).
- `Season vs Subscription Status`: Tỷ lệ khách hàng **không đăng ký (No) luôn cao hơn** khách hàng có đăng ký (Yes) ở tất cả các mùa. Không có sự khác biệt rõ rệt về xu hướng đăng ký theo mùa; tỷ lệ Yes/No khá ổn định qua các mùa.
- `Discount Applied vs Promo Code Used`: Có một **mối quan hệ cực kỳ chặt chẽ** giữa hai biến này. Bảng chéo cho thấy 100% các trường hợp `Discount Applied = Yes` đều tương ứng với `Promo Code Used = Yes`, và 100% trường hợp `Discount Applied = No` tương ứng với `Promo Code Used = No`. Điều này xác nhận hai cột này chứa **thông tin hoàn toàn trùng lặp**.
- `Item Purchased vs Gender` (Phân tích nhãn tiềm năng): Xem xét top 10 mặt hàng phổ biến nhất, tỷ lệ Nam mua nhiều hơn Nữ được duy trì ở hầu hết các mặt hàng, phù hợp với tỷ lệ giới tính chung. Không có mặt hàng nào trong top 10 cho thấy sự ưa chuộng đặc biệt rõ rệt của một giới tính so với giới tính còn lại (ví dụ: tỷ lệ Nam/Nữ mua Blouse tương tự tỷ lệ Nam/Nữ mua Jeans).

## Phân tích đa biến (Multivariate Analysis)

## Pair Plot cho các biến số theo Giới tính

- Thêm cột 'Gender' vào danh sách để dùng làm hue
"""

cols_for_pairplot = numerical_columns.tolist() + ['Gender']
sns.pairplot(df[cols_for_pairplot], hue='Gender', diag_kind='kde', corner=True, palette='pastel')
plt.suptitle('Pair Plot các Biến số theo Giới tính', y=1.02, fontsize=16)
plt.show()

"""**Nhận xét Pair Plot:**
- **Tương quan:** Các biểu đồ phân tán (scatter plots) ở các ô không nằm trên đường chéo xác nhận lại kết quả từ ma trận tương quan: **không có mối quan hệ tuyến tính rõ ràng** giữa các cặp biến số (`Age`, `Purchase Amount (USD)`, `Review Rating`, `Previous Purchases`). Các điểm dữ liệu phân bố khá ngẫu nhiên.
- **Phân phối theo Giới tính:** Các biểu đồ phân phối mật độ (KDE) trên đường chéo cho thấy hình dạng phân phối của từng biến số là **tương tự nhau giữa Nam và Nữ**. Mặc dù số lượng Nam nhiều hơn, nhưng hình dạng chung của phân phối tuổi, số tiền mua hàng, đánh giá, và số lượt mua trước đó không khác biệt đáng kể giữa hai giới.

#### Phân tích Số tiền mua hàng theo Danh mục và Giới tính
"""

plt.figure(figsize=(12, 7))
sns.boxplot(data=df, x='Category', y='Purchase Amount (USD)', hue='Gender', palette='coolwarm')
plt.title('Số tiền mua hàng theo Danh mục và Giới tính', fontsize=16)
plt.xlabel('Danh mục')
plt.ylabel('Số tiền mua hàng (USD)')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Giới tính')
plt.tight_layout()
plt.show()

"""**Nhận xét (Số tiền mua hàng theo Danh mục và Giới tính):**
- Biểu đồ này kết hợp thông tin từ `Purchase Amount`, `Category`, và `Gender`.
- Nó tái khẳng định những gì đã thấy trong phân tích hai biến:
  - Phân phối số tiền mua hàng **khá giống nhau giữa các danh mục**.
  - Phân phối số tiền mua hàng **gần như giống hệt nhau giữa Nam và Nữ** trong từng danh mục.
- Không có sự tương tác đáng kể nào giữa Giới tính và Danh mục đối với Số tiền mua hàng. Ví dụ, không có chuyện Nam chi tiêu nhiều hơn Nữ đáng kể chỉ ở một danh mục cụ thể nào đó.

### 8. Tổng kết và Nhận xét chung:
- **Chất lượng dữ liệu:** Dữ liệu rất tốt, đầy đủ, **không có giá trị thiếu (NULL)** như đã kiểm tra. Các kiểu dữ liệu phù hợp.
- **Phân phối biến:**
  - Các biến số (`Age`, `Purchase Amount`, `Previous Purchases`) có phân phối gần như đồng đều.
  - `Review Rating` lệch trái nhẹ (nhiều đánh giá cao).
  - Biến `Gender` mất cân bằng (Nam nhiều hơn Nữ).
  - Các biến phân loại khác như `Category`, `Size`, `Payment Method`, `Shipping Type` có sự tập trung vào một vài giá trị phổ biến (`Clothing`, `M`, `PayPal`/`Credit Card`, `Free Shipping`).
  - `Season` và `Frequency of Purchases` phân bố khá đều.
- **Ngoại lai:** Các biến số chính (`Age`, `Purchase Amount`, `Previous Purchases`) **không có ngoại lai đáng kể** theo phương pháp IQR. `Review Rating` có một số điểm thấp nhưng nằm trong phạm vi hợp lệ và không ảnh hưởng lớn.
- **Mối quan hệ:**
  - **Không có tương quan tuyến tính** mạnh giữa các biến số.
  - Số tiền mua hàng (`Purchase Amount`) **không phụ thuộc nhiều** vào `Category`, `Gender`, hay `Season`.
  - Khách hàng đăng ký (`Subscription Status=Yes`) có xu hướng **đánh giá cao hơn một chút**.
  - `Discount Applied` và `Promo Code Used` chứa **thông tin hoàn toàn trùng lặp**.
- **Phân tích nhãn tiềm năng (cho bài toán đề xuất):**
  - `Category`: 'Clothing' chiếm đa số, các danh mục khác ít hơn đáng kể.
  - `Item Purchased`: Phân phối đa dạng hơn `Category`, có 25 mặt hàng, top 5 là Blouse, Jeans, Pants, Shirt, Dress. Không có sự khác biệt lớn về sở thích mặt hàng (trong top 10) giữa Nam và Nữ.
- **Hướng tiếp theo:**
  - Dữ liệu sạch và có cấu trúc tốt, sẵn sàng cho các bước tiếp theo như tiền xử lý (mã hóa biến phân loại) và xây dựng mô hình.
  - **Cần loại bỏ một trong hai cột `Discount Applied` hoặc `Promo Code Used`** do tính dư thừa.
  - Sự mất cân bằng giới tính cần được lưu ý nếu xây dựng mô hình dự đoán liên quan đến giới tính, nhưng có thể không ảnh hưởng nhiều đến mô hình đề xuất dựa trên sản phẩm/danh mục.
  - Dữ liệu phù hợp để xây dựng các mô hình đề xuất (ví dụ: lọc cộng tác dựa trên `Customer ID`, `Item Purchased`, `Review Rating`; hoặc lọc dựa trên nội dung/thuộc tính sản phẩm như `Category`, `Color`, `Season`) hoặc phân cụm khách hàng (dựa trên `Age`, `Frequency of Purchases`, `Category` ưa thích, v.v.).

# Bước 3: Tiền xử lý dữ liệu - Preprocessing
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(dataset_path)

pd.set_option('display.max_columns', None)

df.head()

"""- Kiểm tra lại giá trị thiếu"""

print("Kiểm tra giá trị thiếu:")
print(df.isnull().sum())

"""- Kiểm tra ngoại lai cho các cột số (sử dụng IQR)"""

numeric_cols_initial = ['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases']
plt.figure(figsize=(15, 5))
for i, col in enumerate(numeric_cols_initial, 1):
    plt.subplot(1, len(numeric_cols_initial), i)
    sns.boxplot(y=df[col])
    plt.title(f'Box Plot của {col}')
plt.tight_layout()
plt.show()

print("\nNhận xét ngoại lai:")
# Dựa trên EDA trước đó, không có ngoại lai đáng kể cần loại bỏ ở bước này.
# Nếu có, có thể xử lý bằng cách loại bỏ hoặc capping (ví dụ: thay thế bằng percentile 1 và 99).
# Ví dụ capping (chưa áp dụng):
# lower_bound = df['Review Rating'].quantile(0.01)
# upper_bound = df['Review Rating'].quantile(0.99)
# df['Review Rating'] = np.where(df['Review Rating'] < lower_bound, lower_bound, df['Review Rating'])
# df['Review Rating'] = np.where(df['Review Rating'] > upper_bound, upper_bound, df['Review Rating'])

unique_counts = df.nunique()
unique_counts

"""### 1. Tạo đặc trưng mới - `Product_ID`

**Mục tiêu**: Tạo mã định danh duy nhất cho sản phẩm, vì `Item Purchased` không đủ chi tiết để phân biệt các sản phẩm khác nhau.

- **Vấn đề:**

  - Item Purchased (như Blouse, Sweater) chỉ là tên sản phẩm, không bao gồm thông tin về Category, Size, Color.

  - Ví dụ: Blouse màu Gray, size L khác với Blouse màu Black, size M, nhưng chỉ có cùng Item Purchased.

  - Điều này gây khó khăn cho Collaborative Filtering và Content-Based Filtering.

- **Ý nghĩa:**

  - *Product_ID* đảm bảo mỗi sản phẩm được định danh rõ ràng, phù hợp để xây dựng ma trận *User-Item*.

  - Hỗ trợ Content-Based Filtering bằng cách giữ thông tin chi tiết về sản phẩm (danh mục, kích cỡ, màu sắc).

  - Giảm nhầm lẫn khi gợi ý sản phẩm (ví dụ: không gợi ý cùng một **BLOUSE** với màu/kích cỡ không phù hợp).
"""

df['Product_ID'] = (df['Item Purchased'] + '_' +
                    df['Category'] + '_' +
                    df['Size'] + '_' +
                    df['Color'])

df.head()

"""### 2. Mã hóa đặc trưng phân loại

Sử dụng One-Hot Encoding cho các biến có ít giá trị duy nhất và không có thứ tự tự nhiên (Gender, Category).

Sử dụng Label Encoding cho các biến có nhiều giá trị hơn hoặc có thể có thứ tự tiềm ẩn (mặc dù LabelEncoder không bảo toàn thứ tự, nó tiết kiệm chiều dữ liệu).
"""

one_hot_cols = ['Gender', 'Category']
df = pd.get_dummies(df, columns=one_hot_cols, prefix=one_hot_cols, dtype=int)

df.head()

categorical_cols = ['Location', 'Size', 'Color',
                    'Subscription Status', 'Shipping Type', 'Discount Applied',
                    'Promo Code Used', 'Payment Method']

le = LabelEncoder()
label_mappings = {}
for col in categorical_cols:
    df[col + '_encoded'] = le.fit_transform(df[col])
    label_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))

df.head()

"""#### 2.1 Kiểm tra tương quan giữa các đặc trưng đã mã hóa (Redundancy Check)"""

encoded_cols = [col for col in df.columns if '_encoded' in col or 'Gender_' in col or 'Category_' in col]
corr_matrix_encoded = df[encoded_cols].corr()
corr_matrix_encoded

# Hiển thị heatmap cho các tương quan cao
high_corr_threshold = 0.8
high_corr = corr_matrix_encoded[((corr_matrix_encoded > high_corr_threshold) | (corr_matrix_encoded < -high_corr_threshold)) & (corr_matrix_encoded != 1.0)]
high_corr

# Hiển thị heatmap cho các tương quan cao
plt.figure(figsize=(10, 8))
sns.heatmap(high_corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title(f'Tương quan cao (>{high_corr_threshold}) giữa các biến mã hóa')
plt.show()

print("Nhận xét về tương quan cao:")
print("- Gender_Female và Gender_Male có tương quan hoàn hảo -1. Một trong hai cột này là dư thừa và nên được loại bỏ để tránh multicollinearity.")
print("- Discount Applied_encoded và Promo Code Used_encoded có tương quan hoàn hảo 1. Một trong hai cột này là dư thừa.")

# Loại bỏ cột dư thừa (ví dụ: Gender_Male và Promo Code Used_encoded)
df = df.drop(columns=['Gender_Male', 'Promo Code Used_encoded'])
print("\nĐã loại bỏ cột Gender_Male và Promo Code Used_encoded.")

"""### 3. Chuẩn hóa đặc trưng số

Sử dụng StandardScaler để đưa các đặc trưng số về cùng một thang đo (trung bình 0, độ lệch chuẩn 1).

Điều này quan trọng cho các thuật toán nhạy cảm với thang đo như KNN, SVM, hoặc các phương pháp dựa trên khoảng cách.
"""

numeric_cols = ['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases']

scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
df[numeric_cols]

df[numeric_cols].head(5)

plt.figure(figsize=(14, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df[col], kde=True, bins=30, color='skyblue')
    plt.title(f'Phân phối của {col} (sau chuẩn hóa)', fontsize=12)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Tần suất', fontsize=10)
plt.tight_layout()
plt.show()

df.head()

"""### 4. Xử lý đặc trưng `Season`

- Mục tiêu: Mã hóa Season theo cách giữ được tính chu kỳ (Winter → Spring → Summer → Fall → Winter), hỗ trợ Context-Aware Recommendation.

- Phương pháp: Sử dụng mã hóa tuần hoàn (cyclical encoding) với sin và cos để biểu diễn chu kỳ mùa.
"""

season_map = {'Winter': 0, 'Spring': 1, 'Summer': 2, 'Fall': 3}
df['Season_cycle'] = df['Season'].map(season_map)
df['Season_sin'] = np.sin(2 * np.pi * df['Season_cycle'] / 4)
df['Season_cos'] = np.cos(2 * np.pi * df['Season_cycle'] / 4)

df['Season_cycle'].value_counts()

df['Season_sin'].value_counts()

df['Season_cos'].value_counts()

"""#### Vẽ scatter plot để kiểm tra tính chu kỳ"""

plt.figure(figsize=(8, 6))
plt.scatter(df['Season_sin'], df['Season_cos'], c=df['Season_cycle'], cmap='viridis')
plt.title('Mã hóa tuần hoàn của Season', fontsize=12)
plt.xlabel('Season_sin', fontsize=10)
plt.ylabel('Season_cos', fontsize=10)
plt.colorbar(label='Season_cycle')
plt.show()

df.head()

"""### 5. Tạo đặc trưng `Customer_Loyalty_Score`

Mục tiêu: Tạo một điểm số tổng hợp thể hiện mức độ trung thành của khách hàng, kết hợp số lần mua trước, tần suất mua và trạng thái đăng ký.

**Lưu ý về trọng số:** Các trọng số (0.4, 0.4, 0.2) được chọn dựa trên phán đoán heuristic. Trong thực tế, các trọng số này có thể được tối ưu hóa bằng các kỹ thuật như phân tích thành phần chính (PCA) hoặc dựa trên kết quả đánh giá mô hình (ví dụ: grid search nếu điểm số này được dùng trực tiếp trong mô hình).
"""

df['Frequency of Purchases'].unique()

"""- Gán điểm số dựa trên tần suất (cao hơn = thường xuyên hơn)"""

frequency_ranking = {
    'Annually': 1,
    'Quarterly': 2,
    'Every 3 Months': 2,
    'Monthly': 3,
    'Fortnightly': 4,
    'Weekly': 5,
    'Bi-Weekly': 6
}

df['Frequency_score'] = df['Frequency of Purchases'].map(frequency_ranking)

df['Frequency_score']

df['Frequency_score'].value_counts()

# Chuẩn hóa điểm tần suất để có thể kết hợp với các biến đã chuẩn hóa khác
scaler_freq = StandardScaler()
df['Frequency_score'] = scaler_freq.fit_transform(df[['Frequency_score']])

df['Frequency_score']

# Tính điểm trung thành - sử dụng các biến đã được chuẩn hóa/mã hóa
# Previous Purchases đã được chuẩn hóa (mean 0, std 1)
# Frequency_score_scaled đã được chuẩn hóa (mean 0, std 1)
# Subscription Status_encoded là 0 (No) hoặc 1 (Yes)
# Lưu ý: Trọng số 0.4, 0.4, 0.2 là heuristic, có thể cần tối ưu hóa.
df['Customer_Loyalty_Score'] = (df['Previous Purchases'] * 0.4 +
                               df['Frequency_score'] * 0.4 +
                               df['Subscription Status_encoded']* 0.2)

df['Customer_Loyalty_Score']

plt.figure(figsize=(8, 6))
sns.histplot(df['Customer_Loyalty_Score'], kde=True, bins=30, color='skyblue')
plt.title('Phân phối của Customer Loyalty Score', fontsize=12)
plt.xlabel('Loyalty Score', fontsize=10)
plt.ylabel('Tần suất', fontsize=10)
plt.show()

df.head()

"""### 6. Tạo đặc trưng `Dominant_Season` cho sản phẩm

**Mục tiêu**: Xác định mùa phổ biến nhất cho mỗi loại mặt hàng (`Item Purchased`), giúp hiểu rõ hơn về tính thời vụ của sản phẩm.

- **Vấn đề:**

  - Mùa ảnh hưởng mạnh đến lựa chọn sản phẩm (ví dụ: áo khoác vào Winter), giúp gợi ý chính xác hơn.

- **Ý nghĩa:**
  - Đặc trưng này có thể hữu ích cho việc gợi ý sản phẩm theo ngữ cảnh mùa hiện tại.
  - Ví dụ: Nếu đang là mùa đông, hệ thống có thể ưu tiên gợi ý các sản phẩm có `Dominant_Season` là 'Winter'.
"""

# Tính số lần xuất hiện của mỗi Item trong từng Season
product_season = df.groupby(['Item Purchased', 'Season']).size().unstack(fill_value=0)
product_season

# Xác định mùa có số lần xuất hiện cao nhất cho mỗi Item
product_season['Dominant_Season'] = product_season.idxmax(axis=1)
product_season['Dominant_Season']

product_season['Dominant_Season'].value_counts()

# Gộp đặc trưng Dominant_Season vào DataFrame chính
df = df.merge(product_season['Dominant_Season'], left_on='Item Purchased', right_index=True, how='left')

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Dominant_Season', hue='Dominant_Season',
              palette='Set2', order=['Spring', 'Summer', 'Fall', 'Winter'], legend=False)
plt.title('Phân bố Mùa phổ biến (Dominant Season) của các giao dịch', fontsize=12)
plt.xlabel('Mùa phổ biến của sản phẩm được mua', fontsize=10)
plt.ylabel('Số lượng giao dịch', fontsize=10)
plt.show()

print("Số lượng giao dịch theo Mùa phổ biến của sản phẩm:")
print(df['Dominant_Season'].value_counts())

df.head()

"""### 7. Tạo ma trận User-Item và `Interaction_Score`

**Mục tiêu**:
Tạo ma trận biểu diễn sự tương tác giữa người dùng và sản phẩm, cần thiết cho các thuật toán lọc cộng tác.

Tạo `Interaction_Score`: Kết hợp `Review Rating` và `Purchase Amount (USD)` (đã chuẩn hóa) để thể hiện mức độ tương tác/quan tâm của người dùng đối với sản phẩm.

**Lưu ý về trọng số:** Trọng số 0.7 cho `Review Rating` và 0.3 cho `Purchase Amount` là heuristic, ưu tiên đánh giá của người dùng. Có thể điều chỉnh hoặc sử dụng các phương pháp khác để xác định điểm tương tác (ví dụ: chỉ dùng rating, hoặc dùng mô hình phức tạp hơn).

- Tính Interaction_Score sử dụng các cột đã chuẩn hóa
"""

# Lưu ý: Trọng số 0.7 và 0.3 là heuristic, có thể cần tối ưu hóa.
df['Interaction_Score'] = (df['Review Rating'] * 0.7 + df['Purchase Amount (USD)'] * 0.3)

df['Interaction_Score']

"""#### Tạo ma trận User-Item"""

# Sử dụng Product_ID để định danh sản phẩm duy nhất
user_item_matrix = df.pivot_table(index='Customer ID',
                                columns='Product_ID',
                                values='Interaction_Score',
                                aggfunc='mean').fillna(0) # fillna(0) cho các sản phẩm chưa tương tác

user_item_matrix.head(10)

print(f"Kích thước ma trận User-Item: {user_item_matrix.shape}")
plt.figure(figsize=(12, 8))
# Hiển thị một phần nhỏ của heatmap do ma trận lớn
sns.heatmap(user_item_matrix.iloc[:20, :20], cmap='Blues', cbar=True)
plt.title('Heatmap của ma trận User-Item (20x20 đầu tiên)')
plt.xlabel('Product_ID')
plt.ylabel('Customer ID')
plt.show()

df.head()

"""### 8. Đánh giá tầm quan trọng của đặc trưng (Feature Importance - Ví dụ)

Mục tiêu: Xác định các đặc trưng có ảnh hưởng lớn nhất đến một biến mục tiêu (ví dụ: dự đoán `Subscription Status`). Điều này giúp lựa chọn đặc trưng hiệu quả và hiểu rõ hơn về dữ liệu.

Phương pháp: Sử dụng `feature_importances_` từ mô hình RandomForestClassifier.

**Lưu ý:** Đây chỉ là ví dụ minh họa cách đánh giá. Việc lựa chọn biến mục tiêu và giải thích kết quả phụ thuộc vào bài toán cụ thể. Trong bài toán đề xuất, tầm quan trọng có thể không phải là yếu tố duy nhất để chọn đặc trưng.
"""

from sklearn.ensemble import RandomForestClassifier # For Feature Importance
from sklearn.model_selection import train_test_split # For demonstrating split importance

"""- Chuẩn bị dữ liệu cho ví dụ Feature Importance
- Chọn các cột số và cột đã mã hóa (loại bỏ ID, các cột gốc chưa mã hóa, và các cột tạo ra sau)
"""

cols_to_drop_for_fi = ['Customer ID', 'Item Purchased', 'Category', 'Location', 'Size', 'Color', 'Season',
                       'Subscription Status', 'Payment Method', 'Shipping Type', 'Discount Applied',
                       'Promo Code Used', 'Frequency of Purchases', 'Product_ID', 'Season_cycle',
                       'Frequency_score_raw', 'Dominant_Season', 'Interaction_Score']

"""- Kiểm tra xem cột có tồn tại trước khi drop không"""

cols_to_drop_for_fi_existing = [col for col in cols_to_drop_for_fi if col in df.columns]
df_for_fi = df.drop(columns=cols_to_drop_for_fi_existing)

"""- Chọn biến mục tiêu (ví dụ: Subscription Status_encoded)"""

target_col = 'Subscription Status_encoded'
X = df_for_fi.drop(columns=[target_col])
y = df_for_fi[target_col]

"""- Xử lý tên cột có thể chứa ký tự không hợp lệ cho một số hiên bản sklearn/lightgbm"""

X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)

"""- Huấn luyện mô hình RandomForest (trên toàn bộ dữ liệu chỉ để minh họa)
- Trong thực tế, nên huấn luyện trên tập train sau khi đã split
"""

try:
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y)

    # Lấy feature importances
    importances = rf.feature_importances_
    feature_names = X.columns
    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    # Vẽ biểu đồ
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15), palette='viridis')
    plt.title('Top 15 Feature Importances (dự đoán Subscription Status)', fontsize=14)
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()

    print("Top 15 Features:")
    print(feature_importance_df.head(15))
except Exception as e:
    print(f"Lỗi khi tính Feature Importance: {e}")
    print("Có thể do tên cột chứa ký tự đặc biệt hoặc vấn đề khác.")

"""### 9. Tổng kết và Lưu dữ liệu đã xử lý

**Các bước đã thực hiện:**
1.  Kiểm tra và nhận xét về dữ liệu thiếu/ngoại lai.
2.  Tạo `Product_ID` để định danh sản phẩm duy nhất.
3.  Mã hóa biến phân loại (One-Hot và Label Encoding).
4.  Kiểm tra và loại bỏ đặc trưng dư thừa (multicollinearity).
5.  Chuẩn hóa biến số (`StandardScaler`).
6.  Mã hóa tuần hoàn cho `Season` (`Season_sin`, `Season_cos`).
7.  Tạo đặc trưng dẫn xuất `Customer_Loyalty_Score`.
8.  Tạo đặc trưng `Dominant_Season` cho sản phẩm.
9.  Tạo `Interaction_Score` và ma trận `user_item_matrix`.
10. Minh họa cách đánh giá `Feature Importance`.

**Lưu ý:**
-   Việc chia train/test nên được thực hiện *trước* bước 3 (scaling) và bước 2 (encoding - fit trên train, transform trên cả train/test) để tránh data leakage.
-   Các trọng số trong đặc trưng dẫn xuất (`Customer_Loyalty_Score`, `Interaction_Score`) là heuristic và có thể cần tối ưu hóa.
-   Việc lựa chọn đặc trưng cuối cùng cho mô hình nên dựa trên kết quả validation (ví dụ: cross-validation) và mục tiêu cụ thể của bài toán.
-   Có thể khám phá thêm các tương tác phức tạp giữa các biến (ví dụ: `Age` và `Frequency of Purchases`) nếu cần thiết cho mô hình.

Dữ liệu đã được tiền xử lý và sẵn sàng cho các bước xây dựng mô hình tiếp theo.
"""

# Lưu DataFrame đã xử lý và ma trận User-Item
processed_file_path = '/content/datasets/shopping_behavior_processed.csv'
matrix_file_path = '/content/datasets/user_item_matrix.csv'

try:
    df.to_csv(processed_file_path, index=False)
    user_item_matrix.to_csv(matrix_file_path)
    print(f"Đã lưu DataFrame đã xử lý vào: {processed_file_path}")
    print(f"Đã lưu ma trận User-Item vào: {matrix_file_path}")
except Exception as e:
    print(f"Lỗi khi lưu file: {e}")

df.head()

"""# Bước 4: Feature engineering

**Mục tiêu:**
- Chọn lọc và tinh chỉnh các đặc trưng đã được tiền xử lý ở Bước 3.
- Loại bỏ các đặc trưng không cần thiết hoặc dư thừa.
- Sử dụng các phương pháp lựa chọn đặc trưng để xác định các đặc trưng quan trọng nhất cho mô hình đề xuất.
- Chuẩn bị tập dữ liệu cuối cùng cho việc xây dựng mô hình.

**Lưu ý:** Đặc trưng `Season` sẽ được giữ nguyên (không mã hóa tuần hoàn thành `Season_sin`, `Season_cos`).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import f_regression
from scipy import stats

pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')

"""### 1. Tải dữ liệu đã tiền xử lý

Chúng ta sẽ tải dữ liệu đã được tiền xử lý từ Bước 3.
"""

processed_file_path = '/content/datasets/shopping_behavior_processed.csv'
matrix_file_path = '/content/datasets/user_item_matrix.csv'

df_processed = pd.read_csv(processed_file_path)

df_processed.head()

df_processed.info()

user_item_matrix = pd.read_csv(matrix_file_path, index_col=0)
user_item_matrix.head(10)

"""### 1.2. Tạo đặc trưng `Product_Category`"""

category_columns = [col for col in df_processed.columns if col.startswith('Category_')]
category_columns

categories = ['Accessories', 'Clothing', 'Footwear', 'Outerwear']
def get_category(row):
    for cat in categories:
        if row.get(f'Category_{cat}', 0) == 1:
            return cat
    return "Unknown"  #

df_processed['Category_reconstructed'] = df_processed.apply(get_category, axis=1)
df_processed['Category_reconstructed']

df_processed['Category_reconstructed'].value_counts()

df_processed['Product_Category'] = (df_processed['Item Purchased'] + '_' + df_processed['Category_reconstructed'])

df_processed['Product_Category']

df_processed['Product_Category'].value_counts()

print(f"Đã tạo đặc trưng Product_Category. Số lượng danh mục sản phẩm: {df_processed['Product_Category'].nunique()}")
print("Ví dụ một số Product_Category:")
print(df_processed['Product_Category'].value_counts().head(10))

"""### 2. Loại bỏ đặc trưng không cần thiết / dư thừa

Dựa trên phân tích ở Bước 2 và các bước tiền xử lý từ Bước 3, chúng ta sẽ tiếp tục loại bỏ các đặc trưng không cần thiết cho việc dự đoán:
- **Định danh:** `Customer ID` không dùng để huấn luyện mô hình.
- **Đã mã hóa:** Các cột categorical gốc đã được mã hóa (one-hot hoặc label) nên không cần thiết nữa.
- **Trung gian:** Các cột dùng để tính toán trung gian như `Season_cycle`, `Frequency_score_raw` có thể loại bỏ nếu không dùng trực tiếp.
- **Product_ID** chỉ dùng cho ma trận user-item, không phải feature của user/context.
"""

cols_to_drop = [
    'Promo Code Used',
    'Gender', 'Category', 'Location', 'Size', 'Color',
    'Subscription Status', 'Payment Method', 'Shipping Type', 'Discount Applied',
    'Frequency of Purchases',
    'Season_cycle',
    'Frequency_score_raw',
    'Frequency_score_scaled',
    'Product_ID',
    'Season_sin',  # Loại bỏ Season_sin
    'Season_cos'   # Loại bỏ Season_cos
]

cols_to_drop_existing = [col for col in cols_to_drop if col in df_processed.columns]
cols_to_drop_existing

print(f"Đã loại bỏ {len(cols_to_drop_existing)} cột.")

df_final_features = df_processed.drop(columns=cols_to_drop_existing)
df_final_features

print(f"Số cột còn lại sau khi loại bỏ: {len(df_final_features.columns)}")

print("Các cột còn lại:")
print(df_final_features.columns.tolist())

df_final_features.head()

"""### 2.1. Kiểm tra các đặc trưng có phương sai thấp (Low Variance)

Các đặc trưng có phương sai thấp (gần như hằng số) không cung cấp nhiều thông tin hữu ích cho mô hình và có thể bị loại bỏ.
"""

numeric_features = df_processed.select_dtypes(include=['int32', 'int64', 'float64'])
numeric_features

numeric_features.describe()

variances = numeric_features.var()
variances

variances_df = pd.DataFrame({'Feature': variances.index, 'Variance': variances.values})
variances_df

variances_df = variances_df.sort_values('Variance')

print("10 đặc trưng có phương sai thấp nhất:")
variances_df.head(10)

# ngưỡng phương sai
low_variance_threshold = 0.01
low_variance_features = variances_df[variances_df['Variance'] < low_variance_threshold]['Feature'].tolist()

print(f"\nCác đặc trưng có phương sai < {low_variance_threshold}:")
print(low_variance_features)

if low_variance_features:
    plt.figure(figsize=(15, 5 * min(3, len(low_variance_features))))
    for i, feature in enumerate(low_variance_features[:3]):
        plt.subplot(min(3, len(low_variance_features)), 1, i+1)
        sns.histplot(df_processed[feature], kde=True)
        plt.title(f'Phân phối của {feature} (Variance: {variances[feature]:.6f})')
    plt.tight_layout()
    plt.show()

"""**Nhận xét:**
- Phân tích phương sai giúp chúng ta xác định các đặc trưng có rất ít sự biến thiên (gần như hằng số).
- Các đặc trưng có phương sai thấp như `Category_*` có thể do chúng là biến one-hot, chỉ nhận giá trị 0 hoặc 1, và sự phân bố không đồng đều giữa các danh mục.
- Nếu có bất kỳ đặc trưng nào có phương sai gần như bằng 0, chúng ta nên cân nhắc loại bỏ vì chúng không cung cấp thông tin phân biệt.
- Tuy nhiên, cần cẩn thận với các biến one-hot vì độ phổ biến thấp của một danh mục không đồng nghĩa với việc nó không quan trọng.

### 3.1.1. Xóa các đặc trưng không cần thiết dựa trên phương sai thấp
"""

extremely_low_var_threshold = 0.001
extremely_low_var_features = variances_df[variances_df['Variance'] < extremely_low_var_threshold]['Feature'].tolist()

print(f"Các đặc trưng có phương sai cực thấp (< {extremely_low_var_threshold}):", extremely_low_var_features)

binary_features = [col for col in df_processed.columns if set(df_processed[col].unique()).issubset({0, 1})]
binary_features

print("Tỷ lệ giá trị phổ biến nhất trong các đặc trưng binary:")
for col in binary_features:
    value_counts = df_processed[col].value_counts(normalize=True)
    most_common_value = value_counts.index[0]
    most_common_ratio = value_counts.iloc[0]
    print(f"{col}: {most_common_value} ({most_common_ratio:.2%})")

additional_cols_to_drop = []
for feature in extremely_low_var_features:
    if not feature.startswith('Category_') and feature in df_final_features.columns:
        additional_cols_to_drop.append(feature)

print(f"Đặc trưng đề xuất loại bỏ thêm: {additional_cols_to_drop}")

if additional_cols_to_drop:
    df_final_features = df_final_features.drop(columns=additional_cols_to_drop)
    print(f"Đã loại bỏ thêm {len(additional_cols_to_drop)} đặc trưng. Số đặc trưng còn lại: {len(df_final_features.columns)}")
else:
    print("Không có đặc trưng nào được đề xuất loại bỏ thêm.")

"""### 3. Lựa chọn đặc trưng (Feature Selection)

#### 3.1. Dựa vào Ma trận tương quan (Correlation Matrix)

Kiểm tra lại ma trận tương quan trên tập đặc trưng cuối cùng để phát hiện đa cộng tuyến (multicollinearity) giữa các đặc trưng số và đặc trưng đã mã hóa.
"""

plt.figure(figsize=(18, 12))
correlation_matrix_final = df_final_features.corr(numeric_only=True)
sns.heatmap(correlation_matrix_final, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5, annot_kws={"size": 8})
plt.title('Ma trận Tương quan giữa các Đặc trưng Cuối cùng', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""#### Tìm các cặp tương quan cao"""

correlation_matrix_final

threshold = 0.8
highly_correlated_pairs = []
corr_matrix_abs = correlation_matrix_final.abs()
corr_matrix_abs

upper_tri = corr_matrix_abs.where(np.triu(np.ones(corr_matrix_abs.shape), k=1).astype(bool))
upper_tri

highly_correlated_pairs = [(column, index) for column in upper_tri.columns for index in upper_tri.index if upper_tri.loc[index, column] > threshold]
highly_correlated_pairs

if highly_correlated_pairs:
    for pair in highly_correlated_pairs:
        print(f"- {pair[0]} và {pair[1]}: {correlation_matrix_final.loc[pair[1], pair[0]]:.2f}")
else:
    print("Không tìm thấy cặp đặc trưng nào có tương quan cao.")

"""**Nhận xét:**
- Ma trận tương quan cho thấy hầu hết các đặc trưng có độ tương quan thấp với nhau.
- Các cặp tương quan đáng chú ý (nếu có) cần được xem xét. Ví dụ, nếu `Gender_Female` và `Gender_Male` cùng tồn tại, chúng sẽ có tương quan -1 (hoàn hảo nghịch đảo). Trong trường hợp này, chúng ta đã loại bỏ `Gender_Male` ở Bước 3.
- `Season_sin` và `Season_cos` có tương quan thấp với nhau và với các biến khác, cho thấy mã hóa tuần hoàn hoạt động tốt.
- `Customer_Loyalty_Score` có tương quan vừa phải với `Previous Purchases` và `Subscription Status_encoded` vì nó được tạo ra từ chúng.
- Nhìn chung, không có dấu hiệu rõ ràng về đa cộng tuyến nghiêm trọng cần loại bỏ thêm đặc trưng dựa trên ma trận tương quan này.

#### 3.2. Dựa vào Model-Based Methods (Feature Importance)

Sử dụng RandomForestRegressor để đánh giá tầm quan trọng của các đặc trưng trong việc dự đoán `Interaction_Score` (một proxy cho mức độ yêu thích sản phẩm).

#### Loại bỏ các cột string và các cột không dùng cho model
"""

string_columns = ['Item Purchased', 'Dominant_Season', 'Category_reconstructed', 'Product_Category']
string_columns

"""#### Giữ lại 'Season' cho model nếu là dạng số hoặc đã mã hóa, nếu là chuỗi cần mã hóa lại"""

model_features = [col for col in df_final_features.columns if col not in string_columns]
model_features

"""#### Nếu 'Season' là chuỗi, mã hóa one-hot"""

if 'Season' in model_features and df_final_features['Season'].dtype == object:
    X_model = pd.get_dummies(df_final_features[model_features], columns=['Season'], prefix='Season', dtype=int)
    print(f"Đã chuyển 'Season' sang one-hot encoding.")
else:
    X_model = df_final_features[model_features]

X_model.head()

y = df_final_features['Interaction_Score']
y.head(10)

X_model.columns = X_model.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)
X_model.columns

rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_model, y)

importances = rf.feature_importances_
print('importances:\n', importances)

feature_names = X_model.columns
print('Feature names:\n', feature_names)

feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
feature_importance_df

print("Feature Importances (Top 20):")
print(feature_importance_df.head(20))

plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20), palette='viridis', hue='Feature', legend=False)
plt.title('Top 20 Feature Importances từ Random Forest (dự đoán Interaction_Score)', fontsize=16)
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""**Nhận xét:**
- Random Forest cho thấy `Review Rating` và `Purchase Amount (USD)` là hai đặc trưng quan trọng nhất để dự đoán `Interaction_Score`. Điều này hợp lý vì `Interaction_Score` được tạo ra trực tiếp từ hai biến này.
- Các đặc trưng khác như `Customer_Loyalty_Score`, `Previous Purchases`, `Age`, và các đặc trưng được mã hóa từ `Location`, `Color`, `Size`, `Payment Method` cũng có đóng góp, mặc dù ít hơn đáng kể.
- Các đặc trưng one-hot của `Category` và `Gender_Female`, cùng với mã hóa tuần hoàn của `Season` (`Season_sin`, `Season_cos`) có vẻ ít quan trọng hơn trong việc dự đoán `Interaction_Score` theo mô hình này.
- Dựa trên kết quả này, chúng ta có thể cân nhắc loại bỏ các đặc trưng có tầm quan trọng rất thấp nếu muốn mô hình đơn giản hơn. Tuy nhiên, trong bài toán Recommendation, việc giữ lại các đặc trưng về ngữ cảnh (như mùa, danh mục) có thể vẫn hữu ích ngay cả khi chúng không trực tiếp dự đoán `Interaction_Score` tốt.

### 3.2.1. Cải thiện biểu diễn Feature Importance
"""

importances_pct = rf.feature_importances_ / np.sum(rf.feature_importances_) * 100
feature_importance_pct = pd.DataFrame({'Feature': feature_names, 'Importance (%)': importances_pct})
feature_importance_pct = feature_importance_pct.sort_values(by='Importance (%)', ascending=False)
feature_importance_pct

plt.figure(figsize=(14, 10))
ax = sns.barplot(x='Importance (%)', y='Feature', data=feature_importance_pct.head(15), palette='viridis', hue='Feature', legend=False)
for i, v in enumerate(feature_importance_pct.head(15)['Importance (%)']):
    if v >= 0.1:
        ax.text(v + 0.3, i, f"{v:.2f}%", va='center')

plt.title('Top 15 Feature Importances (%) for Predicting Interaction_Score', fontsize=16)
plt.xlabel('Importance (%)')
plt.ylabel('Feature')
plt.xlim(0, max(importances_pct) * 1.1)
plt.tight_layout()
plt.show()

feature_importance_pct['Cumulative Importance (%)'] = feature_importance_pct['Importance (%)'].cumsum()
features_for_95pct = len(feature_importance_pct[feature_importance_pct['Cumulative Importance (%)'] <= 95])
print(f"Chỉ cần {features_for_95pct + 1} đặc trưng hàng đầu là đủ để giải thích 95% độ quan trọng tổng cộng.")
print("\nTop 5 đặc trưng quan trọng nhất:")
print(feature_importance_pct.head(5))

"""#### 3.3. Dựa vào Statistical Tests (ANOVA F-test)

Sử dụng ANOVA F-test để kiểm tra mối quan hệ thống kê giữa từng đặc trưng đầu vào (số hoặc đã mã hóa, bao gồm cả `Season` nếu đã mã hóa) và biến mục tiêu `Interaction_Score`.

##### Use only numeric features for ANOVA F-test
"""

X_numeric = X_model.select_dtypes(include=np.number)
X_numeric

f_values, p_values = f_regression(X_numeric, y)
print("F_values:\n", f_values)

print("P_values:\n", p_values)

anova_results = pd.DataFrame({
    'Feature': X_numeric.columns,
    'F-value': f_values,
    'p-value': p_values
})
anova_results = anova_results.sort_values(by='F-value', ascending=False)

print("Kết quả ANOVA F-test (Feature vs Interaction_Score):")
print(anova_results)

significant_features = anova_results[anova_results['p-value'] < 0.05]['Feature'].tolist()
print("Các đặc trưng có mối quan hệ ý nghĩa thống kê (p < 0.05) với Interaction_Score:")
print(significant_features)

"""**Nhận xét:**
- Kết quả ANOVA F-test xác nhận `Review Rating` và `Purchase Amount (USD)` có mối quan hệ thống kê mạnh nhất với `Interaction_Score` (F-value cao nhất, p-value rất nhỏ), điều này khớp với cách `Interaction_Score` được tạo ra.
- Nhiều đặc trưng khác cũng cho thấy mối quan hệ có ý nghĩa thống kê (p < 0.05) với `Interaction_Score`, mặc dù F-value thấp hơn. Điều này cho thấy chúng vẫn chứa thông tin liên quan đến biến mục tiêu.
- Các đặc trưng như `Gender_Female`, `Season_sin`, `Season_cos` có F-value rất thấp và p-value cao, cho thấy mối quan hệ tuyến tính yếu hoặc không có ý nghĩa thống kê với `Interaction_Score`.

### 3.4. Chi-square Test cho đặc trưng phân loại

Sử dụng **Chi-square test** để đánh giá mối quan hệ giữa các đặc trưng phân loại và biến mục tiêu. Vì `Interaction_Score` là biến liên tục, chúng ta sẽ chuyển đổi nó thành biến phân loại bằng cách phân vị để áp dụng **Chi-square test**.
"""

interaction_bins = pd.qcut(df_final_features['Interaction_Score'], q=3, labels=[0, 1, 2])
interaction_bins

interaction_bins.value_counts()

categorical_features = ['Gender_Female', 'Category_Accessories', 'Category_Clothing',
                        'Category_Footwear', 'Category_Outerwear']

"""##### Thực hiện chi-square (chi2) test"""

chi2_test = {}
for feature in categorical_features:
    contingency_table = pd.crosstab(df_final_features[feature], interaction_bins)
    chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)
    chi2_test[feature] = {'chi2': chi2_stat, 'p-value': p_val}

chi2_results = pd.DataFrame.from_dict(chi2_test, orient='index')
chi2_results = chi2_results.sort_values('chi2', ascending=False)

print("Kết quả Chi-square test cho các đặc trưng phân loại:")
print(chi2_results)

plt.figure(figsize=(10, 6))
sns.barplot(x=chi2_results.index, y='chi2', data=chi2_results)
plt.title('Chi-square Values for Categorical Features')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print("Đặc trưng có p-value < 0.05 (có ý nghĩa thống kê):")
significant_categorical = chi2_results[chi2_results['p-value'] < 0.05].index.tolist()
print(significant_categorical)

"""**Nhận xét:**
- Chi-square test giúp xác định mối quan hệ có ý nghĩa thống kê giữa các đặc trưng phân loại và biến mục tiêu được phân loại.
- Các đặc trưng có giá trị chi-square cao và p-value thấp có mối quan hệ mạnh với `Interaction_Score`.
- Đặc trưng như Category_Footwear, Category_Clothing có chi-square cao nhất, cho thấy loại sản phẩm có liên quan mạnh đến mức độ tương tác của khách hàng.
- Giữ lại các đặc trưng phân loại có ý nghĩa thống kê (p-value < 0.05) là quan trọng cho việc dự đoán `Interaction_Score`.

### 4. Xác định tập đặc trưng cuối cùng

Dựa trên các phân tích trên:
- Các đặc trưng gốc đã được mã hóa và chuẩn hóa.
- Các đặc trưng dư thừa (`Promo Code Used*`, `Gender_Male`) đã bị loại bỏ.
- Ma trận tương quan không cho thấy đa cộng tuyến nghiêm trọng.
- Feature Importance và ANOVA cho thấy nhiều đặc trưng có giá trị dự đoán (đặc biệt là `Review Rating`, `Purchase Amount (USD)`).
- Mặc dù một số đặc trưng (như `Gender_Female`, `Season`) có tầm quan trọng thấp đối với `Interaction_Score`, chúng có thể vẫn quan trọng cho việc cá nhân hóa hoặc đề xuất dựa trên ngữ cảnh. Do đó, chúng ta sẽ giữ lại hầu hết các đặc trưng đã xử lý.

**Quyết định:** Giữ lại tất cả các cột trong `df_final_features` cho bước mô hình hóa tiếp theo. Có thể thử nghiệm loại bỏ các biến có tầm quan trọng thấp nhất (ví dụ: dựa trên ngưỡng p-value từ ANOVA hoặc ngưỡng importance từ RF) sau này nếu cần đơn giản hóa mô hình hoặc cải thiện hiệu suất trên tập validation.
"""

final_selected_features = df_final_features.columns.tolist()
print("Danh sách các đặc trưng cuối cùng được chọn:")
print(final_selected_features)

print(f"\nTổng số đặc trưng: {len(final_selected_features)}")

df_final_features.head()

"""### 5. Lưu dữ liệu đã xử lý

Lưu DataFrame chứa các đặc trưng cuối cùng vào một file mới để sử dụng cho Bước 5: Xây dựng mô hình.
"""

output_file = '/content/datasets/shopping_behavior_final_features.csv'
df_final_features.to_csv(output_file, index=False)
print(f"Đã lưu DataFrame với các đặc trưng cuối cùng vào file: [{output_file}]")

"""**Kết thúc Bước 4.** Dữ liệu đã sẵn sàng cho việc xây dựng các mô hình Recommendation ở Bước 5.

### 6. Giải thích về các đặc trưng được tạo mới

Trong quá trình xử lý dữ liệu, chúng ta đã tạo ra một số đặc trưng mới để nắm bắt thông tin quan trọng. Dưới đây là giải thích chi tiết về mỗi đặc trưng được tạo mới:

1. **Product_ID**: Kết hợp 'Item Purchased', 'Category', 'Size', 'Color' để tạo định danh duy nhất cho sản phẩm. Đặc trưng này hữu ích cho các mô hình đề xuất dựa trên sản phẩm, đặc biệt là collaborative filtering.

2. **Customer_Loyalty_Score**: Đây là đặc trưng tổng hợp đo lường mức độ trung thành của khách hàng dựa trên ba yếu tố:
   - Previous Purchases (40%): Số lượng mua hàng trước đó.
   - Frequency_score_scaled (40%): Tần suất mua hàng được chuẩn hóa.
   - Subscription Status (20%): Liệu khách hàng có đăng ký hay không.
   
   Công thức: `Customer_Loyalty_Score = Previous Purchases * 0.4 + Frequency_score_scaled * 0.4 + Subscription Status_encoded * 0.2`
   
   Trọng số 0.4, 0.4, 0.2 được chọn dựa trên hiểu biết về lĩnh vực, với giả định rằng số lượng và tần suất mua hàng trước đó có tầm quan trọng như nhau và lớn hơn trạng thái đăng ký.

3. **Interaction_Score**: Đặc trưng tổng hợp này là mục tiêu tiềm năng cho mô hình dự đoán, đại diện cho mức độ tương tác tích cực của khách hàng với sản phẩm. Nó kết hợp:
   - Review Rating (70%): Đánh giá trực tiếp của khách hàng về sản phẩm.
   - Purchase Amount (30%): Số tiền chi tiêu, thể hiện sự sẵn sàng đầu tư vào sản phẩm.
   
   Công thức: `Interaction_Score = Review Rating * 0.7 + Purchase Amount (USD) * 0.3`
   
   Trọng số 0.7 và 0.3 phản ánh rằng đánh giá trực tiếp của khách hàng là chỉ báo mạnh mẽ hơn về sự hài lòng so với số tiền chi tiêu.

4. **Frequency_score_scaled**: Chuyển đổi tần suất mua hàng từ chuỗi sang giá trị số, sau đó chuẩn hóa:
   ```
   Annually = 1, Quarterly/Every 3 Months = 2, Monthly = 3, Fortnightly = 4, Weekly = 5, Bi-Weekly = 6
   ```
   Chuẩn hóa áp dụng StandardScaler để đảm bảo phân phối với trung bình 0 và độ lệch chuẩn 1.

**Lưu ý:** Đặc trưng `Season` được giữ nguyên (không mã hóa tuần hoàn).

### 7. Đánh giá phân phối đặc trưng cuối cùng
"""

numerical_features = ['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases', 'Customer_Loyalty_Score', 'Interaction_Score']
numerical_features

plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_features):
    plt.subplot(2, 3, i+1)
    sns.histplot(df_final_features[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

important_features = feature_importance_pct.head(5)['Feature'].tolist()
important_features

important_numerical = [f for f in important_features if f in numerical_features]
important_numerical

plt.figure(figsize=(15, 10))
for i, feature in enumerate(important_numerical):
    if feature != 'Interaction_Score':
        plt.subplot(2, 2, i+1)
        plt.scatter(df_final_features[feature], df_final_features['Interaction_Score'], alpha=0.3)
        plt.title(f'{feature} vs Interaction_Score')
        plt.xlabel(feature)
        plt.ylabel('Interaction_Score')
plt.tight_layout()
plt.show()

"""##### Add visualization for Item Purchased distribution"""

plt.figure(figsize=(15, 8))
item_counts = df_final_features['Item Purchased'].value_counts().head(20)
sns.barplot(x=item_counts.index, y=item_counts.values)
plt.title('Top 20 Most Frequently Purchased Items')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""**Nhận xét:**
- Các đặc trưng số có phân phối tương đối cân đối sau khi chuẩn hóa.
- Có mối tương quan rõ ràng giữa `Review Rating` và `Interaction_Score`, phù hợp với việc `Review Rating` có trọng số 70% trong công thức tính `Interaction_Score`.
- `Purchase Amount (USD)` cũng có mối tương quan với `Interaction_Score` nhưng yếu hơn, phù hợp với trọng số 30% trong công thức.
- Phân phối của `Customer_Loyalty_Score` cho thấy sự phân tầng của khách hàng theo mức độ trung thành, điều này có thể hữu ích cho việc phân đoạn khách hàng.

### 8. Tóm tắt quyết định Feature Engineering

Dựa trên tất cả các phân tích trên, chúng ta đã thực hiện các quyết định sau trong quá trình Feature Engineering:

1. **Loại bỏ đặc trưng**:
   - Đã loại bỏ các đặc trưng định danh không có giá trị dự đoán (`Customer ID`).
   - Đã loại bỏ các đặc trưng dư thừa (`Promo Code Used` ~ `Discount Applied`).
   - Đã loại bỏ các cột gốc đã được mã hóa (ngoại trừ `Item Purchased` được giữ lại cho mục đích đề xuất sản phẩm).
   - Đã loại bỏ các đặc trưng trung gian dùng để tính toán.
   - Đã xác định và cân nhắc loại bỏ các đặc trưng có phương sai cực thấp.
   - **Đã loại bỏ `Season_sin`, `Season_cos` để giữ nguyên đặc trưng `Season`.**

2. **Tạo đặc trưng mới**:
   - Đã tạo `Product_ID` bằng cách kết hợp thông tin sản phẩm.
   - Đã tạo `Customer_Loyalty_Score` từ các chỉ số hành vi mua hàng.
   - Đã tạo `Interaction_Score` là mục tiêu tiềm năng cho mô hình.

3. **Lựa chọn phương pháp mã hóa**:
   - Sử dụng One-hot encoding cho `Gender` và `Category`.
   - Sử dụng Label encoding cho các biến phân loại còn lại.
   - Chuẩn hóa các biến số với `StandardScaler`.
   - Giữ nguyên biến `Item Purchased` để phục vụ cho việc đề xuất sản phẩm cụ thể.
   - **Giữ nguyên đặc trưng `Season` (không mã hóa tuần hoàn, không dùng Season_sin/cos).**

4. **Phương pháp chọn lọc đặc trưng**:
   - Sử dụng ma trận tương quan để phát hiện đa cộng tuyến.
   - Sử dụng Random Forest Feature Importance để xác định đặc trưng quan trọng.
   - Sử dụng ANOVA F-test và Chi-square test để đánh giá ý nghĩa thống kê.
   - Phân tích phương sai để xác định đặc trưng không có giá trị phân biệt.
   - Loại trừ các đặc trưng chuỗi như `Item Purchased` khỏi các phân tích số học nhưng giữ lại trong bộ dữ liệu cuối cùng.

Kết quả là chúng ta đã chọn {len(final_selected_features)} đặc trưng cuối cùng, với đặc trưng quan trọng nhất trong việc dự đoán `Interaction_Score` là `Review Rating` và `Purchase Amount (USD)`. Đặc biệt, chúng ta giữ lại `Item Purchased` trong bộ đặc trưng cuối cùng để hỗ trợ trực tiếp cho việc đề xuất sản phẩm cụ thể trong các mô hình sau này.

## Bước 5: Huấn luyện, đánh giá và tinh chỉnh mô hình
### Xây dựng Hybrid Recommendation System cho Đề xuất Sản phẩm

**Mục tiêu:** Kết hợp Collaborative Filtering (Item-Item) và Content-Based Filtering để đề xuất sản phẩm cá nhân hóa, tận dụng cả lịch sử tương tác và đặc trưng sản phẩm/người dùng.

### Dataset: [Consumer Behavior and Shopping Habits Dataset](https://www.kaggle.com/datasets/zeesolver/consumer-behavior-and-shopping-habits-dataset)

### File sử dụng:
- `shopping_behavior_final_features.csv` (dữ liệu đặc trưng đã xử lý)
- `user_item_matrix.csv` (ma trận tiện ích User-Item)
"""

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')

"""## 1. Load và chuẩn bị dữ liệu"""

# Load dữ liệu đặc trưng và ma trận tiện ích
features_df = pd.read_csv('/content/datasets/shopping_behavior_final_features.csv')
user_item_matrix = pd.read_csv('/content/datasets/user_item_matrix.csv', index_col=0)

print(f"features_df shape: {features_df.shape}")
print(f"user_item_matrix shape: {user_item_matrix.shape}")
display(features_df.head())
display(user_item_matrix.iloc[:5, :5])

"""#### Kiểm tra dữ liệu và xử lý thiếu/khác biệt nếu có

- Kiểm tra missing values
"""

print(features_df.isnull().sum().sort_values(ascending=False).head(10))
print(user_item_matrix.isnull().sum().sum())

"""- Đảm bảo kiểu dữ liệu đúng"""

features_df['Customer ID'] = features_df['Customer ID'].astype(int)
user_item_matrix.index = user_item_matrix.index.astype(int)

# Lấy danh sách user và item chung
all_users = user_item_matrix.index
print(all_users)

all_items = user_item_matrix.columns
print(all_items)

"""#### Chia dữ liệu train/test (theo tương tác)
- Chia theo hàng của features_df (mỗi hàng là 1 tương tác user-item)
"""

# Chia train/test theo tương tác (80/20)
train_df, test_df = train_test_split(features_df, test_size=0.2, random_state=42)
print(f"Train size: {train_df.shape}, Test size: {test_df.shape}")

"""## 2. Item-Item Collaborative Filtering with Matrix Factorization

Cải thiện bằng cách kết hợp hai phương pháp:
1. Item-Item Similarity (sử dụng cosine similarity)
2. Matrix Factorization (SVD) để giải quyết vấn đề ma trận thưa

##### Tạo ma trận tiện ích train (User x Item)
"""

train_matrix = train_df.pivot_table(index='Customer ID', columns='Product_Category', values='Interaction_Score', aggfunc='mean').fillna(0)

"""##### Tính ma trận tương đồng item-item (cosine)"""

item_sim_matrix = pd.DataFrame(
    cosine_similarity(train_matrix.T),
    index=train_matrix.columns,
    columns=train_matrix.columns
)
print(f"Item similarity matrix shape: {item_sim_matrix.shape}")
item_sim_matrix.iloc[:5, :5]

"""#### Triển khai Matrix Factorization (SVD) cho Collaborative Filtering"""

from scipy.sparse.linalg import svds
import numpy as np

# Chuyển đổi thành ma trận numpy
matrix = train_matrix.values

# Số lượng latent factors (tùy chỉnh tùy theo số lượng sản phẩm/người dùng)
num_factors = min(50, min(matrix.shape) - 1)

# Tính trung bình mỗi user
user_ratings_mean = np.mean(matrix, axis=1)
matrix_demeaned = matrix - user_ratings_mean.reshape(-1, 1)

# Phân tích SVD
U, sigma, Vt = svds(matrix_demeaned, k=num_factors)

# Chuyển đổi sigma thành ma trận đường chéo
sigma_diag = np.diag(sigma)

# Dự đoán (khôi phục ma trận ban đầu + trung bình)
svd_preds = np.dot(np.dot(U, sigma_diag), Vt) + user_ratings_mean.reshape(-1, 1)
svd_df = pd.DataFrame(svd_preds, index=train_matrix.index, columns=train_matrix.columns)

print(f"SVD matrix shape: {svd_df.shape}")
print("Vài giá trị dự đoán từ SVD:")
display(svd_df.iloc[:5, :5])

"""#### Hàm dự đoán điểm cho SVD"""

def predict_svd_score(user_id, item_id):
    """Dự đoán điểm tương tác dựa trên Matrix Factorization"""
    if user_id not in svd_df.index or item_id not in svd_df.columns:
        return np.nan
    return svd_df.loc[user_id, item_id]

"""### Hàm dự đoán điểm tương tác user-item dựa trên Item-Item CF"""

def predict_cf_score(user_id, item_id, train_matrix, item_sim_matrix, k=5):
    """
    Dự đoán điểm tương tác của user với item dựa trên các item tương tự mà user đã tương tác
    """
    if user_id not in train_matrix.index:
        return np.nan  # Cold start user
    if item_id not in train_matrix.columns:
        return np.nan  # Cold start item
    user_ratings = train_matrix.loc[user_id]
    interacted_items = user_ratings[user_ratings > 0].index
    if len(interacted_items) == 0:
        return np.nan
    # Lấy k item tương tự nhất với item_id mà user đã từng tương tác
    similarities = item_sim_matrix.loc[item_id, interacted_items]
    top_k = similarities.abs().sort_values(ascending=False).head(k)
    if top_k.sum() == 0:
        return np.nan
    scores = user_ratings[top_k.index]
    pred = np.dot(top_k, scores) / top_k.sum()
    return pred

"""#### Xử lý cold start: Nếu user hoặc item chưa từng xuất hiện, trả về NaN (sẽ xử lý ở hybrid step)"""

# Triển khai Popularity-based Recommendation cho cold start users
item_popularity = train_df.groupby('Product_Category')['Interaction_Score'].agg(['count', 'mean'])
item_popularity['score'] = item_popularity['count'] * item_popularity['mean']
item_popularity = item_popularity.sort_values('score', ascending=False)

print("Top 10 sản phẩm phổ biến:")
display(item_popularity.head(10))

def get_popular_items(n=5):
    """Trả về top N sản phẩm phổ biến nhất"""
    return item_popularity.head(n).index.tolist()

"""## 3. Content-Based Filtering

Trong phần này, chúng ta xây dựng hệ thống Content-Based Filtering hoàn chỉnh bằng cách:
1. Tạo đặc trưng sản phẩm (product features) dựa trên thuộc tính sản phẩm
2. Xây dựng hồ sơ người dùng (user profiles) dựa trên lịch sử tương tác và đặc điểm
3. Tính độ tương đồng giữa người dùng và sản phẩm để đề xuất

Content-based filtering giúp giải quyết vấn đề cold-start khi không có đủ dữ liệu tương tác.
"""

# Chuẩn bị đặc trưng sản phẩm cho content-based filtering
# Chọn các đặc trưng sản phẩm và người dùng liên quan
# Không sử dụng Season_sin, Season_cos nữa
item_features = ['Product_Category', 'Season']

"""- Bổ sung thêm đặc trưng từ product category như size và color"""

def extract_size_color(category_name):
    """Tách size và color từ tên sản phẩm nếu có"""
    parts = category_name.split('_')
    size = parts[1] if len(parts) > 1 else None
    color = parts[2] if len(parts) > 2 else None
    return size, color

# Tạo DataFrame đặc trưng sản phẩm mở rộng
product_features = pd.DataFrame(index=train_matrix.columns)
product_features['category'] = [item.split('_')[0] for item in product_features.index]
product_features

"""#### Tạo DataFrame đặc trưng sản phẩm mở rộng"""

product_features = pd.DataFrame(index=train_matrix.columns)
product_features

product_features['category'] = [item.split('_')[0] for item in product_features.index]
product_features['category']

"""#### Tạo dictionaries cho size và color"""

sizes = []
colors = []
for item in product_features.index:
    size, color = extract_size_color(item)
    sizes.append(size)
    colors.append(color)

product_features['size'] = sizes
product_features['color'] = colors
product_features

"""#### One-hot encoding cho category, size và color"""

category_dummies = pd.get_dummies(product_features['category'], prefix='cat')
size_dummies = pd.get_dummies(product_features['size'], prefix='size')
color_dummies = pd.get_dummies(product_features['color'], prefix='color')

category_dummies

size_dummies

color_dummies

"""#### Nếu muốn sử dụng Season, lấy giá trị Season phổ biến nhất cho từng Product_Category"""

season_map = train_df.groupby('Product_Category')['Season'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')
season_map

product_features['Season'] = product_features.index.map(season_map)
product_features['Season']

season_dummies = pd.get_dummies(product_features['Season'], prefix='season')
season_dummies

"""#### Kết hợp tất cả đặc trưng"""

product_features_encoded = pd.concat([category_dummies, size_dummies, color_dummies, season_dummies], axis=1)
product_features_encoded

"""#### Điền các giá trị bị thiếu"""

product_features_encoded = product_features_encoded.fillna(0)

print(f"Product features shape: {product_features_encoded.shape}")
print("Một vài đặc trưng sản phẩm:")
display(product_features_encoded.iloc[:5, :10])

"""#### Xây dựng hồ sơ người dùng (user profiles) cho content-based filtering"""

user_features = ['Customer ID', 'Customer_Loyalty_Score', 'Age', 'Gender_Female', 'Previous Purchases']

# Tạo hồ sơ người dùng cơ bản từ đặc trưng người dùng
user_profile_base = train_df.groupby('Customer ID')[user_features[1:]].mean()
user_profile_base

"""#### Tính toán sở thích theo loại sản phẩm"""

# Dựa trên lịch sử tương tác với các category
product_categories = train_df['Product_Category'].unique()
category_preferences = {}

# Phân tách các sản phẩm theo loại chính (Clothing, Footwear, etc.)
for category in product_categories:
    # Lấy phần tiền tố category (ví dụ: 'Clothing' từ 'Shirt_Clothing')
    category_type = category.split('_')[1] if '_' in category else category
    category_interactions = train_df[train_df['Product_Category'] == category].groupby('Customer ID')['Interaction_Score'].mean()
    if category_type not in category_preferences:
        category_preferences[f'pref_{category_type}'] = category_interactions
    else:
        # Cập nhật sở thích cho category đã tồn tại
        existing_scores = category_preferences[f'pref_{category_type}']
        merged_scores = pd.concat([existing_scores, category_interactions]).groupby(level=0).mean()
        category_preferences[f'pref_{category_type}'] = merged_scores

# Chuyển thành DataFrame
category_pref_df = pd.DataFrame(category_preferences)
category_pref_df

"""#### Kết hợp đặc trưng người dùng cơ bản với sở thích sản phẩm"""

user_profiles = user_profile_base.join(category_pref_df, how='left')
user_profiles = user_profiles.fillna(0)
user_profiles

"""#### Tiêu chuẩn hóa sở thích người dùng"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
preference_cols = [col for col in user_profiles.columns if col.startswith('pref_')]
if preference_cols:  # Nếu có cột preference
    user_profiles[preference_cols] = scaler.fit_transform(user_profiles[preference_cols])

print(f"User profiles shape: {user_profiles.shape}")
print("Một vài hồ sơ người dùng:")
display(user_profiles.iloc[:5, :])
print("\nThống kê hồ sơ người dùng:")
print(user_profiles.describe())

"""### Hàm dự đoán điểm tương tác user-item dựa trên Content-Based

Hàm này dự đoán điểm tương tác dựa trên sự tương đồng giữa hồ sơ người dùng và đặc trưng sản phẩm. Đây là cốt lõi của Content-Based Filtering, giúp đề xuất các sản phẩm tương tự với những sản phẩm mà người dùng đã thích trong quá khứ.
"""

from sklearn.metrics.pairwise import cosine_similarity as cos_sim

def predict_cb_score_improved(user_id, item_id):
    """Dự đoán điểm dựa trên Content-based Filtering với cải tiến

    Phương pháp:
    1. Lấy vector đặc trưng của user và item
    2. Tính độ tương đồng cosine giữa chúng
    3. Kết hợp với sở thích category của user
    4. Chuẩn hóa điểm về thang tương tác
    """
    # Kiểm tra xem user và item có tồn tại không
    if user_id not in user_profiles.index:
        return np.nan
    if item_id not in product_features_encoded.index:
        return np.nan

    # Lấy thông tin user
    user_profile = user_profiles.loc[user_id]

    # Lấy category của sản phẩm
    item_category = item_id.split('_')[1] if '_' in item_id else item_id

    # Tính độ tương đồng dựa trên sở thích của user với category
    category_preference = 0
    pref_col = f'pref_{item_category}'
    if pref_col in user_profiles.columns:
        category_preference = user_profile[pref_col]

    # Tách user vector thành phần demographic và preference
    user_demo_features = user_profile[['Customer_Loyalty_Score', 'Age',
                                      'Gender_Female', 'Previous Purchases']].values.reshape(1, -1)
    user_pref_features = user_profile[preference_cols].values.reshape(1, -1) if preference_cols else np.array([]).reshape(1, -1)

    # Lấy vector đặc trưng sản phẩm
    item_vector = product_features_encoded.loc[item_id].values.reshape(1, -1)

    # Tính độ tương đồng cosine giữa user preferences và item features
    content_sim = 0.5  # Giá trị mặc định trung bình
    if item_vector.shape[1] > 0 and user_pref_features.shape[1] > 0:
        # Đảm bảo user_pref_features có cùng kích thước với item_vector
        min_dim = min(user_pref_features.shape[1], item_vector.shape[1])
        content_sim = cos_sim(user_pref_features[:,:min_dim], item_vector[:,:min_dim])[0][0]

    # Nhân demographic similarity vào đánh giá (loyalty cao sẽ có xu hướng tương tác hơn)
    loyalty_score = (user_profile['Customer_Loyalty_Score'] + 2) / 4  # Chuẩn hóa về 0-1

    # Kết hợp các thành phần để có điểm cuối cùng
    # 40% dựa trên content similarity, 40% dựa trên category preference, 20% từ loyalty
    raw_score = 0.4 * content_sim + 0.4 * category_preference + 0.2 * loyalty_score

    # Chuẩn hóa điểm về thang -1.5 đến 1.5 (tương tự điểm tương tác)
    scaled_score = raw_score * 3 - 1.5

    return scaled_score

"""#### So sánh với hàm dự đoán cũ"""

def compare_cb_predictions():
    """So sánh dự đoán của CB cải tiến với CB cũ"""
    # Chọn ngẫu nhiên 5 cặp user-item
    sample_users = np.random.choice(list(user_profiles.index), 5)
    sample_items = np.random.choice(list(product_features_encoded.index), 5)

    results = []
    for user in sample_users:
        for item in sample_items:
            old_pred = predict_cb_score(user, item)
            new_pred = predict_cb_score_improved(user, item)
            results.append({
                'User': user,
                'Item': item,
                'Original CB Score': old_pred,
                'Improved CB Score': new_pred
            })

    return pd.DataFrame(results)

# Sử dụng phiên bản cải tiến cho Content-Based -> Biến predict_cb_score được gọi sẽ gọi hàm predict_cb_score_improved
predict_cb_score = predict_cb_score_improved
predict_cb_score

# Thực hiện so sánh
cb_comparison = compare_cb_predictions()
display(cb_comparison)

"""#### Triển khai KNN Collaborative Filtering"""

from sklearn.neighbors import NearestNeighbors

# Tạo mô hình KNN cho người dùng (users)
user_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10, n_jobs=-1)
user_knn.fit(train_matrix.values)

def predict_knn_cf_score(user_id, item_id, k=5):
    """Dự đoán điểm tương tác dựa trên KNN Collaborative Filtering"""
    if user_id not in train_matrix.index:
        return np.nan
    if item_id not in train_matrix.columns:
        return np.nan

    # Lấy vị trí của user_id trong train_matrix
    user_idx = train_matrix.index.get_loc(user_id)

    # Tìm k users tương tự nhất
    distances, indices = user_knn.kneighbors(train_matrix.iloc[user_idx:user_idx+1].values, n_neighbors=k+1)

    # Bỏ qua chính user đó
    similar_users_indices = indices.flatten()[1:]
    distances = distances.flatten()[1:]

    # Đảm bảo là có các user tương tự
    if len(similar_users_indices) == 0:
        return np.nan

    # Convert từ indices sang user IDs
    similar_users = train_matrix.index[similar_users_indices]

    # Lấy điểm cho item từ các user tương tự
    item_col_idx = train_matrix.columns.get_loc(item_id)
    ratings = []
    weights = []

    for i, sim_user in enumerate(similar_users):
        rating = train_matrix.iloc[similar_users_indices[i], item_col_idx]
        if rating > 0:  # Chỉ xét những user đã từng tương tác với item
            ratings.append(rating)
            weights.append(1 - distances[i])  # Độ tương tự (1 - khoảng cách cosine)

    # Nếu không có user nào trong nhóm tương tự từng tương tác với item
    if len(ratings) == 0:
        return np.nan

    # Dự đoán điểm là trung bình có trọng số
    return np.average(ratings, weights=weights)

"""## 4. Hybrid Recommendation: Kết hợp nhiều phương pháp

Hệ thống Hybrid kết hợp 4 phương pháp khác nhau để tạo ra đề xuất tối ưu:

1. **Item-Item Collaborative Filtering**: Dựa trên sự tương đồng giữa các sản phẩm theo lịch sử tương tác
2. **Content-Based Filtering**: Dựa trên đặc trưng sản phẩm và hồ sơ người dùng
3. **Matrix Factorization (SVD)**: Giải quyết vấn đề sparse matrix
4. **KNN Collaborative Filtering**: Dựa trên sự tương đồng giữa người dùng

Mỗi phương pháp có ưu điểm riêng và việc kết hợp chúng giúp cải thiện chất lượng đề xuất.
"""

def hybrid_score(user_id, item_id, alpha=0.3, beta=0.3, gamma=0.2, delta=0.2, k_cf=5):
    """Kết hợp nhiều phương pháp đề xuất với trọng số khác nhau

    Tham số:
    - alpha: trọng số cho Item-Item CF
    - beta: trọng số cho Content-based
    - gamma: trọng số cho SVD
    - delta: trọng số cho KNN CF
    - k_cf: số lượng item lân cận cho Item-Item CF

    Tổng trọng số phải bằng 1: alpha + beta + gamma + delta = 1

    Trả về:
    - Điểm hybrid trong khoảng -1.5 đến 1.5
    - Nếu không thể dự đoán, trả về 0
    """
    # Đảm bảo tổng trọng số = 1
    total = alpha + beta + gamma + delta
    if total != 1:
        alpha, beta, gamma, delta = alpha/total, beta/total, gamma/total, delta/total

    # Lấy dự đoán từ các phương pháp khác nhau
    cf = predict_cf_score(user_id, item_id, train_matrix, item_sim_matrix, k=k_cf)
    cb = predict_cb_score(user_id, item_id)
    svd = predict_svd_score(user_id, item_id)
    knn = predict_knn_cf_score(user_id, item_id)

    # Thêm debug info
    debug = {
        'cf_score': cf,
        'cb_score': cb,
        'svd_score': svd,
        'knn_score': knn
    }

    # Đếm số lượng phương pháp có dự đoán hợp lệ
    valid_preds = 0
    valid_sum = 0

    # Tính tổng có trọng số của các dự đoán hợp lệ
    if not np.isnan(cf):
        valid_sum += alpha * cf
        valid_preds += alpha

    if not np.isnan(cb):
        valid_sum += beta * cb
        valid_preds += beta

    if not np.isnan(svd):
        valid_sum += gamma * svd
        valid_preds += gamma

    if not np.isnan(knn):
        valid_sum += delta * knn
        valid_preds += delta

    # Trường hợp không có dự đoán nào hợp lệ
    if valid_preds == 0:
        return 0  # Fallback score

    # Trả về trung bình có trọng số
    return valid_sum / valid_preds

"""
### Demo hybrid score calculation"""

def show_hybrid_score_components(user_id, item_id):
    """Hiển thị chi tiết các thành phần của hybrid score"""
    cf = predict_cf_score(user_id, item_id, train_matrix, item_sim_matrix, k=5)
    cb = predict_cb_score(user_id, item_id)
    svd = predict_svd_score(user_id, item_id)
    knn = predict_knn_cf_score(user_id, item_id)
    hybrid = hybrid_score(user_id, item_id)

    results = pd.DataFrame({
        'Method': ['Item-Item CF', 'Content-Based', 'SVD', 'KNN CF', 'Hybrid'],
        'Score': [cf, cb, svd, knn, hybrid]
    })

    return results

"""### Chọn một cặp user-item ngẫu nhiên để demo"""

sample_user = np.random.choice(list(user_profiles.index))
sample_item = np.random.choice(list(product_features_encoded.index))
print(f"Demo hybrid scoring cho user {sample_user}, item {sample_item}:")

hybrid_components = show_hybrid_score_components(sample_user, sample_item)
display(hybrid_components)

"""### Hàm đề xuất top-N sản phẩm cho user"""

def recommend_top_n(user_id, N=5, candidate_items=None, alpha=0.3, beta=0.3, gamma=0.2, delta=0.2):
    """Đề xuất top-N sản phẩm cho user_id sử dụng Hybrid Recommendation"""
    if candidate_items is None:
        candidate_items = train_matrix.columns

    # Loại bỏ các item user đã tương tác
    if user_id in train_matrix.index:
        interacted = set(train_matrix.loc[user_id][train_matrix.loc[user_id] > 0].index)
    else:
        interacted = set()

    items_to_score = [item for item in candidate_items if item not in interacted]

    # Nếu là cold-start user (không tìm thấy thông tin user)
    if user_id not in train_matrix.index and user_id not in user_profiles.index:
        print(f"Cold-start user: {user_id}, sử dụng popularity-based recommendation")
        popular_items = get_popular_items(N)
        scores = [(item, 0.5) for item in popular_items if item not in interacted]  # Giá trị mặc định 0.5
        return pd.DataFrame(scores[:N], columns=['Product_Category', 'Hybrid_Score'])

    # Tính điểm cho tất cả các sản phẩm có thể đề xuất
    scores = []
    for item in items_to_score:
        score = hybrid_score(user_id, item, alpha=alpha, beta=beta, gamma=gamma, delta=delta)
        scores.append((item, score))

    # Sắp xếp theo điểm và lấy top-N
    scores = sorted(scores, key=lambda x: x[1], reverse=True)[:N]

    return pd.DataFrame(scores, columns=['Product_Category', 'Hybrid_Score'])

"""## 5. Đánh giá hệ thống"""

from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, ndcg_score

"""### Đánh giá RMSE trên tập test cho từng method"""

def evaluate_rmse(test_df, method='hybrid', top_k_cf=5, alpha=0.3, beta=0.3, gamma=0.2, delta=0.2):
    preds = []
    trues = []

    for _, row in test_df.iterrows():
        user = row['Customer ID']
        item = row['Product_Category']
        true_score = row['Interaction_Score']

        if method == 'hybrid':
            pred = hybrid_score(user, item, alpha=alpha, beta=beta, gamma=gamma, delta=delta, k_cf=top_k_cf)
        elif method == 'cf':
            pred = predict_cf_score(user, item, train_matrix, item_sim_matrix, k=top_k_cf)
        elif method == 'cb':
            pred = predict_cb_score(user, item)
        elif method == 'svd':
            pred = predict_svd_score(user, item)
        elif method == 'knn':
            pred = predict_knn_cf_score(user, item)
        else:
            pred = 0

        if not np.isnan(pred):
            preds.append(pred)
            trues.append(true_score)

    if len(trues) == 0:
        return np.nan, 0

    rmse = np.sqrt(mean_squared_error(trues, preds))
    return rmse, len(trues)

"""### Đánh giá precision@N, recall@N, F1@N và NDCG@N"""

def evaluate_ranking_metrics(test_df, N=5, method='hybrid', alpha=0.3, beta=0.3, gamma=0.2, delta=0.2):
    # Nhóm các item theo user trong tập test
    user_gt = test_df.groupby('Customer ID')['Product_Category'].apply(set)
    users = user_gt.index

    precisions = []
    recalls = []
    f1s = []
    ndcgs = []

    for user in users:
        gt_items = user_gt[user]

        # Lấy top-N gợi ý theo phương pháp
        if method == 'hybrid':
            recs = recommend_top_n(user, N=N, alpha=alpha, beta=beta, gamma=gamma, delta=delta)['Product_Category'].tolist()
        elif method == 'cf':
            recs = recommend_top_n(user, N=N, alpha=1.0, beta=0.0, gamma=0.0, delta=0.0)['Product_Category'].tolist()
        elif method == 'cb':
            recs = recommend_top_n(user, N=N, alpha=0.0, beta=1.0, gamma=0.0, delta=0.0)['Product_Category'].tolist()
        elif method == 'svd':
            recs = recommend_top_n(user, N=N, alpha=0.0, beta=0.0, gamma=1.0, delta=0.0)['Product_Category'].tolist()
        elif method == 'knn':
            recs = recommend_top_n(user, N=N, alpha=0.0, beta=0.0, gamma=0.0, delta=1.0)['Product_Category'].tolist()
        else:
            recs = []

        # Tính các metrics
        hits = len(set(recs) & gt_items)
        precision = hits / N if N > 0 else 0
        recall = hits / len(gt_items) if len(gt_items) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # Tính NDCG
        # Tạo relevance scores
        relevance = np.zeros(len(recs))
        for i, item in enumerate(recs):
            if item in gt_items:
                relevance[i] = 1

        # Nếu không có item liên quan nào thì NDCG = 0
        if np.sum(relevance) == 0:
            ndcg = 0
        else:
            # Tính NDCG
            # Tạo ideal relevance scores (tất cả items liên quan ở đầu)
            ideal = np.zeros_like(relevance)
            ideal[:min(len(gt_items), len(relevance))] = 1

            ndcg = ndcg_score(np.array([ideal]), np.array([relevance]))

        precisions.append(precision)
        recalls.append(recall)
        f1s.append(f1)
        ndcgs.append(ndcg)

    return {
        'Precision@N': np.mean(precisions),
        'Recall@N': np.mean(recalls),
        'F1@N': np.mean(f1s),
        'NDCG@N': np.mean(ndcgs)
    }

"""## So sánh hiệu quả các phương pháp

Trong phần này, chúng ta so sánh hiệu quả của các phương pháp khác nhau:
1. Hybrid Recommendation (kết hợp)
2. Item-Item Collaborative Filtering
3. Content-Based Filtering
4. Matrix Factorization (SVD)
5. KNN Collaborative Filtering

Sử dụng các metrics: RMSE, Precision@N, Recall@N, F1@N và NDCG@N

### Đánh giá RMSE cho từng phương pháp
"""

print("Đánh giá RMSE của các phương pháp:")

def print_rmse_result(name, rmse, n):
    if n == 0 or np.isnan(rmse):
        print(f"RMSE {name}: Không có dự đoán hợp lệ (n=0)")
    else:
        print(f"RMSE {name}: {rmse:.4f} (n={n})")

rmse_hybrid, n_hybrid = evaluate_rmse(test_df, method='hybrid')
rmse_cf, n_cf         = evaluate_rmse(test_df, method='cf')
rmse_cb, n_cb         = evaluate_rmse(test_df, method='cb')
rmse_svd, n_svd       = evaluate_rmse(test_df, method='svd')
rmse_knn, n_knn       = evaluate_rmse(test_df, method='knn')


print_rmse_result('Hybrid', rmse_hybrid, n_hybrid)
print_rmse_result('CF (Item-Item)', rmse_cf, n_cf)
print_rmse_result('CB (Content-Based)', rmse_cb, n_cb)
print_rmse_result('SVD (Matrix Factorization)', rmse_svd, n_svd)
print_rmse_result('KNN (User-User)', rmse_knn, n_knn)

"""### Đánh giá ranking metrics cho từng phương pháp"""

print("\nĐánh giá ranking metrics với N=5:")

metrics_hybrid = evaluate_ranking_metrics(test_df, N=5, method='hybrid')
metrics_cf = evaluate_ranking_metrics(test_df, N=5, method='cf')
metrics_cb = evaluate_ranking_metrics(test_df, N=5, method='cb')
metrics_svd = evaluate_ranking_metrics(test_df, N=5, method='svd')
metrics_knn = evaluate_ranking_metrics(test_df, N=5, method='knn')

"""### Tạo DataFrame so sánh"""

metrics_df = pd.DataFrame({
    'Hybrid': metrics_hybrid,
    'CF (Item-Item)': metrics_cf,
    'CB (Content-Based)': metrics_cb,
    'SVD (Matrix Factorization)': metrics_svd,
    'KNN (User-User)': metrics_knn,
}).T

display(metrics_df)

"""### Visualize so sánh các phương pháp đề xuất"""

import matplotlib.pyplot as plt
import seaborn as sns

# Tạo dữ liệu để vẽ biểu đồ
methods = ['Hybrid', 'CF (Item-Item)', 'CB (Content-Based)', 'SVD (Matrix Factorization)', 'KNN (User-User)']
print(methods)

"""#### 1. Biểu đồ RMSE"""

plt.figure(figsize=(12, 6))
rmses = [rmse_hybrid, rmse_cf, rmse_cb, rmse_svd, rmse_knn]
valid_methods = [methods[i] for i in range(len(methods)) if not np.isnan(rmses[i])]
valid_rmses = [rmse for rmse in rmses if not np.isnan(rmse)]

if valid_rmses:  # Đảm bảo có dữ liệu hợp lệ
    ax = sns.barplot(x=valid_methods, y=valid_rmses)
    plt.title('RMSE của các phương pháp đề xuất (thấp hơn là tốt hơn)', fontsize=14)
    plt.ylabel('RMSE')
    plt.xticks(rotation=45)

    # Thêm giá trị lên đầu mỗi cột
    for i, p in enumerate(ax.patches):
        ax.annotate(f'{p.get_height():.4f}',
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha = 'center', va = 'bottom',
                    fontsize=10,
                    rotation=0,
                    xytext=(0, 5),
                    textcoords='offset points')

    plt.tight_layout()
    plt.show()

"""#### 2. Biểu đồ các ranking metrics"""

metrics_list = [metrics_hybrid, metrics_cf, metrics_cb, metrics_svd, metrics_knn]
metrics_data = pd.DataFrame(metrics_list, index=methods)

plt.figure(figsize=(14, 7))
metrics_data.plot(kind='bar', figsize=(14, 7))
plt.title('So sánh các metrics đánh giá xếp hạng (cao hơn là tốt hơn)', fontsize=14)
plt.ylabel('Giá trị')
plt.xlabel('Phương pháp')
plt.xticks(rotation=45)
plt.legend(title='Metrics')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""## 6. Ví dụ đề xuất cho các user khác nhau

### Chọn một số user đại diện
"""

sample_users = [train_matrix.index[0], train_matrix.index[10], train_matrix.index[-1]]
print(f"Đề xuất cho {len(sample_users)} users")

for user in sample_users:
    print(f"\nTop 5 sản phẩm đề xuất cho user {user} (Hybrid):")
    display(recommend_top_n(user, N=5))
    print(f"Các sản phẩm đã tương tác:")
    interacted = train_matrix.loc[user][train_matrix.loc[user] > 0]
    print(interacted.index.tolist())

"""## 7. Visualization: So sánh điểm dự đoán các phương pháp

### Lấy 100 điểm dự đoán mẫu
"""

sample_test = test_df.sample(100, random_state=42)
print(f"Lấy mẫu {len(sample_test)} điểm dự đoán")

cf_preds = [predict_cf_score(row['Customer ID'], row['Product_Category'], train_matrix, item_sim_matrix) for _, row in sample_test.iterrows()]
print(f"Đã dự đoán {len(cf_preds)} điểm cho CF")
print(f"cf_preds: \n{cf_preds}")
print(f"Có {len([p for p in cf_preds if not np.isnan(p)])} điểm hợp lệ")

cb_preds = [predict_cb_score(row['Customer ID'], row['Product_Category']) for _, row in sample_test.iterrows()]
print(f"Đã dự đoán {len(cb_preds)} điểm cho CF")
print(f"cb_preds: \n{cb_preds}")
print(f"Có {len([p for p in cb_preds if not np.isnan(p)])} điểm hợp lệ")

hyb_preds = [hybrid_score(row['Customer ID'], row['Product_Category']) for _, row in sample_test.iterrows()]
print(f"Đã dự đoán {len(hyb_preds)} điểm cho CF")
print(f"hyb_preds: \n{hyb_preds}")
print(f"Có {len([p for p in hyb_preds if not np.isnan(p)])} điểm hợp lệ")

true_scores = sample_test['Interaction_Score'].values
print(f"true_scores: \n{true_scores}")

plt.figure(figsize=(10,6))
plt.scatter(true_scores, cf_preds, alpha=0.6, label='CF', marker='o')
plt.scatter(true_scores, cb_preds, alpha=0.6, label='CB', marker='x')
plt.scatter(true_scores, hyb_preds, alpha=0.6, label='Hybrid', marker='^')
plt.xlabel('True Interaction Score')
plt.ylabel('Predicted Score')
plt.legend()
plt.title('So sánh điểm dự đoán giữa các phương pháp')
plt.show()

"""## 8. Nhận xét & Tổng kết
- **Hybrid system** tận dụng được cả lịch sử tương tác (CF) và đặc trưng sản phẩm/người dùng (CB), giúp cải thiện hiệu quả đề xuất, đặc biệt với cold start.
- **CF** mạnh khi user có nhiều lịch sử, nhưng yếu với user/item mới.
- **CB** tận dụng được thông tin context (mùa, loyalty), nhưng dễ bị "quá an toàn" (không khám phá sản phẩm mới).
- **Hybrid** cân bằng giữa cá nhân hóa và khả năng khám phá, có thể điều chỉnh trọng số `alpha` để phù hợp từng bài toán.
- Có thể mở rộng thêm context (ví dụ: thời tiết, sự kiện, v.v.) vào Content-Based để tăng tính cá nhân hóa.

## 9. Phần A: So sánh các mô hình Machine Learning với Train-Test Split

Trong phần này, chúng ta phát triển và so sánh 5 mô hình ML khác nhau:
1. Linear Regression
2. KNN Regressor
3. Random Forest Regressor
4. Gradient Boosting Regressor
5. XGBoost Regressor

Các mô hình này dự đoán điểm tương tác (Interaction Score) dựa trên đặc trưng người dùng và sản phẩm. Chúng ta sẽ so sánh kết quả với Hybrid Recommendation System đã xây dựng.

Phương pháp:
- Chia dữ liệu thành tập train (80%) và test (20%)
- Huấn luyện các mô hình ML trên tập train
- Đánh giá RMSE và R² trên tập test
- So sánh với Hybrid Recommendation System

### Import các thư viện cần thiết cho Machine Learning models
"""

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### Chuẩn bị dữ liệu cho các mô hình ML"""

def prepare_ml_data(df):
    """Chuẩn bị feature và target từ DataFrame"""
    # Chọn các đặc trưng phù hợp cho ML model (loại bỏ ID, category labels, v.v.)
    feature_cols = [
        'Customer_Loyalty_Score', 'Age', 'Season',
        'Gender_Female', 'Previous Purchases', 'Review Rating',
        'Shipping Type_encoded', 'Discount Applied_encoded',
        'Category_Accessories', 'Category_Clothing', 'Category_Footwear', 'Category_Outerwear'
    ]

    # Nếu 'Season' là chuỗi, mã hóa one-hot
    X = df[feature_cols].copy()
    if X['Season'].dtype == object:
        X = pd.get_dummies(X, columns=['Season'], prefix='season', dtype=int)
    y = df['Interaction_Score']
    return X, y

"""
### Chuẩn bị dữ liệu"""

X_train, y_train = prepare_ml_data(train_df)
X_test, y_test = prepare_ml_data(test_df)

print(f"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}")

"""### Định nghĩa 5 mô hình ML để so sánh"""

models = {
    'Linear Regression': LinearRegression(),
    'KNN Regressor': KNeighborsRegressor(n_neighbors=5),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

for name, model in models.items():
    print(f"{name}: {model}\n")

"""### Dictionary để lưu kết quả"""

results_a = {}

"""###  Train và đánh giá từng mô hình"""

for name, model in models.items():
    print(f"\nTraining {name}...")

    # Tạo pipeline với StandardScaler
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    # Train
    pipeline.fit(X_train, y_train)

    # Dự đoán và đánh giá
    y_pred_train = pipeline.predict(X_train)
    y_pred_test = pipeline.predict(X_test)

    # Tính metrics
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)

    # Lưu kết quả
    results_a[name] = {
        'Train RMSE': train_rmse,
        'Test RMSE': test_rmse,
        'Train R2': train_r2,
        'Test R2': test_r2,
        'Model': pipeline
    }

    print(f"  Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}")
    print(f"  Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}")

"""### So sánh với hybrid recommendation system"""

results_a['Hybrid Recommendation'] = {
    'Test RMSE': rmse_hybrid,
    'Model': 'Hybrid'
}

# Tạo DataFrame để hiển thị kết quả
results_df_a = pd.DataFrame({
    'Model': list(results_a.keys()),
    'Test RMSE': [results_a[model]['Test RMSE'] for model in results_a.keys()]
})

# Sắp xếp theo Test RMSE
results_df_a = results_df_a.sort_values('Test RMSE', ascending=False)

display(results_df_a)

"""### Visualize kết quả"""

plt.figure(figsize=(10, 6))
ax = sns.barplot(x='Model', y='Test RMSE', data=results_df_a)
plt.title('So sánh RMSE giữa các mô hình (Train-Test Split)', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.ylabel('RMSE (thấp hơn là tốt hơn)')
plt.tight_layout()

# Thêm giá trị lên đầu mỗi cột
for i, p in enumerate(ax.patches):
    ax.annotate(f'{p.get_height():.4f}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'bottom',
                fontsize=10,
                rotation=0,
                xytext=(0, 5),
                textcoords='offset points')

plt.show()

"""## 10. Phần B: Đánh giá mô hình bằng K-Fold Cross-Validation

Sử dụng K-fold cross-validation (K=5) để đánh giá mô hình một cách robust hơn, giảm thiểu sự phụ thuộc vào một cách chia train-test cụ thể.

Cross-validation cho phép chúng ta:
1. Kiểm tra độ ổn định của mô hình với các phân chia dữ liệu khác nhau
2. Phát hiện overfitting
3. Đánh giá độ tin cậy của các mô hình

Phương pháp:
- Chia dữ liệu thành 5 fold
- Với mỗi fold, huấn luyện trên 4 fold còn lại và kiểm tra trên fold hiện tại
- Tính RMSE trung bình và độ lệch chuẩn qua 5 lần chạy
"""

from sklearn.model_selection import KFold, cross_val_score

# Kết hợp train và test để thực hiện cross-validation trên toàn bộ dữ liệu
X_full = pd.concat([X_train, X_test])
y_full = pd.concat([y_train, y_test])
print(f"X_full.shape: {X_full.shape} & y_full.shape: {y_full.shape}")

"""### Số lượng fold"""

k_folds = 5
kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
print(kf)

# Dictionary để lưu kết quả
results_cv = {}

# Thực hiện cross-validation cho từng mô hình
for name, model in models.items():
    print(f"\nCross-validation for {name}...")

    # Tạo pipeline với StandardScaler
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    # Thực hiện cross-validation
    cv_scores = -cross_val_score(pipeline, X_full, y_full,
                                cv=kf, scoring='neg_root_mean_squared_error')

    # Lưu kết quả
    results_cv[name] = {
        'CV RMSE': cv_scores,
        'Mean CV RMSE': cv_scores.mean(),
        'Std CV RMSE': cv_scores.std()
    }

    print(f"  Fold RMSEs: {cv_scores}")
    print(f"  Mean RMSE: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

"""### Tạo DataFrame để hiển thị kết quả"""

results_df_cv = pd.DataFrame({
    'Model': list(results_cv.keys()),
    'Mean CV RMSE': [results_cv[model]['Mean CV RMSE'] for model in results_cv.keys()],
    'Std CV RMSE': [results_cv[model]['Std CV RMSE'] for model in results_cv.keys()]
})

# Sắp xếp theo Mean CV RMSE
results_df_cv = results_df_cv.sort_values('Mean CV RMSE', ascending=False)

display(results_df_cv)

"""### Visualize cross-validation results"""

plt.figure(figsize=(12, 6))

# Create violin plots for each model's CV results
data_to_plot = [results_cv[model]['CV RMSE'] for model in results_df_cv['Model']]
ax = plt.violinplot(data_to_plot, showmeans=True)

plt.xticks(np.arange(1, len(results_df_cv) + 1), results_df_cv['Model'], rotation=45, ha='right')
plt.title('5-Fold Cross-Validation RMSE Distribution by Model', fontsize=14)
plt.ylabel('RMSE (thấp hơn là tốt hơn)')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add mean ± std text annotations
for i, model in enumerate(results_df_cv['Model']):
    mean = results_cv[model]['Mean CV RMSE']
    std = results_cv[model]['Std CV RMSE']
    plt.text(i+1, mean + 0.02, f'{mean:.4f} ± {std:.4f}',
             ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

"""## 11. Phần C: Hyperparameter Tuning và So sánh kết quả

Trong phần này, chúng ta tinh chỉnh hyperparameters cho các mô hình tốt nhất từ phần A và B để cải thiện hiệu suất.

Hyperparameter tuning giúp:
1. Tìm cấu hình tối ưu cho mỗi mô hình
2. Cải thiện hiệu suất so với cấu hình mặc định
3. Hiểu rõ hơn về tác động của các hyperparameter

Phương pháp:
- Sử dụng GridSearchCV với 5-fold cross-validation
- Tinh chỉnh các hyperparameter chính của mỗi mô hình
- So sánh hiệu suất trước và sau khi tinh chỉnh
- Xác định mô hình tốt nhất tổng thể
"""

from sklearn.model_selection import GridSearchCV

"""### Chọn 3 mô hình tốt nhất để tinh chỉnh
- (giả sử là Random Forest, Gradient Boosting và XGBoost, nhưng sẽ thay đổi dựa trên kết quả phần A và B)
"""

top_models = {
    'Random Forest': {
        'model': RandomForestRegressor(random_state=42),
        'params': {
            'model__n_estimators': [50, 100, 200],
            'model__max_depth': [None, 10, 20, 30],
            'model__min_samples_split': [2, 5, 10],
            'model__min_samples_leaf': [1, 2, 4]
        }
    },
    'Gradient Boosting': {
        'model': GradientBoostingRegressor(random_state=42),
        'params': {
            'model__n_estimators': [50, 100, 200],
            'model__learning_rate': [0.01, 0.1, 0.2],
            'model__max_depth': [3, 5, 7],
            'model__subsample': [0.8, 1.0]
        }
    },
    'XGBoost': {
        'model': XGBRegressor(random_state=42),
        'params': {
            'model__n_estimators': [50, 100, 200],
            'model__learning_rate': [0.01, 0.1, 0.2],
            'model__max_depth': [3, 5, 7],
            'model__subsample': [0.8, 1.0],
            'model__colsample_bytree': [0.8, 1.0]
        }
    }
}

for name, model_info in top_models.items():
    print(f"{name}: {model_info['model']}\n")

# Dictionary để lưu kết quả
results_tuned = {}

# Thực hiện GridSearchCV
for name, model_info in top_models.items():
    print(f"\nTuning hyperparameters for {name}...")

    # Tạo pipeline với StandardScaler
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model_info['model'])
    ])

    # GridSearchCV
    grid_search = GridSearchCV(
        pipeline, model_info['params'],
        cv=5, scoring='neg_root_mean_squared_error',
        n_jobs=-1, verbose=1
    )

    grid_search.fit(X_train, y_train)

    # Mô hình tốt nhất
    best_model = grid_search.best_estimator_

    # Dự đoán với mô hình tốt nhất
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)

    # Tính metrics
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    # Lưu kết quả
    results_tuned[name] = {
        'Train RMSE': train_rmse,
        'Test RMSE': test_rmse,
        'Best Params': grid_search.best_params_,
        'Model': best_model
    }

    print(f"  Best parameters: {grid_search.best_params_}")
    print(f"  Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}")
    print(f"  Improvement vs. Base: {results_a[name]['Test RMSE'] - test_rmse:.4f}")

"""### Tạo DataFrame để hiển thị kết quả"""

results_df_tuned = pd.DataFrame({
    'Model': list(results_tuned.keys()),
    'Base Test RMSE': [results_a[model]['Test RMSE'] for model in results_tuned.keys()],
    'Tuned Test RMSE': [results_tuned[model]['Test RMSE'] for model in results_tuned.keys()],
    'Improvement': [results_a[model]['Test RMSE'] - results_tuned[model]['Test RMSE'] for model in results_tuned.keys()]
})

# Sắp xếp theo Test RMSE
results_df_tuned = results_df_tuned.sort_values('Tuned Test RMSE', ascending=False)

display(results_df_tuned)

"""### Final comparison visualizing all approaches"""

plt.figure(figsize=(14, 8))

# Collect all results
final_comparison = {
    'Model': [],
    'RMSE': [],
    'Method': []
}

# Add original hybrid
final_comparison['Model'].append('Hybrid Recommendation')
final_comparison['RMSE'].append(rmse_hybrid)
final_comparison['Method'].append('Original')

# Add base models
for model in results_a.keys():
    if model != 'Hybrid Recommendation':
        final_comparison['Model'].append(model)
        final_comparison['RMSE'].append(results_a[model]['Test RMSE'])
        final_comparison['Method'].append('Base Model')

# Add cross-validation results
for model in results_cv.keys():
    final_comparison['Model'].append(model)
    final_comparison['RMSE'].append(results_cv[model]['Mean CV RMSE'])
    final_comparison['Method'].append('Cross-Validation')

# Add tuned models
for model in results_tuned.keys():
    final_comparison['Model'].append(model)
    final_comparison['RMSE'].append(results_tuned[model]['Test RMSE'])
    final_comparison['Method'].append('Tuned Model')

# Create DataFrame
final_df = pd.DataFrame(final_comparison)
display(final_df)

"""
### Plot"""

ax = sns.barplot(x='Model', y='RMSE', hue='Method', data=final_df)
plt.title('So sánh RMSE giữa tất cả các phương pháp', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.ylabel('RMSE (thấp hơn là tốt hơn)')
plt.legend(title='Phương pháp')
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()

plt.show()

"""## 12. Phân tích và Kết luận

Sau khi so sánh các phương pháp tiếp cận khác nhau, chúng ta có thể rút ra một số kết luận về mô hình tốt nhất để triển khai hệ thống đề xuất sản phẩm.

### Phân tích Feature Importance của mô hình tốt nhất
"""

# Giả sử mô hình tuned tốt nhất là Random Forest hoặc XGBoost
best_model_name = results_df_tuned.iloc[0]['Model']  # Mô hình có RMSE thấp nhất
best_model = results_tuned[best_model_name]['Model']

try:
    # Extract feature importances if model supports it
    if hasattr(best_model.named_steps['model'], 'feature_importances_'):
        importances = best_model.named_steps['model'].feature_importances_
        feat_importance = pd.DataFrame({
            'Feature': X_train.columns,
            'Importance': importances
        }).sort_values('Importance', ascending=False)

        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=feat_importance)
        plt.title(f'Feature Importance for {best_model_name}', fontsize=14)
        plt.tight_layout()
        plt.show()

        print("Top 5 đặc trưng quan trọng nhất:")
        display(feat_importance.head())
except Exception as e:
    print(f"Mô hình không hỗ trợ feature importance hoặc có lỗi: {e}")

"""### So sánh Hybrid Recommendation System với các phương pháp Machine Learning truyền thống

Sau khi phân tích các kết quả từ 3 phần A, B, C, chúng ta có thể đưa ra nhận xét về hiệu quả của các phương pháp:

1. **Hybrid Recommendation System**:
   - **Ưu điểm**:
     - Kết hợp được thông tin ngữ cảnh và lịch sử tương tác
     - Xử lý tốt cold start
     - Cân bằng giữa cá nhân hóa và khám phá
   - **Nhược điểm**:
     - Phức tạp để triển khai và điều chỉnh
     - Cần điều chỉnh trọng số (alpha, beta, gamma, delta)

2. **Machine Learning Truyền thống**:
   - **Ưu điểm**:
     - Dễ triển khai với các thư viện có sẵn
     - Tận dụng được nhiều loại đặc trưng
     - Dự đoán điểm tương tác chính xác
   - **Nhược điểm**:
     - Thiếu tính linh hoạt cho các trường hợp cold start
     - Khó diễn giải kết quả (đặc biệt với mô hình ensemble)

3. **Kết hợp cả hai phương pháp**:
   - Sử dụng ML để dự đoán điểm tương tác trong hệ thống hybrid
   - Áp dụng ensemble method để kết hợp kết quả từ nhiều mô hình
   - Sử dụng hybrid cho cold start và ML cho người dùng quen thuộc

Qua phân tích, chúng ta có thể thấy: <!-- Kết luận dựa trên kết quả thực tế của các phần A, B, C -->

### Tạo một hàm đề xuất sản phẩm sử dụng mô hình ML tốt nhất
"""

def recommend_with_ml(user_id, n=5):
    """Đề xuất sản phẩm dựa trên mô hình ML tốt nhất"""
    # Tìm tất cả sản phẩm mà user chưa tương tác
    if user_id in train_matrix.index:
        interacted = set(train_matrix.loc[user_id][train_matrix.loc[user_id] > 0].index)
    else:
        interacted = set()

    all_items = set(train_matrix.columns)
    items_to_predict = list(all_items - interacted)

    # Nếu không tìm thấy user trong dữ liệu
    if user_id not in user_profiles.index:
        print(f"Không tìm thấy thông tin cho user {user_id}. Sử dụng đề xuất phổ biến.")
        # Return most popular items
        popular_items = get_popular_items(n)
        return pd.DataFrame({
            'Product_Category': popular_items,
            'Score': [0.5] * len(popular_items) # Giá trị mặc định 0.5
        })

    # Lấy thông tin user
    user_data = user_profiles.loc[user_id]
    loyalty_score = user_data['Customer_Loyalty_Score']
    age = user_data['Age']
    gender_female = user_data['Gender_Female']
    previous_purchases = user_data['Previous Purchases']

    # Prepare predictions
    predictions = []

    # Generate features for each user-item pair
    for item in items_to_predict[:min(100, len(items_to_predict))]:  # Giới hạn 100 items để tăng tốc demo
        # Extract item features
        item_category = item.split('_')[1] if '_' in item else item
        item_type = item.split('_')[0]

        # Get item's most common season from mapping
        season = 'Spring'  # Default
        if item in season_map:
            season = season_map[item]

        # Create category indicators
        category_accessories = 1 if item_category == 'Accessories' else 0
        category_clothing = 1 if item_category == 'Clothing' else 0
        category_footwear = 1 if item_category == 'Footwear' else 0
        category_outerwear = 1 if item_category == 'Outerwear' else 0

        # Create feature vector for this user-item pair
        features = {
            'Customer_Loyalty_Score': loyalty_score,
            'Age': age,
            'Season': season,
            'Gender_Female': gender_female,
            'Previous Purchases': previous_purchases,
            'Review Rating': 4.0,  # Placeholder - average rating
            'Shipping Type_encoded': 1,  # Placeholder
            'Discount Applied_encoded': 0,  # Placeholder
            'Category_Accessories': category_accessories,
            'Category_Clothing': category_clothing,
            'Category_Footwear': category_footwear,
            'Category_Outerwear': category_outerwear
        }

        # Convert to DataFrame with consistent column order
        features_df = pd.DataFrame([features])
        # Nếu 'Season' là chuỗi, mã hóa one-hot giống như khi train
        if 'Season' in features_df.columns and features_df['Season'].dtype == object:
            features_df = pd.get_dummies(features_df, columns=['Season'], prefix='season', dtype=int)

        # Đảm bảo các cột giống với X_train
        for col in X_train.columns:
            if col not in features_df.columns:
                features_df[col] = 0

        # Chỉ giữ các cột có trong mô hình
        features_df = features_df[X_train.columns]

        # Predict using best model
        try:
            score = best_model.predict(features_df)[0]
            predictions.append((item, score))
        except Exception as e:
            print(f"Error predicting for {item}: {e}")
            continue

    # Sort by predicted score
    predictions.sort(key=lambda x: x[1], reverse=True)

    # Return top N
    return pd.DataFrame(predictions[:n], columns=['Product_Category', 'ML_Score'])

"""### Demo: compare recommendations from hybrid vs. ML model"""

sample_user = train_matrix.index[15]  # Pick a sample user

print(f"Đề xuất cho user {sample_user} bằng Hybrid Recommendation:")
hybrid_recs = recommend_top_n(sample_user, N=5)
display(hybrid_recs)

print(f"Đề xuất cho user {sample_user} bằng ML model ({best_model_name}):")
ml_recs = recommend_with_ml(sample_user, n=5)
display(ml_recs)

"""
### Compare overlap"""

hybrid_items = set(hybrid_recs['Product_Category'])
ml_items = set(ml_recs['Product_Category'])
overlap = hybrid_items.intersection(ml_items)

print(f"\nSố lượng sản phẩm trùng lặp: {len(overlap)} ({len(overlap)/5*100:.1f}%)")
if len(overlap) > 0:
    print(f"Các sản phẩm trùng lặp: {list(overlap)}")

"""### Visualize recommendation distribution by category"""

def get_category_from_product(product):
    parts = product.split('_')
    return parts[1] if len(parts) > 1 else 'Unknown'

hybrid_categories = [get_category_from_product(p) for p in hybrid_items]
print(f"Hybrid_categories: \n{hybrid_categories}")

ml_categories = [get_category_from_product(p) for p in ml_items]
print(f"Ml_categories: \n{ml_categories}")

category_counts = pd.DataFrame({
    'Hybrid': pd.Series(hybrid_categories).value_counts(),
    'ML': pd.Series(ml_categories).value_counts()
})

print("\nPhân bố category trong các đề xuất:")
display(category_counts)

"""
#### Visualize"""

plt.figure(figsize=(10, 5))
category_counts.plot(kind='bar')
plt.title(f'So sánh phân bố category giữa Hybrid và ML cho user {sample_user}')
plt.ylabel('Số lượng sản phẩm')
plt.xlabel('Category')
plt.tight_layout()
plt.show()

"""## 13. Kết luận và Hướng phát triển

### Các kết luận chính

1. **Hiệu suất các phương pháp**:
   - **Hybrid Recommendation System** cân bằng tốt giữa các metrics, đặc biệt phù hợp với dữ liệu thưa
   - **Content-Based Filtering** hiệu quả cho người dùng mới (cold start)
   - **Collaborative Filtering** hoạt động tốt với người dùng có nhiều lịch sử tương tác
   - **Machine Learning** models (đặc biệt là ensemble models) cung cấp độ chính xác cao nhưng cần nhiều dữ liệu

2. **Đặc trưng quan trọng**:
   - **Customer_Loyalty_Score** có tác động lớn đến sở thích mua hàng
   - **Product_Category** là yếu tố quan trọng nhất trong việc dự đoán sở thích
   - **Mùa (Season)** ảnh hưởng đáng kể đến lựa chọn sản phẩm

3. **Hướng phát triển**:
   - **Kết hợp nhiều mô hình**: Sử dụng ensemble method để kết hợp kết quả từ hybrid và ML model
   - **Deep Learning**: Thử nghiệm với neural network để học các pattern phức tạp hơn
   - **Contextual Recommendation**: Thêm ngữ cảnh như thời gian, vị trí, thời tiết
   - **Online Learning**: Cập nhật mô hình liên tục từ feedback người dùng
   - **A/B Testing**: Đánh giá thực tế các phương pháp trên người dùng thật

### Tổng hợp so sánh cuối cùng giữa các phương pháp
"""

methods = ['Hybrid', 'Content-Based', 'Collaborative Filtering', 'SVD', 'KNN CF', 'Best ML Model']
metrics = ['RMSE', 'Precision@5', 'Recall@5', 'F1@5', 'NDCG@5']

print(f"Methods: \n{methods}")
print(f"\nMetrics: \n{metrics}")

"""
### Giả định kết quả (sẽ được cập nhật từ kết quả thực tế chạy ở trên)"""

final_comparison = pd.DataFrame({
    'Method': methods,
    'RMSE': [rmse_hybrid, rmse_cb, rmse_cf, rmse_svd, rmse_knn, results_df_tuned.iloc[0]['Tuned Test RMSE']],
    'Precision@5': [metrics_hybrid['Precision@N'], metrics_cb['Precision@N'],
                   metrics_cf['Precision@N'], metrics_svd['Precision@N'],
                   metrics_knn['Precision@N'], None],
    'Recall@5': [metrics_hybrid['Recall@N'], metrics_cb['Recall@N'],
                metrics_cf['Recall@N'], metrics_svd['Recall@N'],
                metrics_knn['Recall@N'], None],
    'F1@5': [metrics_hybrid['F1@N'], metrics_cb['F1@N'],
            metrics_cf['F1@N'], metrics_svd['F1@N'],
            metrics_knn['F1@N'], None],
    'NDCG@5': [metrics_hybrid['NDCG@N'], metrics_cb['NDCG@N'],
              metrics_cf['NDCG@N'], metrics_svd['NDCG@N'],
              metrics_knn['NDCG@N'], None],
    'Cold-Start Handling': ['Rất tốt', 'Tốt', 'Kém', 'Trung bình', 'Kém', 'Trung bình'],
    'Implementation Complexity': ['Cao', 'Trung bình', 'Thấp', 'Trung bình', 'Thấp', 'Trung bình'],
    'Explainability': ['Trung bình', 'Cao', 'Trung bình', 'Thấp', 'Trung bình', 'Thấp'],
    'Real-time Performance': ['Trung bình', 'Cao', 'Cao', 'Cao', 'Thấp', 'Cao']
})

"""### Hiển thị kết quả"""

print("BẢNG SO SÁNH TỔNG HỢP CÁC PHƯƠNG PHÁP:")
display(final_comparison[['Method', 'RMSE', 'Precision@5', 'Recall@5', 'F1@5', 'NDCG@5']])

print("\nĐÁNH GIÁ ĐỊNH TÍNH:")
display(final_comparison[['Method', 'Cold-Start Handling', 'Implementation Complexity', 'Explainability', 'Real-time Performance']])

"""### Visualize RMSE và F1@5"""

fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# RMSE (thấp hơn là tốt hơn)
rmse_data = final_comparison[['Method', 'RMSE']].dropna()
ax[0].barh(rmse_data['Method'], rmse_data['RMSE'], color='salmon')
ax[0].set_title('RMSE by Method (thấp hơn là tốt hơn)')
ax[0].set_xlabel('RMSE')

# F1@5 (cao hơn là tốt hơn)
f1_data = final_comparison[['Method', 'F1@5']].dropna()
ax[1].barh(f1_data['Method'], f1_data['F1@5'], color='skyblue')
ax[1].set_title('F1@5 by Method (cao hơn là tốt hơn)')
ax[1].set_xlabel('F1@5')

plt.tight_layout()
plt.show()